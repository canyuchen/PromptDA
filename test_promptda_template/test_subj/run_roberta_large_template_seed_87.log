06/27 03:42:05 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-87/', data_seed=87, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 03:42:05 PM device: cuda n_gpu: 1
06/27 03:42:05 PM Writing example 0 of 48
06/27 03:42:05 PM *** Example ***
06/27 03:42:05 PM guid: train-1
06/27 03:42:05 PM tokens: <s> the Ġpace Ġof Ġthe Ġfilm Ġis Ġvery Ġslow Ġ( Ġfor Ġobvious Ġreasons Ġ) Ġand Ġthat Ġtoo Ġbecomes Ġoff - put ting Ġ. </s> ĠIt Ġis <mask>
06/27 03:42:05 PM input_ids: 0 627 2877 9 5 822 16 182 2635 36 13 4678 2188 4839 8 14 350 3374 160 12 9179 2577 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM label: ['Ġwonderful']
06/27 03:42:05 PM Writing example 0 of 16
06/27 03:42:05 PM *** Example ***
06/27 03:42:05 PM guid: dev-1
06/27 03:42:05 PM tokens: <s> what Ġcould Ġhave Ġbeen Ġa Ġpointed Ġlittle Ġch iller Ġabout Ġthe Ġfrightening Ġsed uct iveness Ġof Ġnew Ġtechnology Ġloses Ġfaith Ġin Ġits Ġown Ġviability Ġand Ġsucc umbs Ġto Ġjoy less Ġspecial - effects Ġexcess Ġ. </s> ĠIt Ġis <mask>
06/27 03:42:05 PM input_ids: 0 12196 115 33 57 10 3273 410 1855 8690 59 5 21111 10195 21491 12367 9 92 806 13585 3975 11 63 308 23990 8 25047 29123 7 5823 1672 780 12 38375 7400 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM label: ['Ġwonderful']
06/27 03:42:05 PM Writing example 0 of 2000
06/27 03:42:05 PM *** Example ***
06/27 03:42:05 PM guid: dev-1
06/27 03:42:05 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 03:42:05 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:42:05 PM label: ['Ġwonderful']
06/27 03:42:18 PM ***** Running training *****
06/27 03:42:18 PM   Num examples = 48
06/27 03:42:18 PM   Batch size = 4
06/27 03:42:18 PM   Num steps = 120
06/27 03:42:18 PM n: embeddings.word_embeddings.weight
06/27 03:42:18 PM n: embeddings.position_embeddings.weight
06/27 03:42:18 PM n: embeddings.token_type_embeddings.weight
06/27 03:42:18 PM n: embeddings.LayerNorm.weight
06/27 03:42:18 PM n: embeddings.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.0.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.0.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.0.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.0.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.0.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.0.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.0.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.0.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.0.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.0.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.0.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.0.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.1.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.1.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.1.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.1.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.1.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.1.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.1.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.1.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.1.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.1.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.1.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.1.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.2.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.2.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.2.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.2.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.2.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.2.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.2.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.2.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.2.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.2.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.2.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.2.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.3.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.3.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.3.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.3.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.3.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.3.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.3.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.3.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.3.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.3.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.3.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.3.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.4.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.4.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.4.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.4.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.4.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.4.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.4.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.4.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.4.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.4.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.4.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.4.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.5.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.5.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.5.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.5.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.5.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.5.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.5.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.5.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.5.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.5.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.5.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.5.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.6.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.6.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.6.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.6.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.6.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.6.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.6.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.6.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.6.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.6.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.6.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.6.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.7.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.7.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.7.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.7.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.7.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.7.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.7.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.7.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.7.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.7.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.7.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.7.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.8.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.8.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.8.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.8.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.8.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.8.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.8.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.8.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.8.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.8.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.8.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.8.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.9.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.9.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.9.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.9.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.9.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.9.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.9.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.9.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.9.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.9.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.9.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.9.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.10.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.10.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.10.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.10.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.10.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.10.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.10.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.10.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.10.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.10.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.10.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.10.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.11.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.11.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.11.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.11.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.11.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.11.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.11.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.11.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.11.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.11.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.11.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.11.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.12.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.12.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.12.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.12.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.12.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.12.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.12.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.12.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.12.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.12.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.12.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.12.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.13.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.13.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.13.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.13.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.13.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.13.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.13.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.13.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.13.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.13.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.13.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.13.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.14.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.14.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.14.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.14.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.14.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.14.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.14.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.14.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.14.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.14.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.14.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.14.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.15.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.15.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.15.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.15.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.15.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.15.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.15.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.15.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.15.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.15.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.15.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.15.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.16.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.16.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.16.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.16.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.16.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.16.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.16.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.16.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.16.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.16.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.16.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.16.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.17.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.17.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.17.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.17.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.17.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.17.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.17.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.17.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.17.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.17.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.17.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.17.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.18.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.18.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.18.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.18.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.18.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.18.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.18.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.18.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.18.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.18.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.18.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.18.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.19.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.19.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.19.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.19.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.19.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.19.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.19.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.19.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.19.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.19.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.19.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.19.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.20.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.20.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.20.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.20.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.20.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.20.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.20.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.20.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.20.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.20.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.20.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.20.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.21.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.21.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.21.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.21.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.21.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.21.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.21.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.21.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.21.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.21.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.21.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.21.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.22.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.22.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.22.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.22.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.22.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.22.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.22.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.22.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.22.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.22.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.22.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.22.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.23.attention.self.query.weight
06/27 03:42:18 PM n: encoder.layer.23.attention.self.query.bias
06/27 03:42:18 PM n: encoder.layer.23.attention.self.key.weight
06/27 03:42:18 PM n: encoder.layer.23.attention.self.key.bias
06/27 03:42:18 PM n: encoder.layer.23.attention.self.value.weight
06/27 03:42:18 PM n: encoder.layer.23.attention.self.value.bias
06/27 03:42:18 PM n: encoder.layer.23.attention.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.23.attention.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: encoder.layer.23.intermediate.dense.weight
06/27 03:42:18 PM n: encoder.layer.23.intermediate.dense.bias
06/27 03:42:18 PM n: encoder.layer.23.output.dense.weight
06/27 03:42:18 PM n: encoder.layer.23.output.dense.bias
06/27 03:42:18 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 03:42:18 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 03:42:18 PM n: pooler.dense.weight
06/27 03:42:18 PM n: pooler.dense.bias
06/27 03:42:18 PM n: roberta.embeddings.word_embeddings.weight
06/27 03:42:18 PM n: roberta.embeddings.position_embeddings.weight
06/27 03:42:18 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 03:42:18 PM n: roberta.embeddings.LayerNorm.weight
06/27 03:42:18 PM n: roberta.embeddings.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 03:42:18 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 03:42:18 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 03:42:18 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 03:42:18 PM n: roberta.pooler.dense.weight
06/27 03:42:18 PM n: roberta.pooler.dense.bias
06/27 03:42:18 PM n: lm_head.bias
06/27 03:42:18 PM n: lm_head.dense.weight
06/27 03:42:18 PM n: lm_head.dense.bias
06/27 03:42:18 PM n: lm_head.layer_norm.weight
06/27 03:42:18 PM n: lm_head.layer_norm.bias
06/27 03:42:18 PM n: lm_head.decoder.weight
06/27 03:42:18 PM Total parameters: 763292761
06/27 03:42:18 PM ***** LOSS printing *****
06/27 03:42:18 PM loss
06/27 03:42:18 PM tensor(19.3731, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:19 PM ***** LOSS printing *****
06/27 03:42:19 PM loss
06/27 03:42:19 PM tensor(13.3621, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:19 PM ***** LOSS printing *****
06/27 03:42:19 PM loss
06/27 03:42:19 PM tensor(8.3472, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:19 PM ***** LOSS printing *****
06/27 03:42:19 PM loss
06/27 03:42:19 PM tensor(4.2664, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:19 PM ***** Running evaluation MLM *****
06/27 03:42:19 PM   Epoch = 0 iter 4 step
06/27 03:42:19 PM   Num examples = 16
06/27 03:42:19 PM   Batch size = 32
06/27 03:42:20 PM ***** Eval results *****
06/27 03:42:20 PM   acc = 0.5625
06/27 03:42:20 PM   cls_loss = 11.337189435958862
06/27 03:42:20 PM   eval_loss = 4.841754913330078
06/27 03:42:20 PM   global_step = 4
06/27 03:42:20 PM   loss = 11.337189435958862
06/27 03:42:20 PM ***** Save model *****
06/27 03:42:20 PM ***** Test Dataset Eval Result *****
06/27 03:43:23 PM ***** Eval results *****
06/27 03:43:23 PM   acc = 0.5955
06/27 03:43:23 PM   cls_loss = 11.337189435958862
06/27 03:43:23 PM   eval_loss = 4.838297745538136
06/27 03:43:23 PM   global_step = 4
06/27 03:43:23 PM   loss = 11.337189435958862
06/27 03:43:28 PM ***** LOSS printing *****
06/27 03:43:28 PM loss
06/27 03:43:28 PM tensor(4.0945, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:43:28 PM ***** LOSS printing *****
06/27 03:43:28 PM loss
06/27 03:43:28 PM tensor(3.6225, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:43:28 PM ***** LOSS printing *****
06/27 03:43:28 PM loss
06/27 03:43:28 PM tensor(3.5690, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:43:28 PM ***** LOSS printing *****
06/27 03:43:28 PM loss
06/27 03:43:28 PM tensor(4.0672, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:43:28 PM ***** LOSS printing *****
06/27 03:43:28 PM loss
06/27 03:43:28 PM tensor(5.5747, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:43:29 PM ***** Running evaluation MLM *****
06/27 03:43:29 PM   Epoch = 0 iter 9 step
06/27 03:43:29 PM   Num examples = 16
06/27 03:43:29 PM   Batch size = 32
06/27 03:43:29 PM ***** Eval results *****
06/27 03:43:29 PM   acc = 0.625
06/27 03:43:29 PM   cls_loss = 7.3640675279829235
06/27 03:43:29 PM   eval_loss = 2.787708044052124
06/27 03:43:29 PM   global_step = 9
06/27 03:43:29 PM   loss = 7.3640675279829235
06/27 03:43:29 PM ***** Save model *****
06/27 03:43:29 PM ***** Test Dataset Eval Result *****
06/27 03:44:33 PM ***** Eval results *****
06/27 03:44:33 PM   acc = 0.71
06/27 03:44:33 PM   cls_loss = 7.3640675279829235
06/27 03:44:33 PM   eval_loss = 2.885118619790153
06/27 03:44:33 PM   global_step = 9
06/27 03:44:33 PM   loss = 7.3640675279829235
06/27 03:44:37 PM ***** LOSS printing *****
06/27 03:44:37 PM loss
06/27 03:44:37 PM tensor(2.9684, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:44:37 PM ***** LOSS printing *****
06/27 03:44:37 PM loss
06/27 03:44:37 PM tensor(2.9075, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:44:37 PM ***** LOSS printing *****
06/27 03:44:37 PM loss
06/27 03:44:37 PM tensor(3.4704, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:44:38 PM ***** LOSS printing *****
06/27 03:44:38 PM loss
06/27 03:44:38 PM tensor(3.8949, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:44:38 PM ***** LOSS printing *****
06/27 03:44:38 PM loss
06/27 03:44:38 PM tensor(2.6595, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:44:38 PM ***** Running evaluation MLM *****
06/27 03:44:38 PM   Epoch = 1 iter 14 step
06/27 03:44:38 PM   Num examples = 16
06/27 03:44:38 PM   Batch size = 32
06/27 03:44:38 PM ***** Eval results *****
06/27 03:44:38 PM   acc = 0.875
06/27 03:44:38 PM   cls_loss = 3.277166485786438
06/27 03:44:38 PM   eval_loss = 1.0845205783843994
06/27 03:44:38 PM   global_step = 14
06/27 03:44:38 PM   loss = 3.277166485786438
06/27 03:44:38 PM ***** Save model *****
06/27 03:44:38 PM ***** Test Dataset Eval Result *****
06/27 03:45:42 PM ***** Eval results *****
06/27 03:45:42 PM   acc = 0.767
06/27 03:45:42 PM   cls_loss = 3.277166485786438
06/27 03:45:42 PM   eval_loss = 1.200425248297434
06/27 03:45:42 PM   global_step = 14
06/27 03:45:42 PM   loss = 3.277166485786438
06/27 03:45:46 PM ***** LOSS printing *****
06/27 03:45:46 PM loss
06/27 03:45:46 PM tensor(1.6090, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:46 PM ***** LOSS printing *****
06/27 03:45:46 PM loss
06/27 03:45:46 PM tensor(2.1389, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:46 PM ***** LOSS printing *****
06/27 03:45:46 PM loss
06/27 03:45:46 PM tensor(2.4059, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:46 PM ***** LOSS printing *****
06/27 03:45:46 PM loss
06/27 03:45:46 PM tensor(2.0902, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:47 PM ***** LOSS printing *****
06/27 03:45:47 PM loss
06/27 03:45:47 PM tensor(2.4029, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:47 PM ***** Running evaluation MLM *****
06/27 03:45:47 PM   Epoch = 1 iter 19 step
06/27 03:45:47 PM   Num examples = 16
06/27 03:45:47 PM   Batch size = 32
06/27 03:45:47 PM ***** Eval results *****
06/27 03:45:47 PM   acc = 0.875
06/27 03:45:47 PM   cls_loss = 2.457312447684152
06/27 03:45:47 PM   eval_loss = 2.8197455406188965
06/27 03:45:47 PM   global_step = 19
06/27 03:45:47 PM   loss = 2.457312447684152
06/27 03:45:47 PM ***** LOSS printing *****
06/27 03:45:47 PM loss
06/27 03:45:47 PM tensor(1.6733, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:48 PM ***** LOSS printing *****
06/27 03:45:48 PM loss
06/27 03:45:48 PM tensor(1.5233, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:48 PM ***** LOSS printing *****
06/27 03:45:48 PM loss
06/27 03:45:48 PM tensor(2.0730, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:48 PM ***** LOSS printing *****
06/27 03:45:48 PM loss
06/27 03:45:48 PM tensor(2.9705, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:48 PM ***** LOSS printing *****
06/27 03:45:48 PM loss
06/27 03:45:48 PM tensor(3.5913, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:48 PM ***** Running evaluation MLM *****
06/27 03:45:48 PM   Epoch = 1 iter 24 step
06/27 03:45:48 PM   Num examples = 16
06/27 03:45:48 PM   Batch size = 32
06/27 03:45:49 PM ***** Eval results *****
06/27 03:45:49 PM   acc = 0.5625
06/27 03:45:49 PM   cls_loss = 2.419386178255081
06/27 03:45:49 PM   eval_loss = 2.288508653640747
06/27 03:45:49 PM   global_step = 24
06/27 03:45:49 PM   loss = 2.419386178255081
06/27 03:45:49 PM ***** LOSS printing *****
06/27 03:45:49 PM loss
06/27 03:45:49 PM tensor(1.9589, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:49 PM ***** LOSS printing *****
06/27 03:45:49 PM loss
06/27 03:45:49 PM tensor(2.9226, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:49 PM ***** LOSS printing *****
06/27 03:45:49 PM loss
06/27 03:45:49 PM tensor(1.1554, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:50 PM ***** LOSS printing *****
06/27 03:45:50 PM loss
06/27 03:45:50 PM tensor(1.1021, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:50 PM ***** LOSS printing *****
06/27 03:45:50 PM loss
06/27 03:45:50 PM tensor(2.7697, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:50 PM ***** Running evaluation MLM *****
06/27 03:45:50 PM   Epoch = 2 iter 29 step
06/27 03:45:50 PM   Num examples = 16
06/27 03:45:50 PM   Batch size = 32
06/27 03:45:51 PM ***** Eval results *****
06/27 03:45:51 PM   acc = 0.875
06/27 03:45:51 PM   cls_loss = 1.9817448139190674
06/27 03:45:51 PM   eval_loss = 0.9315425157546997
06/27 03:45:51 PM   global_step = 29
06/27 03:45:51 PM   loss = 1.9817448139190674
06/27 03:45:51 PM ***** LOSS printing *****
06/27 03:45:51 PM loss
06/27 03:45:51 PM tensor(2.1630, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:51 PM ***** LOSS printing *****
06/27 03:45:51 PM loss
06/27 03:45:51 PM tensor(1.1563, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:51 PM ***** LOSS printing *****
06/27 03:45:51 PM loss
06/27 03:45:51 PM tensor(1.5204, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:51 PM ***** LOSS printing *****
06/27 03:45:51 PM loss
06/27 03:45:51 PM tensor(1.2418, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:51 PM ***** LOSS printing *****
06/27 03:45:51 PM loss
06/27 03:45:51 PM tensor(1.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:45:52 PM ***** Running evaluation MLM *****
06/27 03:45:52 PM   Epoch = 2 iter 34 step
06/27 03:45:52 PM   Num examples = 16
06/27 03:45:52 PM   Batch size = 32
06/27 03:45:52 PM ***** Eval results *****
06/27 03:45:52 PM   acc = 0.9375
06/27 03:45:52 PM   cls_loss = 1.706946063041687
06/27 03:45:52 PM   eval_loss = 2.1663708686828613
06/27 03:45:52 PM   global_step = 34
06/27 03:45:52 PM   loss = 1.706946063041687
06/27 03:45:52 PM ***** Save model *****
06/27 03:45:52 PM ***** Test Dataset Eval Result *****
06/27 03:46:55 PM ***** Eval results *****
06/27 03:46:55 PM   acc = 0.8725
06/27 03:46:55 PM   cls_loss = 1.706946063041687
06/27 03:46:55 PM   eval_loss = 2.1973201736571295
06/27 03:46:55 PM   global_step = 34
06/27 03:46:55 PM   loss = 1.706946063041687
06/27 03:47:00 PM ***** LOSS printing *****
06/27 03:47:00 PM loss
06/27 03:47:00 PM tensor(1.8095, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:00 PM ***** LOSS printing *****
06/27 03:47:00 PM loss
06/27 03:47:00 PM tensor(2.3896, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:00 PM ***** LOSS printing *****
06/27 03:47:00 PM loss
06/27 03:47:00 PM tensor(2.6778, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:00 PM ***** LOSS printing *****
06/27 03:47:00 PM loss
06/27 03:47:00 PM tensor(2.4488, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:00 PM ***** LOSS printing *****
06/27 03:47:00 PM loss
06/27 03:47:00 PM tensor(1.0205, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:01 PM ***** Running evaluation MLM *****
06/27 03:47:01 PM   Epoch = 3 iter 39 step
06/27 03:47:01 PM   Num examples = 16
06/27 03:47:01 PM   Batch size = 32
06/27 03:47:01 PM ***** Eval results *****
06/27 03:47:01 PM   acc = 0.9375
06/27 03:47:01 PM   cls_loss = 2.049018144607544
06/27 03:47:01 PM   eval_loss = 4.256996154785156
06/27 03:47:01 PM   global_step = 39
06/27 03:47:01 PM   loss = 2.049018144607544
06/27 03:47:01 PM ***** LOSS printing *****
06/27 03:47:01 PM loss
06/27 03:47:01 PM tensor(0.7696, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:01 PM ***** LOSS printing *****
06/27 03:47:01 PM loss
06/27 03:47:01 PM tensor(3.6298, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:02 PM ***** LOSS printing *****
06/27 03:47:02 PM loss
06/27 03:47:02 PM tensor(3.4471, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:02 PM ***** LOSS printing *****
06/27 03:47:02 PM loss
06/27 03:47:02 PM tensor(2.9930, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:02 PM ***** LOSS printing *****
06/27 03:47:02 PM loss
06/27 03:47:02 PM tensor(2.5701, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:02 PM ***** Running evaluation MLM *****
06/27 03:47:02 PM   Epoch = 3 iter 44 step
06/27 03:47:02 PM   Num examples = 16
06/27 03:47:02 PM   Batch size = 32
06/27 03:47:03 PM ***** Eval results *****
06/27 03:47:03 PM   acc = 0.875
06/27 03:47:03 PM   cls_loss = 2.4445987194776535
06/27 03:47:03 PM   eval_loss = 4.421393394470215
06/27 03:47:03 PM   global_step = 44
06/27 03:47:03 PM   loss = 2.4445987194776535
06/27 03:47:03 PM ***** LOSS printing *****
06/27 03:47:03 PM loss
06/27 03:47:03 PM tensor(2.9203, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:03 PM ***** LOSS printing *****
06/27 03:47:03 PM loss
06/27 03:47:03 PM tensor(1.4451, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:03 PM ***** LOSS printing *****
06/27 03:47:03 PM loss
06/27 03:47:03 PM tensor(1.5830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:03 PM ***** LOSS printing *****
06/27 03:47:03 PM loss
06/27 03:47:03 PM tensor(2.0390, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:04 PM ***** LOSS printing *****
06/27 03:47:04 PM loss
06/27 03:47:04 PM tensor(1.5820, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:04 PM ***** Running evaluation MLM *****
06/27 03:47:04 PM   Epoch = 4 iter 49 step
06/27 03:47:04 PM   Num examples = 16
06/27 03:47:04 PM   Batch size = 32
06/27 03:47:04 PM ***** Eval results *****
06/27 03:47:04 PM   acc = 0.75
06/27 03:47:04 PM   cls_loss = 1.5820167064666748
06/27 03:47:04 PM   eval_loss = 2.293973445892334
06/27 03:47:04 PM   global_step = 49
06/27 03:47:04 PM   loss = 1.5820167064666748
06/27 03:47:04 PM ***** LOSS printing *****
06/27 03:47:04 PM loss
06/27 03:47:04 PM tensor(1.5692, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:05 PM ***** LOSS printing *****
06/27 03:47:05 PM loss
06/27 03:47:05 PM tensor(0.9005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:05 PM ***** LOSS printing *****
06/27 03:47:05 PM loss
06/27 03:47:05 PM tensor(1.2458, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:05 PM ***** LOSS printing *****
06/27 03:47:05 PM loss
06/27 03:47:05 PM tensor(0.8084, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:05 PM ***** LOSS printing *****
06/27 03:47:05 PM loss
06/27 03:47:05 PM tensor(1.3998, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:05 PM ***** Running evaluation MLM *****
06/27 03:47:05 PM   Epoch = 4 iter 54 step
06/27 03:47:05 PM   Num examples = 16
06/27 03:47:05 PM   Batch size = 32
06/27 03:47:06 PM ***** Eval results *****
06/27 03:47:06 PM   acc = 0.9375
06/27 03:47:06 PM   cls_loss = 1.2509594957033794
06/27 03:47:06 PM   eval_loss = 1.4909392595291138
06/27 03:47:06 PM   global_step = 54
06/27 03:47:06 PM   loss = 1.2509594957033794
06/27 03:47:06 PM ***** LOSS printing *****
06/27 03:47:06 PM loss
06/27 03:47:06 PM tensor(1.2367, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:06 PM ***** LOSS printing *****
06/27 03:47:06 PM loss
06/27 03:47:06 PM tensor(2.3292, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:06 PM ***** LOSS printing *****
06/27 03:47:06 PM loss
06/27 03:47:06 PM tensor(1.9788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:07 PM ***** LOSS printing *****
06/27 03:47:07 PM loss
06/27 03:47:07 PM tensor(2.7745, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:07 PM ***** LOSS printing *****
06/27 03:47:07 PM loss
06/27 03:47:07 PM tensor(1.6718, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:07 PM ***** Running evaluation MLM *****
06/27 03:47:07 PM   Epoch = 4 iter 59 step
06/27 03:47:07 PM   Num examples = 16
06/27 03:47:07 PM   Batch size = 32
06/27 03:47:08 PM ***** Eval results *****
06/27 03:47:08 PM   acc = 0.9375
06/27 03:47:08 PM   cls_loss = 1.5906085209413008
06/27 03:47:08 PM   eval_loss = 0.7618217468261719
06/27 03:47:08 PM   global_step = 59
06/27 03:47:08 PM   loss = 1.5906085209413008
06/27 03:47:08 PM ***** LOSS printing *****
06/27 03:47:08 PM loss
06/27 03:47:08 PM tensor(1.5360, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:08 PM ***** LOSS printing *****
06/27 03:47:08 PM loss
06/27 03:47:08 PM tensor(1.1259, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:08 PM ***** LOSS printing *****
06/27 03:47:08 PM loss
06/27 03:47:08 PM tensor(1.4930, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:08 PM ***** LOSS printing *****
06/27 03:47:08 PM loss
06/27 03:47:08 PM tensor(1.1378, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:08 PM ***** LOSS printing *****
06/27 03:47:08 PM loss
06/27 03:47:08 PM tensor(1.4665, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:09 PM ***** Running evaluation MLM *****
06/27 03:47:09 PM   Epoch = 5 iter 64 step
06/27 03:47:09 PM   Num examples = 16
06/27 03:47:09 PM   Batch size = 32
06/27 03:47:09 PM ***** Eval results *****
06/27 03:47:09 PM   acc = 0.875
06/27 03:47:09 PM   cls_loss = 1.305827796459198
06/27 03:47:09 PM   eval_loss = 1.5102721452713013
06/27 03:47:09 PM   global_step = 64
06/27 03:47:09 PM   loss = 1.305827796459198
06/27 03:47:09 PM ***** LOSS printing *****
06/27 03:47:09 PM loss
06/27 03:47:09 PM tensor(1.4673, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:09 PM ***** LOSS printing *****
06/27 03:47:09 PM loss
06/27 03:47:09 PM tensor(1.7072, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:10 PM ***** LOSS printing *****
06/27 03:47:10 PM loss
06/27 03:47:10 PM tensor(1.5794, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:10 PM ***** LOSS printing *****
06/27 03:47:10 PM loss
06/27 03:47:10 PM tensor(0.9465, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:10 PM ***** LOSS printing *****
06/27 03:47:10 PM loss
06/27 03:47:10 PM tensor(1.7856, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:10 PM ***** Running evaluation MLM *****
06/27 03:47:10 PM   Epoch = 5 iter 69 step
06/27 03:47:10 PM   Num examples = 16
06/27 03:47:10 PM   Batch size = 32
06/27 03:47:11 PM ***** Eval results *****
06/27 03:47:11 PM   acc = 0.9375
06/27 03:47:11 PM   cls_loss = 1.4121421641773648
06/27 03:47:11 PM   eval_loss = 1.4964417219161987
06/27 03:47:11 PM   global_step = 69
06/27 03:47:11 PM   loss = 1.4121421641773648
06/27 03:47:11 PM ***** LOSS printing *****
06/27 03:47:11 PM loss
06/27 03:47:11 PM tensor(2.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:11 PM ***** LOSS printing *****
06/27 03:47:11 PM loss
06/27 03:47:11 PM tensor(1.0408, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:11 PM ***** LOSS printing *****
06/27 03:47:11 PM loss
06/27 03:47:11 PM tensor(1.3905, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:11 PM ***** LOSS printing *****
06/27 03:47:11 PM loss
06/27 03:47:11 PM tensor(1.1180, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:12 PM ***** LOSS printing *****
06/27 03:47:12 PM loss
06/27 03:47:12 PM tensor(1.2674, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:12 PM ***** Running evaluation MLM *****
06/27 03:47:12 PM   Epoch = 6 iter 74 step
06/27 03:47:12 PM   Num examples = 16
06/27 03:47:12 PM   Batch size = 32
06/27 03:47:12 PM ***** Eval results *****
06/27 03:47:12 PM   acc = 0.9375
06/27 03:47:12 PM   cls_loss = 1.1927013993263245
06/27 03:47:12 PM   eval_loss = 1.3663909435272217
06/27 03:47:12 PM   global_step = 74
06/27 03:47:12 PM   loss = 1.1927013993263245
06/27 03:47:12 PM ***** LOSS printing *****
06/27 03:47:12 PM loss
06/27 03:47:12 PM tensor(1.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:13 PM ***** LOSS printing *****
06/27 03:47:13 PM loss
06/27 03:47:13 PM tensor(1.2130, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:13 PM ***** LOSS printing *****
06/27 03:47:13 PM loss
06/27 03:47:13 PM tensor(1.4018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:13 PM ***** LOSS printing *****
06/27 03:47:13 PM loss
06/27 03:47:13 PM tensor(1.4865, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:13 PM ***** LOSS printing *****
06/27 03:47:13 PM loss
06/27 03:47:13 PM tensor(1.1858, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:13 PM ***** Running evaluation MLM *****
06/27 03:47:13 PM   Epoch = 6 iter 79 step
06/27 03:47:13 PM   Num examples = 16
06/27 03:47:13 PM   Batch size = 32
06/27 03:47:14 PM ***** Eval results *****
06/27 03:47:14 PM   acc = 0.9375
06/27 03:47:14 PM   cls_loss = 1.2460706744875227
06/27 03:47:14 PM   eval_loss = 1.334513783454895
06/27 03:47:14 PM   global_step = 79
06/27 03:47:14 PM   loss = 1.2460706744875227
06/27 03:47:14 PM ***** LOSS printing *****
06/27 03:47:14 PM loss
06/27 03:47:14 PM tensor(1.2222, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:14 PM ***** LOSS printing *****
06/27 03:47:14 PM loss
06/27 03:47:14 PM tensor(1.4417, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:14 PM ***** LOSS printing *****
06/27 03:47:14 PM loss
06/27 03:47:14 PM tensor(1.5188, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:15 PM ***** LOSS printing *****
06/27 03:47:15 PM loss
06/27 03:47:15 PM tensor(1.3790, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:15 PM ***** LOSS printing *****
06/27 03:47:15 PM loss
06/27 03:47:15 PM tensor(1.4084, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:15 PM ***** Running evaluation MLM *****
06/27 03:47:15 PM   Epoch = 6 iter 84 step
06/27 03:47:15 PM   Num examples = 16
06/27 03:47:15 PM   Batch size = 32
06/27 03:47:16 PM ***** Eval results *****
06/27 03:47:16 PM   acc = 0.9375
06/27 03:47:16 PM   cls_loss = 1.3077145318190257
06/27 03:47:16 PM   eval_loss = 1.0310994386672974
06/27 03:47:16 PM   global_step = 84
06/27 03:47:16 PM   loss = 1.3077145318190257
06/27 03:47:16 PM ***** LOSS printing *****
06/27 03:47:16 PM loss
06/27 03:47:16 PM tensor(0.9969, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:16 PM ***** LOSS printing *****
06/27 03:47:16 PM loss
06/27 03:47:16 PM tensor(1.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:16 PM ***** LOSS printing *****
06/27 03:47:16 PM loss
06/27 03:47:16 PM tensor(1.1466, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:16 PM ***** LOSS printing *****
06/27 03:47:16 PM loss
06/27 03:47:16 PM tensor(1.0961, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:17 PM ***** LOSS printing *****
06/27 03:47:17 PM loss
06/27 03:47:17 PM tensor(1.5735, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:17 PM ***** Running evaluation MLM *****
06/27 03:47:17 PM   Epoch = 7 iter 89 step
06/27 03:47:17 PM   Num examples = 16
06/27 03:47:17 PM   Batch size = 32
06/27 03:47:17 PM ***** Eval results *****
06/27 03:47:17 PM   acc = 0.9375
06/27 03:47:17 PM   cls_loss = 1.202959454059601
06/27 03:47:17 PM   eval_loss = 1.138803482055664
06/27 03:47:17 PM   global_step = 89
06/27 03:47:17 PM   loss = 1.202959454059601
06/27 03:47:17 PM ***** LOSS printing *****
06/27 03:47:17 PM loss
06/27 03:47:17 PM tensor(1.8413, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:18 PM ***** LOSS printing *****
06/27 03:47:18 PM loss
06/27 03:47:18 PM tensor(1.0762, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:18 PM ***** LOSS printing *****
06/27 03:47:18 PM loss
06/27 03:47:18 PM tensor(1.9499, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:18 PM ***** LOSS printing *****
06/27 03:47:18 PM loss
06/27 03:47:18 PM tensor(1.0315, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:18 PM ***** LOSS printing *****
06/27 03:47:18 PM loss
06/27 03:47:18 PM tensor(1.5126, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:47:18 PM ***** Running evaluation MLM *****
06/27 03:47:18 PM   Epoch = 7 iter 94 step
06/27 03:47:18 PM   Num examples = 16
06/27 03:47:18 PM   Batch size = 32
06/27 03:47:19 PM ***** Eval results *****
06/27 03:47:19 PM   acc = 1.0
06/27 03:47:19 PM   cls_loss = 1.3426312625408172
06/27 03:47:19 PM   eval_loss = 1.1143193244934082
06/27 03:47:19 PM   global_step = 94
06/27 03:47:19 PM   loss = 1.3426312625408172
06/27 03:47:19 PM ***** Save model *****
06/27 03:47:19 PM ***** Test Dataset Eval Result *****
06/27 03:48:22 PM ***** Eval results *****
06/27 03:48:22 PM   acc = 0.8825
06/27 03:48:22 PM   cls_loss = 1.3426312625408172
06/27 03:48:22 PM   eval_loss = 1.4783956919397627
06/27 03:48:22 PM   global_step = 94
06/27 03:48:22 PM   loss = 1.3426312625408172
06/27 03:48:26 PM ***** LOSS printing *****
06/27 03:48:26 PM loss
06/27 03:48:26 PM tensor(1.2719, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:26 PM ***** LOSS printing *****
06/27 03:48:26 PM loss
06/27 03:48:26 PM tensor(1.4168, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:26 PM ***** LOSS printing *****
06/27 03:48:26 PM loss
06/27 03:48:26 PM tensor(0.8537, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:27 PM ***** LOSS printing *****
06/27 03:48:27 PM loss
06/27 03:48:27 PM tensor(1.2202, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:27 PM ***** LOSS printing *****
06/27 03:48:27 PM loss
06/27 03:48:27 PM tensor(1.1332, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:27 PM ***** Running evaluation MLM *****
06/27 03:48:27 PM   Epoch = 8 iter 99 step
06/27 03:48:27 PM   Num examples = 16
06/27 03:48:27 PM   Batch size = 32
06/27 03:48:27 PM ***** Eval results *****
06/27 03:48:27 PM   acc = 1.0
06/27 03:48:27 PM   cls_loss = 1.0690492788950603
06/27 03:48:27 PM   eval_loss = 1.1811888217926025
06/27 03:48:27 PM   global_step = 99
06/27 03:48:27 PM   loss = 1.0690492788950603
06/27 03:48:27 PM ***** LOSS printing *****
06/27 03:48:27 PM loss
06/27 03:48:27 PM tensor(1.4456, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:28 PM ***** LOSS printing *****
06/27 03:48:28 PM loss
06/27 03:48:28 PM tensor(1.8686, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:28 PM ***** LOSS printing *****
06/27 03:48:28 PM loss
06/27 03:48:28 PM tensor(0.7364, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:28 PM ***** LOSS printing *****
06/27 03:48:28 PM loss
06/27 03:48:28 PM tensor(2.0949, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:28 PM ***** LOSS printing *****
06/27 03:48:28 PM loss
06/27 03:48:28 PM tensor(0.9854, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:29 PM ***** Running evaluation MLM *****
06/27 03:48:29 PM   Epoch = 8 iter 104 step
06/27 03:48:29 PM   Num examples = 16
06/27 03:48:29 PM   Batch size = 32
06/27 03:48:29 PM ***** Eval results *****
06/27 03:48:29 PM   acc = 0.9375
06/27 03:48:29 PM   cls_loss = 1.2922719344496727
06/27 03:48:29 PM   eval_loss = 1.06666100025177
06/27 03:48:29 PM   global_step = 104
06/27 03:48:29 PM   loss = 1.2922719344496727
06/27 03:48:29 PM ***** LOSS printing *****
06/27 03:48:29 PM loss
06/27 03:48:29 PM tensor(1.3228, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:29 PM ***** LOSS printing *****
06/27 03:48:29 PM loss
06/27 03:48:29 PM tensor(1.1785, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:30 PM ***** LOSS printing *****
06/27 03:48:30 PM loss
06/27 03:48:30 PM tensor(1.0960, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:30 PM ***** LOSS printing *****
06/27 03:48:30 PM loss
06/27 03:48:30 PM tensor(1.4498, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:30 PM ***** LOSS printing *****
06/27 03:48:30 PM loss
06/27 03:48:30 PM tensor(1.0647, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:30 PM ***** Running evaluation MLM *****
06/27 03:48:30 PM   Epoch = 9 iter 109 step
06/27 03:48:30 PM   Num examples = 16
06/27 03:48:30 PM   Batch size = 32
06/27 03:48:31 PM ***** Eval results *****
06/27 03:48:31 PM   acc = 0.9375
06/27 03:48:31 PM   cls_loss = 1.0647228956222534
06/27 03:48:31 PM   eval_loss = 1.4132195711135864
06/27 03:48:31 PM   global_step = 109
06/27 03:48:31 PM   loss = 1.0647228956222534
06/27 03:48:31 PM ***** LOSS printing *****
06/27 03:48:31 PM loss
06/27 03:48:31 PM tensor(1.0921, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:31 PM ***** LOSS printing *****
06/27 03:48:31 PM loss
06/27 03:48:31 PM tensor(1.6097, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:31 PM ***** LOSS printing *****
06/27 03:48:31 PM loss
06/27 03:48:31 PM tensor(1.0840, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:31 PM ***** LOSS printing *****
06/27 03:48:31 PM loss
06/27 03:48:31 PM tensor(1.3790, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:32 PM ***** LOSS printing *****
06/27 03:48:32 PM loss
06/27 03:48:32 PM tensor(1.0400, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:32 PM ***** Running evaluation MLM *****
06/27 03:48:32 PM   Epoch = 9 iter 114 step
06/27 03:48:32 PM   Num examples = 16
06/27 03:48:32 PM   Batch size = 32
06/27 03:48:32 PM ***** Eval results *****
06/27 03:48:32 PM   acc = 0.9375
06/27 03:48:32 PM   cls_loss = 1.2115790049235027
06/27 03:48:32 PM   eval_loss = 0.9501615762710571
06/27 03:48:32 PM   global_step = 114
06/27 03:48:32 PM   loss = 1.2115790049235027
06/27 03:48:32 PM ***** LOSS printing *****
06/27 03:48:32 PM loss
06/27 03:48:32 PM tensor(1.0648, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:33 PM ***** LOSS printing *****
06/27 03:48:33 PM loss
06/27 03:48:33 PM tensor(1.7105, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:33 PM ***** LOSS printing *****
06/27 03:48:33 PM loss
06/27 03:48:33 PM tensor(0.9752, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:33 PM ***** LOSS printing *****
06/27 03:48:33 PM loss
06/27 03:48:33 PM tensor(2.1699, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:33 PM ***** LOSS printing *****
06/27 03:48:33 PM loss
06/27 03:48:33 PM tensor(1.3932, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:48:33 PM ***** Running evaluation MLM *****
06/27 03:48:33 PM   Epoch = 9 iter 119 step
06/27 03:48:33 PM   Num examples = 16
06/27 03:48:33 PM   Batch size = 32
06/27 03:48:34 PM ***** Eval results *****
06/27 03:48:34 PM   acc = 0.9375
06/27 03:48:34 PM   cls_loss = 1.325733022256331
06/27 03:48:34 PM   eval_loss = 0.8822991251945496
06/27 03:48:34 PM   global_step = 119
06/27 03:48:34 PM   loss = 1.325733022256331
06/27 03:48:34 PM ***** LOSS printing *****
06/27 03:48:34 PM loss
06/27 03:48:34 PM tensor(2.4853, device='cuda:0', grad_fn=<NllLossBackward0>)

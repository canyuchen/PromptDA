06/27 03:59:22 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-13/', data_seed=13, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 03:59:22 PM device: cuda n_gpu: 1
06/27 03:59:23 PM Writing example 0 of 48
06/27 03:59:23 PM *** Example ***
06/27 03:59:23 PM guid: train-1
06/27 03:59:23 PM tokens: <s> this Ġis Ġthe Ġstory Ġof Ġnat Ġbanks Ġ, Ġan Ġ8 th Ġgeneration Ġvirgin ian Ġgentleman Ġfarmer Ġliving Ġin Ġthe Ġpast Ġ, Ġwho Ġloses Ġhis Ġfamily Ġfarm Ġ, Ġgreen wood Ġ, Ġto Ġa Ġpair Ġof Ġland Ġspec ulators Ġfrom Ġwashing ton Ġ, Ġd Ġ. Ġc Ġ. </s> ĠIt Ġis <mask>
06/27 03:59:23 PM input_ids: 0 9226 16 5 527 9 23577 1520 2156 41 290 212 2706 33799 811 22164 10305 1207 11 5 375 2156 54 13585 39 284 3380 2156 2272 1845 2156 7 10 1763 9 1212 12002 17810 31 14784 1054 2156 385 479 740 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM label: ['Ġepic']
06/27 03:59:23 PM Writing example 0 of 16
06/27 03:59:23 PM *** Example ***
06/27 03:59:23 PM guid: dev-1
06/27 03:59:23 PM tokens: <s> he Ġis Ġstill Ġfamous Ġ, Ġalthough Ġstill Ġdisliked Ġby Ġsn ape Ġ, Ġmalf oy Ġ, Ġand Ġthe Ġrest Ġof Ġthe Ġsly ther ins Ġ. </s> ĠIt Ġis <mask>
06/27 03:59:23 PM input_ids: 0 700 16 202 3395 2156 1712 202 40891 30 4543 5776 2156 36432 2160 2156 8 5 1079 9 5 40568 12968 1344 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM label: ['Ġepic']
06/27 03:59:23 PM Writing example 0 of 2000
06/27 03:59:23 PM *** Example ***
06/27 03:59:23 PM guid: dev-1
06/27 03:59:23 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 03:59:23 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:59:23 PM label: ['Ġmadness']
06/27 03:59:36 PM ***** Running training *****
06/27 03:59:36 PM   Num examples = 48
06/27 03:59:36 PM   Batch size = 4
06/27 03:59:36 PM   Num steps = 120
06/27 03:59:36 PM n: embeddings.word_embeddings.weight
06/27 03:59:36 PM n: embeddings.position_embeddings.weight
06/27 03:59:36 PM n: embeddings.token_type_embeddings.weight
06/27 03:59:36 PM n: embeddings.LayerNorm.weight
06/27 03:59:36 PM n: embeddings.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.0.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.0.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.0.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.0.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.0.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.0.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.0.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.0.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.0.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.0.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.0.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.0.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.1.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.1.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.1.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.1.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.1.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.1.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.1.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.1.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.1.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.1.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.1.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.1.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.2.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.2.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.2.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.2.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.2.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.2.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.2.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.2.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.2.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.2.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.2.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.2.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.3.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.3.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.3.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.3.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.3.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.3.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.3.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.3.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.3.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.3.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.3.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.3.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.4.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.4.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.4.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.4.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.4.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.4.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.4.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.4.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.4.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.4.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.4.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.4.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.5.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.5.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.5.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.5.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.5.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.5.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.5.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.5.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.5.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.5.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.5.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.5.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.6.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.6.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.6.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.6.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.6.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.6.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.6.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.6.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.6.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.6.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.6.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.6.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.7.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.7.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.7.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.7.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.7.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.7.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.7.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.7.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.7.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.7.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.7.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.7.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.8.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.8.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.8.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.8.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.8.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.8.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.8.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.8.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.8.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.8.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.8.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.8.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.9.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.9.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.9.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.9.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.9.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.9.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.9.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.9.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.9.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.9.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.9.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.9.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.10.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.10.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.10.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.10.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.10.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.10.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.10.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.10.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.10.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.10.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.10.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.10.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.11.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.11.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.11.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.11.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.11.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.11.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.11.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.11.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.11.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.11.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.11.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.11.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.12.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.12.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.12.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.12.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.12.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.12.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.12.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.12.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.12.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.12.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.12.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.12.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.13.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.13.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.13.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.13.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.13.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.13.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.13.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.13.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.13.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.13.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.13.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.13.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.14.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.14.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.14.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.14.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.14.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.14.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.14.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.14.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.14.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.14.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.14.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.14.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.15.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.15.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.15.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.15.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.15.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.15.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.15.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.15.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.15.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.15.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.15.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.15.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.16.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.16.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.16.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.16.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.16.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.16.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.16.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.16.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.16.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.16.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.16.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.16.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.17.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.17.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.17.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.17.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.17.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.17.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.17.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.17.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.17.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.17.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.17.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.17.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.18.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.18.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.18.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.18.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.18.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.18.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.18.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.18.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.18.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.18.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.18.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.18.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.19.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.19.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.19.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.19.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.19.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.19.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.19.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.19.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.19.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.19.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.19.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.19.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.20.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.20.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.20.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.20.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.20.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.20.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.20.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.20.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.20.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.20.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.20.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.20.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.21.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.21.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.21.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.21.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.21.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.21.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.21.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.21.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.21.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.21.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.21.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.21.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.22.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.22.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.22.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.22.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.22.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.22.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.22.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.22.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.22.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.22.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.22.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.22.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.23.attention.self.query.weight
06/27 03:59:36 PM n: encoder.layer.23.attention.self.query.bias
06/27 03:59:36 PM n: encoder.layer.23.attention.self.key.weight
06/27 03:59:36 PM n: encoder.layer.23.attention.self.key.bias
06/27 03:59:36 PM n: encoder.layer.23.attention.self.value.weight
06/27 03:59:36 PM n: encoder.layer.23.attention.self.value.bias
06/27 03:59:36 PM n: encoder.layer.23.attention.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.23.attention.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: encoder.layer.23.intermediate.dense.weight
06/27 03:59:36 PM n: encoder.layer.23.intermediate.dense.bias
06/27 03:59:36 PM n: encoder.layer.23.output.dense.weight
06/27 03:59:36 PM n: encoder.layer.23.output.dense.bias
06/27 03:59:36 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 03:59:36 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 03:59:36 PM n: pooler.dense.weight
06/27 03:59:36 PM n: pooler.dense.bias
06/27 03:59:36 PM n: roberta.embeddings.word_embeddings.weight
06/27 03:59:36 PM n: roberta.embeddings.position_embeddings.weight
06/27 03:59:36 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 03:59:36 PM n: roberta.embeddings.LayerNorm.weight
06/27 03:59:36 PM n: roberta.embeddings.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 03:59:36 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 03:59:36 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 03:59:36 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 03:59:36 PM n: roberta.pooler.dense.weight
06/27 03:59:36 PM n: roberta.pooler.dense.bias
06/27 03:59:36 PM n: lm_head.bias
06/27 03:59:36 PM n: lm_head.dense.weight
06/27 03:59:36 PM n: lm_head.dense.bias
06/27 03:59:36 PM n: lm_head.layer_norm.weight
06/27 03:59:36 PM n: lm_head.layer_norm.bias
06/27 03:59:36 PM n: lm_head.decoder.weight
06/27 03:59:36 PM Total parameters: 763292761
06/27 03:59:36 PM ***** LOSS printing *****
06/27 03:59:36 PM loss
06/27 03:59:36 PM tensor(17.8759, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:36 PM ***** LOSS printing *****
06/27 03:59:36 PM loss
06/27 03:59:36 PM tensor(16.9353, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:37 PM ***** LOSS printing *****
06/27 03:59:37 PM loss
06/27 03:59:37 PM tensor(9.9281, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:37 PM ***** LOSS printing *****
06/27 03:59:37 PM loss
06/27 03:59:37 PM tensor(7.3283, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:37 PM ***** Running evaluation MLM *****
06/27 03:59:37 PM   Epoch = 0 iter 4 step
06/27 03:59:37 PM   Num examples = 16
06/27 03:59:37 PM   Batch size = 32
06/27 03:59:38 PM ***** Eval results *****
06/27 03:59:38 PM   acc = 0.5
06/27 03:59:38 PM   cls_loss = 13.016901969909668
06/27 03:59:38 PM   eval_loss = 9.19159984588623
06/27 03:59:38 PM   global_step = 4
06/27 03:59:38 PM   loss = 13.016901969909668
06/27 03:59:38 PM ***** Save model *****
06/27 03:59:38 PM ***** Test Dataset Eval Result *****
06/27 04:00:40 PM ***** Eval results *****
06/27 04:00:40 PM   acc = 0.5
06/27 04:00:40 PM   cls_loss = 13.016901969909668
06/27 04:00:40 PM   eval_loss = 9.212402767605251
06/27 04:00:40 PM   global_step = 4
06/27 04:00:40 PM   loss = 13.016901969909668
06/27 04:00:44 PM ***** LOSS printing *****
06/27 04:00:44 PM loss
06/27 04:00:44 PM tensor(5.5842, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:00:45 PM ***** LOSS printing *****
06/27 04:00:45 PM loss
06/27 04:00:45 PM tensor(4.4347, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:00:45 PM ***** LOSS printing *****
06/27 04:00:45 PM loss
06/27 04:00:45 PM tensor(3.9849, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:00:45 PM ***** LOSS printing *****
06/27 04:00:45 PM loss
06/27 04:00:45 PM tensor(5.4282, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:00:45 PM ***** LOSS printing *****
06/27 04:00:45 PM loss
06/27 04:00:45 PM tensor(5.7097, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:00:45 PM ***** Running evaluation MLM *****
06/27 04:00:45 PM   Epoch = 0 iter 9 step
06/27 04:00:45 PM   Num examples = 16
06/27 04:00:45 PM   Batch size = 32
06/27 04:00:46 PM ***** Eval results *****
06/27 04:00:46 PM   acc = 0.625
06/27 04:00:46 PM   cls_loss = 8.578824016782972
06/27 04:00:46 PM   eval_loss = 3.1224441528320312
06/27 04:00:46 PM   global_step = 9
06/27 04:00:46 PM   loss = 8.578824016782972
06/27 04:00:46 PM ***** Save model *****
06/27 04:00:46 PM ***** Test Dataset Eval Result *****
06/27 04:01:49 PM ***** Eval results *****
06/27 04:01:49 PM   acc = 0.4795
06/27 04:01:49 PM   cls_loss = 8.578824016782972
06/27 04:01:49 PM   eval_loss = 3.1952194808021424
06/27 04:01:49 PM   global_step = 9
06/27 04:01:49 PM   loss = 8.578824016782972
06/27 04:01:53 PM ***** LOSS printing *****
06/27 04:01:53 PM loss
06/27 04:01:53 PM tensor(3.6311, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:53 PM ***** LOSS printing *****
06/27 04:01:53 PM loss
06/27 04:01:53 PM tensor(3.8797, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:53 PM ***** LOSS printing *****
06/27 04:01:53 PM loss
06/27 04:01:53 PM tensor(4.6763, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:54 PM ***** LOSS printing *****
06/27 04:01:54 PM loss
06/27 04:01:54 PM tensor(2.5542, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:54 PM ***** LOSS printing *****
06/27 04:01:54 PM loss
06/27 04:01:54 PM tensor(3.0961, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:54 PM ***** Running evaluation MLM *****
06/27 04:01:54 PM   Epoch = 1 iter 14 step
06/27 04:01:54 PM   Num examples = 16
06/27 04:01:54 PM   Batch size = 32
06/27 04:01:55 PM ***** Eval results *****
06/27 04:01:55 PM   acc = 0.625
06/27 04:01:55 PM   cls_loss = 2.8251609802246094
06/27 04:01:55 PM   eval_loss = 2.3137576580047607
06/27 04:01:55 PM   global_step = 14
06/27 04:01:55 PM   loss = 2.8251609802246094
06/27 04:01:55 PM ***** LOSS printing *****
06/27 04:01:55 PM loss
06/27 04:01:55 PM tensor(1.6492, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:55 PM ***** LOSS printing *****
06/27 04:01:55 PM loss
06/27 04:01:55 PM tensor(2.1290, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:55 PM ***** LOSS printing *****
06/27 04:01:55 PM loss
06/27 04:01:55 PM tensor(3.9730, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:55 PM ***** LOSS printing *****
06/27 04:01:55 PM loss
06/27 04:01:55 PM tensor(2.8255, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:55 PM ***** LOSS printing *****
06/27 04:01:55 PM loss
06/27 04:01:55 PM tensor(2.4925, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:01:56 PM ***** Running evaluation MLM *****
06/27 04:01:56 PM   Epoch = 1 iter 19 step
06/27 04:01:56 PM   Num examples = 16
06/27 04:01:56 PM   Batch size = 32
06/27 04:01:56 PM ***** Eval results *****
06/27 04:01:56 PM   acc = 0.6875
06/27 04:01:56 PM   cls_loss = 2.6742365530558994
06/27 04:01:56 PM   eval_loss = 4.807344913482666
06/27 04:01:56 PM   global_step = 19
06/27 04:01:56 PM   loss = 2.6742365530558994
06/27 04:01:56 PM ***** Save model *****
06/27 04:01:56 PM ***** Test Dataset Eval Result *****
06/27 04:02:59 PM ***** Eval results *****
06/27 04:02:59 PM   acc = 0.7175
06/27 04:02:59 PM   cls_loss = 2.6742365530558994
06/27 04:02:59 PM   eval_loss = 4.953658505091592
06/27 04:02:59 PM   global_step = 19
06/27 04:02:59 PM   loss = 2.6742365530558994
06/27 04:03:03 PM ***** LOSS printing *****
06/27 04:03:03 PM loss
06/27 04:03:03 PM tensor(1.9556, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:03 PM ***** LOSS printing *****
06/27 04:03:03 PM loss
06/27 04:03:03 PM tensor(1.7167, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:04 PM ***** LOSS printing *****
06/27 04:03:04 PM loss
06/27 04:03:04 PM tensor(2.1460, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:04 PM ***** LOSS printing *****
06/27 04:03:04 PM loss
06/27 04:03:04 PM tensor(3.8910, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:04 PM ***** LOSS printing *****
06/27 04:03:04 PM loss
06/27 04:03:04 PM tensor(4.8941, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:04 PM ***** Running evaluation MLM *****
06/27 04:03:04 PM   Epoch = 1 iter 24 step
06/27 04:03:04 PM   Num examples = 16
06/27 04:03:04 PM   Batch size = 32
06/27 04:03:05 PM ***** Eval results *****
06/27 04:03:05 PM   acc = 0.5
06/27 04:03:05 PM   cls_loss = 2.7769260903199515
06/27 04:03:05 PM   eval_loss = 3.1685173511505127
06/27 04:03:05 PM   global_step = 24
06/27 04:03:05 PM   loss = 2.7769260903199515
06/27 04:03:05 PM ***** LOSS printing *****
06/27 04:03:05 PM loss
06/27 04:03:05 PM tensor(2.7775, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:05 PM ***** LOSS printing *****
06/27 04:03:05 PM loss
06/27 04:03:05 PM tensor(2.6942, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:05 PM ***** LOSS printing *****
06/27 04:03:05 PM loss
06/27 04:03:05 PM tensor(1.4611, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:05 PM ***** LOSS printing *****
06/27 04:03:05 PM loss
06/27 04:03:05 PM tensor(1.3471, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:06 PM ***** LOSS printing *****
06/27 04:03:06 PM loss
06/27 04:03:06 PM tensor(1.7384, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:03:06 PM ***** Running evaluation MLM *****
06/27 04:03:06 PM   Epoch = 2 iter 29 step
06/27 04:03:06 PM   Num examples = 16
06/27 04:03:06 PM   Batch size = 32
06/27 04:03:06 PM ***** Eval results *****
06/27 04:03:06 PM   acc = 0.75
06/27 04:03:06 PM   cls_loss = 2.0036696910858156
06/27 04:03:06 PM   eval_loss = 1.3560731410980225
06/27 04:03:06 PM   global_step = 29
06/27 04:03:06 PM   loss = 2.0036696910858156
06/27 04:03:06 PM ***** Save model *****
06/27 04:03:06 PM ***** Test Dataset Eval Result *****
06/27 04:04:09 PM ***** Eval results *****
06/27 04:04:09 PM   acc = 0.7005
06/27 04:04:09 PM   cls_loss = 2.0036696910858156
06/27 04:04:09 PM   eval_loss = 1.5355268120765686
06/27 04:04:09 PM   global_step = 29
06/27 04:04:09 PM   loss = 2.0036696910858156
06/27 04:04:13 PM ***** LOSS printing *****
06/27 04:04:13 PM loss
06/27 04:04:13 PM tensor(2.3970, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:04:14 PM ***** LOSS printing *****
06/27 04:04:14 PM loss
06/27 04:04:14 PM tensor(1.0870, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:04:14 PM ***** LOSS printing *****
06/27 04:04:14 PM loss
06/27 04:04:14 PM tensor(2.2627, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:04:14 PM ***** LOSS printing *****
06/27 04:04:14 PM loss
06/27 04:04:14 PM tensor(2.2576, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:04:14 PM ***** LOSS printing *****
06/27 04:04:14 PM loss
06/27 04:04:14 PM tensor(1.8016, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:04:14 PM ***** Running evaluation MLM *****
06/27 04:04:14 PM   Epoch = 2 iter 34 step
06/27 04:04:14 PM   Num examples = 16
06/27 04:04:14 PM   Batch size = 32
06/27 04:04:15 PM ***** Eval results *****
06/27 04:04:15 PM   acc = 0.9375
06/27 04:04:15 PM   cls_loss = 1.982423484325409
06/27 04:04:15 PM   eval_loss = 1.830142855644226
06/27 04:04:15 PM   global_step = 34
06/27 04:04:15 PM   loss = 1.982423484325409
06/27 04:04:15 PM ***** Save model *****
06/27 04:04:15 PM ***** Test Dataset Eval Result *****
06/27 04:05:18 PM ***** Eval results *****
06/27 04:05:18 PM   acc = 0.816
06/27 04:05:18 PM   cls_loss = 1.982423484325409
06/27 04:05:18 PM   eval_loss = 2.00514953287821
06/27 04:05:18 PM   global_step = 34
06/27 04:05:18 PM   loss = 1.982423484325409
06/27 04:05:22 PM ***** LOSS printing *****
06/27 04:05:22 PM loss
06/27 04:05:22 PM tensor(1.4350, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:22 PM ***** LOSS printing *****
06/27 04:05:22 PM loss
06/27 04:05:22 PM tensor(1.3892, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:22 PM ***** LOSS printing *****
06/27 04:05:22 PM loss
06/27 04:05:22 PM tensor(1.8992, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:22 PM ***** LOSS printing *****
06/27 04:05:22 PM loss
06/27 04:05:22 PM tensor(2.2075, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:23 PM ***** LOSS printing *****
06/27 04:05:23 PM loss
06/27 04:05:23 PM tensor(2.6951, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:23 PM ***** Running evaluation MLM *****
06/27 04:05:23 PM   Epoch = 3 iter 39 step
06/27 04:05:23 PM   Num examples = 16
06/27 04:05:23 PM   Batch size = 32
06/27 04:05:23 PM ***** Eval results *****
06/27 04:05:23 PM   acc = 0.8125
06/27 04:05:23 PM   cls_loss = 2.2672719160715737
06/27 04:05:23 PM   eval_loss = 2.951986312866211
06/27 04:05:23 PM   global_step = 39
06/27 04:05:23 PM   loss = 2.2672719160715737
06/27 04:05:23 PM ***** LOSS printing *****
06/27 04:05:23 PM loss
06/27 04:05:23 PM tensor(2.4954, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:24 PM ***** LOSS printing *****
06/27 04:05:24 PM loss
06/27 04:05:24 PM tensor(2.0375, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:24 PM ***** LOSS printing *****
06/27 04:05:24 PM loss
06/27 04:05:24 PM tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:24 PM ***** LOSS printing *****
06/27 04:05:24 PM loss
06/27 04:05:24 PM tensor(1.3240, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:24 PM ***** LOSS printing *****
06/27 04:05:24 PM loss
06/27 04:05:24 PM tensor(1.4539, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:24 PM ***** Running evaluation MLM *****
06/27 04:05:24 PM   Epoch = 3 iter 44 step
06/27 04:05:24 PM   Num examples = 16
06/27 04:05:24 PM   Batch size = 32
06/27 04:05:25 PM ***** Eval results *****
06/27 04:05:25 PM   acc = 0.9375
06/27 04:05:25 PM   cls_loss = 2.0074357241392136
06/27 04:05:25 PM   eval_loss = 1.5471820831298828
06/27 04:05:25 PM   global_step = 44
06/27 04:05:25 PM   loss = 2.0074357241392136
06/27 04:05:25 PM ***** LOSS printing *****
06/27 04:05:25 PM loss
06/27 04:05:25 PM tensor(1.8369, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:25 PM ***** LOSS printing *****
06/27 04:05:25 PM loss
06/27 04:05:25 PM tensor(1.2433, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:25 PM ***** LOSS printing *****
06/27 04:05:25 PM loss
06/27 04:05:25 PM tensor(2.4256, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:26 PM ***** LOSS printing *****
06/27 04:05:26 PM loss
06/27 04:05:26 PM tensor(1.6626, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:26 PM ***** LOSS printing *****
06/27 04:05:26 PM loss
06/27 04:05:26 PM tensor(1.8914, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:26 PM ***** Running evaluation MLM *****
06/27 04:05:26 PM   Epoch = 4 iter 49 step
06/27 04:05:26 PM   Num examples = 16
06/27 04:05:26 PM   Batch size = 32
06/27 04:05:27 PM ***** Eval results *****
06/27 04:05:27 PM   acc = 0.75
06/27 04:05:27 PM   cls_loss = 1.8913521766662598
06/27 04:05:27 PM   eval_loss = 1.9410003423690796
06/27 04:05:27 PM   global_step = 49
06/27 04:05:27 PM   loss = 1.8913521766662598
06/27 04:05:27 PM ***** LOSS printing *****
06/27 04:05:27 PM loss
06/27 04:05:27 PM tensor(1.5803, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:27 PM ***** LOSS printing *****
06/27 04:05:27 PM loss
06/27 04:05:27 PM tensor(1.9386, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:27 PM ***** LOSS printing *****
06/27 04:05:27 PM loss
06/27 04:05:27 PM tensor(1.5632, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:27 PM ***** LOSS printing *****
06/27 04:05:27 PM loss
06/27 04:05:27 PM tensor(1.6506, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:27 PM ***** LOSS printing *****
06/27 04:05:27 PM loss
06/27 04:05:27 PM tensor(1.0142, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:28 PM ***** Running evaluation MLM *****
06/27 04:05:28 PM   Epoch = 4 iter 54 step
06/27 04:05:28 PM   Num examples = 16
06/27 04:05:28 PM   Batch size = 32
06/27 04:05:28 PM ***** Eval results *****
06/27 04:05:28 PM   acc = 0.9375
06/27 04:05:28 PM   cls_loss = 1.6063746213912964
06/27 04:05:28 PM   eval_loss = 1.1899070739746094
06/27 04:05:28 PM   global_step = 54
06/27 04:05:28 PM   loss = 1.6063746213912964
06/27 04:05:28 PM ***** LOSS printing *****
06/27 04:05:28 PM loss
06/27 04:05:28 PM tensor(2.1521, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:28 PM ***** LOSS printing *****
06/27 04:05:28 PM loss
06/27 04:05:28 PM tensor(0.9494, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:29 PM ***** LOSS printing *****
06/27 04:05:29 PM loss
06/27 04:05:29 PM tensor(2.7237, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:29 PM ***** LOSS printing *****
06/27 04:05:29 PM loss
06/27 04:05:29 PM tensor(1.2762, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:29 PM ***** LOSS printing *****
06/27 04:05:29 PM loss
06/27 04:05:29 PM tensor(1.7637, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:05:29 PM ***** Running evaluation MLM *****
06/27 04:05:29 PM   Epoch = 4 iter 59 step
06/27 04:05:29 PM   Num examples = 16
06/27 04:05:29 PM   Batch size = 32
06/27 04:05:30 PM ***** Eval results *****
06/27 04:05:30 PM   acc = 1.0
06/27 04:05:30 PM   cls_loss = 1.682126056064259
06/27 04:05:30 PM   eval_loss = 1.228616714477539
06/27 04:05:30 PM   global_step = 59
06/27 04:05:30 PM   loss = 1.682126056064259
06/27 04:05:30 PM ***** Save model *****
06/27 04:05:30 PM ***** Test Dataset Eval Result *****
06/27 04:06:32 PM ***** Eval results *****
06/27 04:06:32 PM   acc = 0.868
06/27 04:06:32 PM   cls_loss = 1.682126056064259
06/27 04:06:32 PM   eval_loss = 1.3484419073377336
06/27 04:06:32 PM   global_step = 59
06/27 04:06:32 PM   loss = 1.682126056064259
06/27 04:06:37 PM ***** LOSS printing *****
06/27 04:06:37 PM loss
06/27 04:06:37 PM tensor(1.6313, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:37 PM ***** LOSS printing *****
06/27 04:06:37 PM loss
06/27 04:06:37 PM tensor(1.3005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:37 PM ***** LOSS printing *****
06/27 04:06:37 PM loss
06/27 04:06:37 PM tensor(1.1997, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:37 PM ***** LOSS printing *****
06/27 04:06:37 PM loss
06/27 04:06:37 PM tensor(1.4877, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:37 PM ***** LOSS printing *****
06/27 04:06:37 PM loss
06/27 04:06:37 PM tensor(1.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:38 PM ***** Running evaluation MLM *****
06/27 04:06:38 PM   Epoch = 5 iter 64 step
06/27 04:06:38 PM   Num examples = 16
06/27 04:06:38 PM   Batch size = 32
06/27 04:06:38 PM ***** Eval results *****
06/27 04:06:38 PM   acc = 1.0
06/27 04:06:38 PM   cls_loss = 1.254237711429596
06/27 04:06:38 PM   eval_loss = 1.2570428848266602
06/27 04:06:38 PM   global_step = 64
06/27 04:06:38 PM   loss = 1.254237711429596
06/27 04:06:38 PM ***** LOSS printing *****
06/27 04:06:38 PM loss
06/27 04:06:38 PM tensor(1.2376, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:38 PM ***** LOSS printing *****
06/27 04:06:38 PM loss
06/27 04:06:38 PM tensor(0.9494, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:39 PM ***** LOSS printing *****
06/27 04:06:39 PM loss
06/27 04:06:39 PM tensor(1.7135, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:39 PM ***** LOSS printing *****
06/27 04:06:39 PM loss
06/27 04:06:39 PM tensor(2.6405, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:39 PM ***** LOSS printing *****
06/27 04:06:39 PM loss
06/27 04:06:39 PM tensor(1.6953, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:39 PM ***** Running evaluation MLM *****
06/27 04:06:39 PM   Epoch = 5 iter 69 step
06/27 04:06:39 PM   Num examples = 16
06/27 04:06:39 PM   Batch size = 32
06/27 04:06:40 PM ***** Eval results *****
06/27 04:06:40 PM   acc = 1.0
06/27 04:06:40 PM   cls_loss = 1.4725815918710496
06/27 04:06:40 PM   eval_loss = 1.9590868949890137
06/27 04:06:40 PM   global_step = 69
06/27 04:06:40 PM   loss = 1.4725815918710496
06/27 04:06:40 PM ***** LOSS printing *****
06/27 04:06:40 PM loss
06/27 04:06:40 PM tensor(0.8997, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:40 PM ***** LOSS printing *****
06/27 04:06:40 PM loss
06/27 04:06:40 PM tensor(1.2596, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:40 PM ***** LOSS printing *****
06/27 04:06:40 PM loss
06/27 04:06:40 PM tensor(2.1180, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:40 PM ***** LOSS printing *****
06/27 04:06:40 PM loss
06/27 04:06:40 PM tensor(0.9467, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:41 PM ***** LOSS printing *****
06/27 04:06:41 PM loss
06/27 04:06:41 PM tensor(1.0630, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:41 PM ***** Running evaluation MLM *****
06/27 04:06:41 PM   Epoch = 6 iter 74 step
06/27 04:06:41 PM   Num examples = 16
06/27 04:06:41 PM   Batch size = 32
06/27 04:06:41 PM ***** Eval results *****
06/27 04:06:41 PM   acc = 0.75
06/27 04:06:41 PM   cls_loss = 1.0048701167106628
06/27 04:06:41 PM   eval_loss = 2.243102550506592
06/27 04:06:41 PM   global_step = 74
06/27 04:06:41 PM   loss = 1.0048701167106628
06/27 04:06:41 PM ***** LOSS printing *****
06/27 04:06:41 PM loss
06/27 04:06:41 PM tensor(2.1890, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:42 PM ***** LOSS printing *****
06/27 04:06:42 PM loss
06/27 04:06:42 PM tensor(1.8278, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:42 PM ***** LOSS printing *****
06/27 04:06:42 PM loss
06/27 04:06:42 PM tensor(1.7963, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:42 PM ***** LOSS printing *****
06/27 04:06:42 PM loss
06/27 04:06:42 PM tensor(1.7916, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:42 PM ***** LOSS printing *****
06/27 04:06:42 PM loss
06/27 04:06:42 PM tensor(1.5671, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:42 PM ***** Running evaluation MLM *****
06/27 04:06:42 PM   Epoch = 6 iter 79 step
06/27 04:06:42 PM   Num examples = 16
06/27 04:06:42 PM   Batch size = 32
06/27 04:06:43 PM ***** Eval results *****
06/27 04:06:43 PM   acc = 0.875
06/27 04:06:43 PM   cls_loss = 1.5973536457334245
06/27 04:06:43 PM   eval_loss = 1.3925063610076904
06/27 04:06:43 PM   global_step = 79
06/27 04:06:43 PM   loss = 1.5973536457334245
06/27 04:06:43 PM ***** LOSS printing *****
06/27 04:06:43 PM loss
06/27 04:06:43 PM tensor(1.9424, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:43 PM ***** LOSS printing *****
06/27 04:06:43 PM loss
06/27 04:06:43 PM tensor(1.2787, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:43 PM ***** LOSS printing *****
06/27 04:06:43 PM loss
06/27 04:06:43 PM tensor(1.4098, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:44 PM ***** LOSS printing *****
06/27 04:06:44 PM loss
06/27 04:06:44 PM tensor(1.2472, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:44 PM ***** LOSS printing *****
06/27 04:06:44 PM loss
06/27 04:06:44 PM tensor(2.0821, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:44 PM ***** Running evaluation MLM *****
06/27 04:06:44 PM   Epoch = 6 iter 84 step
06/27 04:06:44 PM   Num examples = 16
06/27 04:06:44 PM   Batch size = 32
06/27 04:06:45 PM ***** Eval results *****
06/27 04:06:45 PM   acc = 0.9375
06/27 04:06:45 PM   cls_loss = 1.5951445400714874
06/27 04:06:45 PM   eval_loss = 0.9216495752334595
06/27 04:06:45 PM   global_step = 84
06/27 04:06:45 PM   loss = 1.5951445400714874
06/27 04:06:45 PM ***** LOSS printing *****
06/27 04:06:45 PM loss
06/27 04:06:45 PM tensor(1.5875, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:45 PM ***** LOSS printing *****
06/27 04:06:45 PM loss
06/27 04:06:45 PM tensor(1.3084, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:45 PM ***** LOSS printing *****
06/27 04:06:45 PM loss
06/27 04:06:45 PM tensor(0.9115, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:45 PM ***** LOSS printing *****
06/27 04:06:45 PM loss
06/27 04:06:45 PM tensor(1.0926, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:45 PM ***** LOSS printing *****
06/27 04:06:45 PM loss
06/27 04:06:45 PM tensor(0.8769, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:46 PM ***** Running evaluation MLM *****
06/27 04:06:46 PM   Epoch = 7 iter 89 step
06/27 04:06:46 PM   Num examples = 16
06/27 04:06:46 PM   Batch size = 32
06/27 04:06:46 PM ***** Eval results *****
06/27 04:06:46 PM   acc = 1.0
06/27 04:06:46 PM   cls_loss = 1.1553772211074829
06/27 04:06:46 PM   eval_loss = 1.4843811988830566
06/27 04:06:46 PM   global_step = 89
06/27 04:06:46 PM   loss = 1.1553772211074829
06/27 04:06:46 PM ***** LOSS printing *****
06/27 04:06:46 PM loss
06/27 04:06:46 PM tensor(1.6542, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:46 PM ***** LOSS printing *****
06/27 04:06:46 PM loss
06/27 04:06:46 PM tensor(1.8266, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:47 PM ***** LOSS printing *****
06/27 04:06:47 PM loss
06/27 04:06:47 PM tensor(1.2886, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:47 PM ***** LOSS printing *****
06/27 04:06:47 PM loss
06/27 04:06:47 PM tensor(1.3707, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:47 PM ***** LOSS printing *****
06/27 04:06:47 PM loss
06/27 04:06:47 PM tensor(2.7742, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:47 PM ***** Running evaluation MLM *****
06/27 04:06:47 PM   Epoch = 7 iter 94 step
06/27 04:06:47 PM   Num examples = 16
06/27 04:06:47 PM   Batch size = 32
06/27 04:06:48 PM ***** Eval results *****
06/27 04:06:48 PM   acc = 1.0
06/27 04:06:48 PM   cls_loss = 1.4691138505935668
06/27 04:06:48 PM   eval_loss = 2.23628830909729
06/27 04:06:48 PM   global_step = 94
06/27 04:06:48 PM   loss = 1.4691138505935668
06/27 04:06:48 PM ***** LOSS printing *****
06/27 04:06:48 PM loss
06/27 04:06:48 PM tensor(1.7497, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:48 PM ***** LOSS printing *****
06/27 04:06:48 PM loss
06/27 04:06:48 PM tensor(1.7146, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:48 PM ***** LOSS printing *****
06/27 04:06:48 PM loss
06/27 04:06:48 PM tensor(1.2483, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:48 PM ***** LOSS printing *****
06/27 04:06:48 PM loss
06/27 04:06:48 PM tensor(1.3779, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:49 PM ***** LOSS printing *****
06/27 04:06:49 PM loss
06/27 04:06:49 PM tensor(1.1615, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:49 PM ***** Running evaluation MLM *****
06/27 04:06:49 PM   Epoch = 8 iter 99 step
06/27 04:06:49 PM   Num examples = 16
06/27 04:06:49 PM   Batch size = 32
06/27 04:06:49 PM ***** Eval results *****
06/27 04:06:49 PM   acc = 1.0
06/27 04:06:49 PM   cls_loss = 1.262579043706258
06/27 04:06:49 PM   eval_loss = 1.4840463399887085
06/27 04:06:49 PM   global_step = 99
06/27 04:06:49 PM   loss = 1.262579043706258
06/27 04:06:49 PM ***** LOSS printing *****
06/27 04:06:49 PM loss
06/27 04:06:49 PM tensor(1.2911, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:50 PM ***** LOSS printing *****
06/27 04:06:50 PM loss
06/27 04:06:50 PM tensor(1.6609, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:50 PM ***** LOSS printing *****
06/27 04:06:50 PM loss
06/27 04:06:50 PM tensor(0.9727, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:50 PM ***** LOSS printing *****
06/27 04:06:50 PM loss
06/27 04:06:50 PM tensor(1.2621, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:50 PM ***** LOSS printing *****
06/27 04:06:50 PM loss
06/27 04:06:50 PM tensor(1.3412, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:50 PM ***** Running evaluation MLM *****
06/27 04:06:50 PM   Epoch = 8 iter 104 step
06/27 04:06:50 PM   Num examples = 16
06/27 04:06:50 PM   Batch size = 32
06/27 04:06:51 PM ***** Eval results *****
06/27 04:06:51 PM   acc = 0.875
06/27 04:06:51 PM   cls_loss = 1.2894519791007042
06/27 04:06:51 PM   eval_loss = 1.5399731397628784
06/27 04:06:51 PM   global_step = 104
06/27 04:06:51 PM   loss = 1.2894519791007042
06/27 04:06:51 PM ***** LOSS printing *****
06/27 04:06:51 PM loss
06/27 04:06:51 PM tensor(1.4417, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:51 PM ***** LOSS printing *****
06/27 04:06:51 PM loss
06/27 04:06:51 PM tensor(1.4038, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:51 PM ***** LOSS printing *****
06/27 04:06:51 PM loss
06/27 04:06:51 PM tensor(1.3671, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:52 PM ***** LOSS printing *****
06/27 04:06:52 PM loss
06/27 04:06:52 PM tensor(0.9944, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:52 PM ***** LOSS printing *****
06/27 04:06:52 PM loss
06/27 04:06:52 PM tensor(1.3654, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:52 PM ***** Running evaluation MLM *****
06/27 04:06:52 PM   Epoch = 9 iter 109 step
06/27 04:06:52 PM   Num examples = 16
06/27 04:06:52 PM   Batch size = 32
06/27 04:06:53 PM ***** Eval results *****
06/27 04:06:53 PM   acc = 0.9375
06/27 04:06:53 PM   cls_loss = 1.3654296398162842
06/27 04:06:53 PM   eval_loss = 1.2039686441421509
06/27 04:06:53 PM   global_step = 109
06/27 04:06:53 PM   loss = 1.3654296398162842
06/27 04:06:53 PM ***** LOSS printing *****
06/27 04:06:53 PM loss
06/27 04:06:53 PM tensor(1.4048, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:53 PM ***** LOSS printing *****
06/27 04:06:53 PM loss
06/27 04:06:53 PM tensor(1.4738, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:53 PM ***** LOSS printing *****
06/27 04:06:53 PM loss
06/27 04:06:53 PM tensor(1.3863, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:53 PM ***** LOSS printing *****
06/27 04:06:53 PM loss
06/27 04:06:53 PM tensor(1.2190, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:54 PM ***** LOSS printing *****
06/27 04:06:54 PM loss
06/27 04:06:54 PM tensor(1.3736, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:54 PM ***** Running evaluation MLM *****
06/27 04:06:54 PM   Epoch = 9 iter 114 step
06/27 04:06:54 PM   Num examples = 16
06/27 04:06:54 PM   Batch size = 32
06/27 04:06:54 PM ***** Eval results *****
06/27 04:06:54 PM   acc = 1.0
06/27 04:06:54 PM   cls_loss = 1.3704945246378581
06/27 04:06:54 PM   eval_loss = 1.2867423295974731
06/27 04:06:54 PM   global_step = 114
06/27 04:06:54 PM   loss = 1.3704945246378581
06/27 04:06:54 PM ***** LOSS printing *****
06/27 04:06:54 PM loss
06/27 04:06:54 PM tensor(1.4576, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:54 PM ***** LOSS printing *****
06/27 04:06:54 PM loss
06/27 04:06:54 PM tensor(1.2330, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:55 PM ***** LOSS printing *****
06/27 04:06:55 PM loss
06/27 04:06:55 PM tensor(1.4336, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:55 PM ***** LOSS printing *****
06/27 04:06:55 PM loss
06/27 04:06:55 PM tensor(1.6187, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:55 PM ***** LOSS printing *****
06/27 04:06:55 PM loss
06/27 04:06:55 PM tensor(1.1123, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:06:55 PM ***** Running evaluation MLM *****
06/27 04:06:55 PM   Epoch = 9 iter 119 step
06/27 04:06:55 PM   Num examples = 16
06/27 04:06:55 PM   Batch size = 32
06/27 04:06:56 PM ***** Eval results *****
06/27 04:06:56 PM   acc = 1.0
06/27 04:06:56 PM   cls_loss = 1.3707441091537476
06/27 04:06:56 PM   eval_loss = 1.433915615081787
06/27 04:06:56 PM   global_step = 119
06/27 04:06:56 PM   loss = 1.3707441091537476
06/27 04:06:56 PM ***** LOSS printing *****
06/27 04:06:56 PM loss
06/27 04:06:56 PM tensor(1.3463, device='cuda:0', grad_fn=<NllLossBackward0>)

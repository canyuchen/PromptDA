06/27 03:36:44 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-100/', data_seed=100, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 03:36:44 PM device: cuda n_gpu: 1
06/27 03:36:45 PM Writing example 0 of 48
06/27 03:36:45 PM *** Example ***
06/27 03:36:45 PM guid: train-1
06/27 03:36:45 PM tokens: <s> a Ġfew Ġyears Ġlater Ġ, Ġtragedy Ġstruck Ġher Ġ, Ġfirst Ġa Ġfire Ġin Ġher Ġhouse Ġwhich Ġcaused Ġher Ġto Ġnot Ġbe Ġable Ġto Ġgo Ġinto Ġany Ġtype Ġof Ġlight Ġ, Ġand Ġthen Ġshe Ġwas Ġhanged Ġ. </s> ĠIt Ġis <mask>
06/27 03:36:45 PM input_ids: 0 102 367 107 423 2156 6906 2322 69 2156 78 10 668 11 69 790 61 1726 69 7 45 28 441 7 213 88 143 1907 9 1109 2156 8 172 79 21 32466 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM label: ['Ġdisturbing']
06/27 03:36:45 PM Writing example 0 of 16
06/27 03:36:45 PM *** Example ***
06/27 03:36:45 PM guid: dev-1
06/27 03:36:45 PM tokens: <s> ch arl ie Ġis Ġman Ġwho Ġwakes Ġup Ġto Ġfind Ġthat Ġno Ġone Ġcan Ġsee Ġhim Ġ, Ġby Ġchance Ġhe Ġmeets Ġcar ol Ġon Ġa Ġlonely Ġhighway Ġ. </s> ĠIt Ġis <mask>
06/27 03:36:45 PM input_ids: 0 611 11278 324 16 313 54 34142 62 7 465 14 117 65 64 192 123 2156 30 778 37 6616 512 1168 15 10 20100 6418 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM label: ['Ġdisturbing']
06/27 03:36:45 PM Writing example 0 of 2000
06/27 03:36:45 PM *** Example ***
06/27 03:36:45 PM guid: dev-1
06/27 03:36:45 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 03:36:45 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:36:45 PM label: ['Ġbad']
06/27 03:36:58 PM ***** Running training *****
06/27 03:36:58 PM   Num examples = 48
06/27 03:36:58 PM   Batch size = 4
06/27 03:36:58 PM   Num steps = 120
06/27 03:36:58 PM n: embeddings.word_embeddings.weight
06/27 03:36:58 PM n: embeddings.position_embeddings.weight
06/27 03:36:58 PM n: embeddings.token_type_embeddings.weight
06/27 03:36:58 PM n: embeddings.LayerNorm.weight
06/27 03:36:58 PM n: embeddings.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.0.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.0.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.0.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.0.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.0.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.0.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.0.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.0.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.0.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.0.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.0.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.0.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.1.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.1.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.1.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.1.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.1.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.1.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.1.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.1.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.1.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.1.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.1.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.1.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.2.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.2.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.2.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.2.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.2.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.2.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.2.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.2.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.2.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.2.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.2.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.2.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.3.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.3.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.3.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.3.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.3.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.3.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.3.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.3.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.3.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.3.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.3.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.3.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.4.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.4.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.4.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.4.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.4.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.4.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.4.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.4.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.4.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.4.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.4.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.4.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.5.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.5.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.5.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.5.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.5.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.5.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.5.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.5.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.5.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.5.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.5.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.5.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.6.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.6.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.6.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.6.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.6.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.6.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.6.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.6.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.6.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.6.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.6.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.6.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.7.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.7.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.7.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.7.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.7.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.7.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.7.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.7.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.7.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.7.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.7.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.7.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.8.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.8.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.8.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.8.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.8.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.8.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.8.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.8.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.8.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.8.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.8.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.8.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.9.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.9.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.9.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.9.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.9.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.9.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.9.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.9.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.9.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.9.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.9.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.9.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.10.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.10.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.10.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.10.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.10.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.10.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.10.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.10.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.10.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.10.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.10.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.10.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.11.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.11.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.11.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.11.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.11.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.11.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.11.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.11.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.11.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.11.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.11.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.11.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.12.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.12.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.12.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.12.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.12.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.12.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.12.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.12.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.12.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.12.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.12.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.12.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.13.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.13.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.13.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.13.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.13.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.13.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.13.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.13.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.13.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.13.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.13.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.13.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.14.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.14.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.14.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.14.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.14.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.14.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.14.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.14.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.14.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.14.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.14.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.14.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.15.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.15.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.15.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.15.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.15.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.15.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.15.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.15.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.15.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.15.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.15.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.15.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.16.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.16.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.16.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.16.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.16.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.16.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.16.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.16.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.16.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.16.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.16.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.16.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.17.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.17.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.17.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.17.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.17.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.17.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.17.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.17.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.17.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.17.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.17.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.17.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.18.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.18.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.18.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.18.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.18.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.18.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.18.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.18.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.18.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.18.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.18.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.18.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.19.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.19.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.19.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.19.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.19.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.19.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.19.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.19.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.19.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.19.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.19.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.19.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.20.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.20.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.20.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.20.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.20.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.20.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.20.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.20.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.20.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.20.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.20.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.20.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.21.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.21.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.21.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.21.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.21.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.21.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.21.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.21.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.21.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.21.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.21.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.21.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.22.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.22.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.22.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.22.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.22.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.22.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.22.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.22.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.22.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.22.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.22.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.22.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.23.attention.self.query.weight
06/27 03:36:58 PM n: encoder.layer.23.attention.self.query.bias
06/27 03:36:58 PM n: encoder.layer.23.attention.self.key.weight
06/27 03:36:58 PM n: encoder.layer.23.attention.self.key.bias
06/27 03:36:58 PM n: encoder.layer.23.attention.self.value.weight
06/27 03:36:58 PM n: encoder.layer.23.attention.self.value.bias
06/27 03:36:58 PM n: encoder.layer.23.attention.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.23.attention.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: encoder.layer.23.intermediate.dense.weight
06/27 03:36:58 PM n: encoder.layer.23.intermediate.dense.bias
06/27 03:36:58 PM n: encoder.layer.23.output.dense.weight
06/27 03:36:58 PM n: encoder.layer.23.output.dense.bias
06/27 03:36:58 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 03:36:58 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 03:36:58 PM n: pooler.dense.weight
06/27 03:36:58 PM n: pooler.dense.bias
06/27 03:36:58 PM n: roberta.embeddings.word_embeddings.weight
06/27 03:36:58 PM n: roberta.embeddings.position_embeddings.weight
06/27 03:36:58 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 03:36:58 PM n: roberta.embeddings.LayerNorm.weight
06/27 03:36:58 PM n: roberta.embeddings.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 03:36:58 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 03:36:58 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 03:36:58 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 03:36:58 PM n: roberta.pooler.dense.weight
06/27 03:36:58 PM n: roberta.pooler.dense.bias
06/27 03:36:58 PM n: lm_head.bias
06/27 03:36:58 PM n: lm_head.dense.weight
06/27 03:36:58 PM n: lm_head.dense.bias
06/27 03:36:58 PM n: lm_head.layer_norm.weight
06/27 03:36:58 PM n: lm_head.layer_norm.bias
06/27 03:36:58 PM n: lm_head.decoder.weight
06/27 03:36:58 PM Total parameters: 763292761
06/27 03:36:58 PM ***** LOSS printing *****
06/27 03:36:58 PM loss
06/27 03:36:58 PM tensor(21.2274, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:36:58 PM ***** LOSS printing *****
06/27 03:36:58 PM loss
06/27 03:36:58 PM tensor(14.8179, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:36:59 PM ***** LOSS printing *****
06/27 03:36:59 PM loss
06/27 03:36:59 PM tensor(8.8865, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:36:59 PM ***** LOSS printing *****
06/27 03:36:59 PM loss
06/27 03:36:59 PM tensor(5.1177, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:36:59 PM ***** Running evaluation MLM *****
06/27 03:36:59 PM   Epoch = 0 iter 4 step
06/27 03:36:59 PM   Num examples = 16
06/27 03:36:59 PM   Batch size = 32
06/27 03:37:00 PM ***** Eval results *****
06/27 03:37:00 PM   acc = 0.625
06/27 03:37:00 PM   cls_loss = 12.512361288070679
06/27 03:37:00 PM   eval_loss = 5.110637664794922
06/27 03:37:00 PM   global_step = 4
06/27 03:37:00 PM   loss = 12.512361288070679
06/27 03:37:00 PM ***** Save model *****
06/27 03:37:00 PM ***** Test Dataset Eval Result *****
06/27 03:38:02 PM ***** Eval results *****
06/27 03:38:02 PM   acc = 0.518
06/27 03:38:02 PM   cls_loss = 12.512361288070679
06/27 03:38:02 PM   eval_loss = 5.296872676364959
06/27 03:38:02 PM   global_step = 4
06/27 03:38:02 PM   loss = 12.512361288070679
06/27 03:38:06 PM ***** LOSS printing *****
06/27 03:38:06 PM loss
06/27 03:38:06 PM tensor(6.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:07 PM ***** LOSS printing *****
06/27 03:38:07 PM loss
06/27 03:38:07 PM tensor(3.8773, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:07 PM ***** LOSS printing *****
06/27 03:38:07 PM loss
06/27 03:38:07 PM tensor(3.4597, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:07 PM ***** LOSS printing *****
06/27 03:38:07 PM loss
06/27 03:38:07 PM tensor(3.3879, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:07 PM ***** LOSS printing *****
06/27 03:38:07 PM loss
06/27 03:38:07 PM tensor(6.2064, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:07 PM ***** Running evaluation MLM *****
06/27 03:38:07 PM   Epoch = 0 iter 9 step
06/27 03:38:07 PM   Num examples = 16
06/27 03:38:07 PM   Batch size = 32
06/27 03:38:08 PM ***** Eval results *****
06/27 03:38:08 PM   acc = 0.5625
06/27 03:38:08 PM   cls_loss = 8.117476251390245
06/27 03:38:08 PM   eval_loss = 3.069610357284546
06/27 03:38:08 PM   global_step = 9
06/27 03:38:08 PM   loss = 8.117476251390245
06/27 03:38:08 PM ***** LOSS printing *****
06/27 03:38:08 PM loss
06/27 03:38:08 PM tensor(3.4975, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:08 PM ***** LOSS printing *****
06/27 03:38:08 PM loss
06/27 03:38:08 PM tensor(2.9323, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:08 PM ***** LOSS printing *****
06/27 03:38:08 PM loss
06/27 03:38:08 PM tensor(4.3146, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:09 PM ***** LOSS printing *****
06/27 03:38:09 PM loss
06/27 03:38:09 PM tensor(1.9194, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:09 PM ***** LOSS printing *****
06/27 03:38:09 PM loss
06/27 03:38:09 PM tensor(3.2991, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:09 PM ***** Running evaluation MLM *****
06/27 03:38:09 PM   Epoch = 1 iter 14 step
06/27 03:38:09 PM   Num examples = 16
06/27 03:38:09 PM   Batch size = 32
06/27 03:38:10 PM ***** Eval results *****
06/27 03:38:10 PM   acc = 0.5
06/27 03:38:10 PM   cls_loss = 2.6092761754989624
06/27 03:38:10 PM   eval_loss = 3.2493228912353516
06/27 03:38:10 PM   global_step = 14
06/27 03:38:10 PM   loss = 2.6092761754989624
06/27 03:38:10 PM ***** LOSS printing *****
06/27 03:38:10 PM loss
06/27 03:38:10 PM tensor(2.8789, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:10 PM ***** LOSS printing *****
06/27 03:38:10 PM loss
06/27 03:38:10 PM tensor(2.9052, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:10 PM ***** LOSS printing *****
06/27 03:38:10 PM loss
06/27 03:38:10 PM tensor(2.8888, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:10 PM ***** LOSS printing *****
06/27 03:38:10 PM loss
06/27 03:38:10 PM tensor(1.6218, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:11 PM ***** LOSS printing *****
06/27 03:38:11 PM loss
06/27 03:38:11 PM tensor(2.5976, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:11 PM ***** Running evaluation MLM *****
06/27 03:38:11 PM   Epoch = 1 iter 19 step
06/27 03:38:11 PM   Num examples = 16
06/27 03:38:11 PM   Batch size = 32
06/27 03:38:11 PM ***** Eval results *****
06/27 03:38:11 PM   acc = 0.5
06/27 03:38:11 PM   cls_loss = 2.5872523614338467
06/27 03:38:11 PM   eval_loss = 2.386683464050293
06/27 03:38:11 PM   global_step = 19
06/27 03:38:11 PM   loss = 2.5872523614338467
06/27 03:38:11 PM ***** LOSS printing *****
06/27 03:38:11 PM loss
06/27 03:38:11 PM tensor(1.8116, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:11 PM ***** LOSS printing *****
06/27 03:38:11 PM loss
06/27 03:38:11 PM tensor(3.0149, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:12 PM ***** LOSS printing *****
06/27 03:38:12 PM loss
06/27 03:38:12 PM tensor(2.8925, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:12 PM ***** LOSS printing *****
06/27 03:38:12 PM loss
06/27 03:38:12 PM tensor(2.2028, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:12 PM ***** LOSS printing *****
06/27 03:38:12 PM loss
06/27 03:38:12 PM tensor(2.8823, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:38:12 PM ***** Running evaluation MLM *****
06/27 03:38:12 PM   Epoch = 1 iter 24 step
06/27 03:38:12 PM   Num examples = 16
06/27 03:38:12 PM   Batch size = 32
06/27 03:38:13 PM ***** Eval results *****
06/27 03:38:13 PM   acc = 0.75
06/27 03:38:13 PM   cls_loss = 2.576246033112208
06/27 03:38:13 PM   eval_loss = 1.7608236074447632
06/27 03:38:13 PM   global_step = 24
06/27 03:38:13 PM   loss = 2.576246033112208
06/27 03:38:13 PM ***** Save model *****
06/27 03:38:13 PM ***** Test Dataset Eval Result *****
06/27 03:39:15 PM ***** Eval results *****
06/27 03:39:15 PM   acc = 0.5595
06/27 03:39:15 PM   cls_loss = 2.576246033112208
06/27 03:39:15 PM   eval_loss = 2.002840390281072
06/27 03:39:15 PM   global_step = 24
06/27 03:39:15 PM   loss = 2.576246033112208
06/27 03:39:19 PM ***** LOSS printing *****
06/27 03:39:19 PM loss
06/27 03:39:19 PM tensor(2.3617, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:39:19 PM ***** LOSS printing *****
06/27 03:39:19 PM loss
06/27 03:39:19 PM tensor(2.1050, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:39:20 PM ***** LOSS printing *****
06/27 03:39:20 PM loss
06/27 03:39:20 PM tensor(1.5492, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:39:20 PM ***** LOSS printing *****
06/27 03:39:20 PM loss
06/27 03:39:20 PM tensor(2.2973, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:39:20 PM ***** LOSS printing *****
06/27 03:39:20 PM loss
06/27 03:39:20 PM tensor(1.2017, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:39:20 PM ***** Running evaluation MLM *****
06/27 03:39:20 PM   Epoch = 2 iter 29 step
06/27 03:39:20 PM   Num examples = 16
06/27 03:39:20 PM   Batch size = 32
06/27 03:39:21 PM ***** Eval results *****
06/27 03:39:21 PM   acc = 0.875
06/27 03:39:21 PM   cls_loss = 1.9029674291610719
06/27 03:39:21 PM   eval_loss = 2.3289358615875244
06/27 03:39:21 PM   global_step = 29
06/27 03:39:21 PM   loss = 1.9029674291610719
06/27 03:39:21 PM ***** Save model *****
06/27 03:39:21 PM ***** Test Dataset Eval Result *****
06/27 03:40:23 PM ***** Eval results *****
06/27 03:40:23 PM   acc = 0.7175
06/27 03:40:23 PM   cls_loss = 1.9029674291610719
06/27 03:40:23 PM   eval_loss = 2.5800163064684187
06/27 03:40:23 PM   global_step = 29
06/27 03:40:23 PM   loss = 1.9029674291610719
06/27 03:40:27 PM ***** LOSS printing *****
06/27 03:40:27 PM loss
06/27 03:40:27 PM tensor(1.4251, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:28 PM ***** LOSS printing *****
06/27 03:40:28 PM loss
06/27 03:40:28 PM tensor(1.4469, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:28 PM ***** LOSS printing *****
06/27 03:40:28 PM loss
06/27 03:40:28 PM tensor(2.4878, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:28 PM ***** LOSS printing *****
06/27 03:40:28 PM loss
06/27 03:40:28 PM tensor(1.9101, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:28 PM ***** LOSS printing *****
06/27 03:40:28 PM loss
06/27 03:40:28 PM tensor(2.2281, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:28 PM ***** Running evaluation MLM *****
06/27 03:40:28 PM   Epoch = 2 iter 34 step
06/27 03:40:28 PM   Num examples = 16
06/27 03:40:28 PM   Batch size = 32
06/27 03:40:29 PM ***** Eval results *****
06/27 03:40:29 PM   acc = 0.8125
06/27 03:40:29 PM   cls_loss = 1.9012837648391723
06/27 03:40:29 PM   eval_loss = 1.77822744846344
06/27 03:40:29 PM   global_step = 34
06/27 03:40:29 PM   loss = 1.9012837648391723
06/27 03:40:29 PM ***** LOSS printing *****
06/27 03:40:29 PM loss
06/27 03:40:29 PM tensor(2.3597, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:29 PM ***** LOSS printing *****
06/27 03:40:29 PM loss
06/27 03:40:29 PM tensor(1.8905, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:29 PM ***** LOSS printing *****
06/27 03:40:29 PM loss
06/27 03:40:29 PM tensor(1.9159, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:30 PM ***** LOSS printing *****
06/27 03:40:30 PM loss
06/27 03:40:30 PM tensor(1.4831, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:30 PM ***** LOSS printing *****
06/27 03:40:30 PM loss
06/27 03:40:30 PM tensor(1.4854, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:30 PM ***** Running evaluation MLM *****
06/27 03:40:30 PM   Epoch = 3 iter 39 step
06/27 03:40:30 PM   Num examples = 16
06/27 03:40:30 PM   Batch size = 32
06/27 03:40:30 PM ***** Eval results *****
06/27 03:40:30 PM   acc = 0.875
06/27 03:40:30 PM   cls_loss = 1.6281434694925945
06/27 03:40:30 PM   eval_loss = 0.915905773639679
06/27 03:40:30 PM   global_step = 39
06/27 03:40:30 PM   loss = 1.6281434694925945
06/27 03:40:31 PM ***** LOSS printing *****
06/27 03:40:31 PM loss
06/27 03:40:31 PM tensor(1.5583, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:31 PM ***** LOSS printing *****
06/27 03:40:31 PM loss
06/27 03:40:31 PM tensor(2.3195, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:31 PM ***** LOSS printing *****
06/27 03:40:31 PM loss
06/27 03:40:31 PM tensor(1.4809, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:31 PM ***** LOSS printing *****
06/27 03:40:31 PM loss
06/27 03:40:31 PM tensor(2.0500, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:31 PM ***** LOSS printing *****
06/27 03:40:31 PM loss
06/27 03:40:31 PM tensor(2.0645, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:32 PM ***** Running evaluation MLM *****
06/27 03:40:32 PM   Epoch = 3 iter 44 step
06/27 03:40:32 PM   Num examples = 16
06/27 03:40:32 PM   Batch size = 32
06/27 03:40:32 PM ***** Eval results *****
06/27 03:40:32 PM   acc = 0.875
06/27 03:40:32 PM   cls_loss = 1.794714093208313
06/27 03:40:32 PM   eval_loss = 1.5908608436584473
06/27 03:40:32 PM   global_step = 44
06/27 03:40:32 PM   loss = 1.794714093208313
06/27 03:40:32 PM ***** LOSS printing *****
06/27 03:40:32 PM loss
06/27 03:40:32 PM tensor(1.2307, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:32 PM ***** LOSS printing *****
06/27 03:40:32 PM loss
06/27 03:40:32 PM tensor(1.7393, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:33 PM ***** LOSS printing *****
06/27 03:40:33 PM loss
06/27 03:40:33 PM tensor(1.9723, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:33 PM ***** LOSS printing *****
06/27 03:40:33 PM loss
06/27 03:40:33 PM tensor(1.7792, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:33 PM ***** LOSS printing *****
06/27 03:40:33 PM loss
06/27 03:40:33 PM tensor(1.5364, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:33 PM ***** Running evaluation MLM *****
06/27 03:40:33 PM   Epoch = 4 iter 49 step
06/27 03:40:33 PM   Num examples = 16
06/27 03:40:33 PM   Batch size = 32
06/27 03:40:34 PM ***** Eval results *****
06/27 03:40:34 PM   acc = 0.875
06/27 03:40:34 PM   cls_loss = 1.5364229679107666
06/27 03:40:34 PM   eval_loss = 1.7029662132263184
06/27 03:40:34 PM   global_step = 49
06/27 03:40:34 PM   loss = 1.5364229679107666
06/27 03:40:34 PM ***** LOSS printing *****
06/27 03:40:34 PM loss
06/27 03:40:34 PM tensor(1.4202, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:34 PM ***** LOSS printing *****
06/27 03:40:34 PM loss
06/27 03:40:34 PM tensor(1.2351, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:34 PM ***** LOSS printing *****
06/27 03:40:34 PM loss
06/27 03:40:34 PM tensor(1.5814, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:34 PM ***** LOSS printing *****
06/27 03:40:34 PM loss
06/27 03:40:34 PM tensor(1.8190, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:35 PM ***** LOSS printing *****
06/27 03:40:35 PM loss
06/27 03:40:35 PM tensor(1.3918, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:40:35 PM ***** Running evaluation MLM *****
06/27 03:40:35 PM   Epoch = 4 iter 54 step
06/27 03:40:35 PM   Num examples = 16
06/27 03:40:35 PM   Batch size = 32
06/27 03:40:35 PM ***** Eval results *****
06/27 03:40:35 PM   acc = 1.0
06/27 03:40:35 PM   cls_loss = 1.497323751449585
06/27 03:40:35 PM   eval_loss = 1.0133439302444458
06/27 03:40:35 PM   global_step = 54
06/27 03:40:35 PM   loss = 1.497323751449585
06/27 03:40:35 PM ***** Save model *****
06/27 03:40:35 PM ***** Test Dataset Eval Result *****
06/27 03:41:38 PM ***** Eval results *****
06/27 03:41:38 PM   acc = 0.8605
06/27 03:41:38 PM   cls_loss = 1.497323751449585
06/27 03:41:38 PM   eval_loss = 1.287842017317575
06/27 03:41:38 PM   global_step = 54
06/27 03:41:38 PM   loss = 1.497323751449585
06/27 03:41:42 PM ***** LOSS printing *****
06/27 03:41:42 PM loss
06/27 03:41:42 PM tensor(0.9575, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:42 PM ***** LOSS printing *****
06/27 03:41:42 PM loss
06/27 03:41:42 PM tensor(1.6381, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:42 PM ***** LOSS printing *****
06/27 03:41:42 PM loss
06/27 03:41:42 PM tensor(1.9224, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:43 PM ***** LOSS printing *****
06/27 03:41:43 PM loss
06/27 03:41:43 PM tensor(1.9760, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:43 PM ***** LOSS printing *****
06/27 03:41:43 PM loss
06/27 03:41:43 PM tensor(1.9198, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:43 PM ***** Running evaluation MLM *****
06/27 03:41:43 PM   Epoch = 4 iter 59 step
06/27 03:41:43 PM   Num examples = 16
06/27 03:41:43 PM   Batch size = 32
06/27 03:41:43 PM ***** Eval results *****
06/27 03:41:43 PM   acc = 1.0
06/27 03:41:43 PM   cls_loss = 1.5815984769300981
06/27 03:41:43 PM   eval_loss = 1.1703282594680786
06/27 03:41:43 PM   global_step = 59
06/27 03:41:43 PM   loss = 1.5815984769300981
06/27 03:41:44 PM ***** LOSS printing *****
06/27 03:41:44 PM loss
06/27 03:41:44 PM tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:44 PM ***** LOSS printing *****
06/27 03:41:44 PM loss
06/27 03:41:44 PM tensor(1.0346, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:44 PM ***** LOSS printing *****
06/27 03:41:44 PM loss
06/27 03:41:44 PM tensor(1.2774, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:44 PM ***** LOSS printing *****
06/27 03:41:44 PM loss
06/27 03:41:44 PM tensor(1.2309, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:44 PM ***** LOSS printing *****
06/27 03:41:44 PM loss
06/27 03:41:44 PM tensor(1.1162, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:45 PM ***** Running evaluation MLM *****
06/27 03:41:45 PM   Epoch = 5 iter 64 step
06/27 03:41:45 PM   Num examples = 16
06/27 03:41:45 PM   Batch size = 32
06/27 03:41:45 PM ***** Eval results *****
06/27 03:41:45 PM   acc = 0.9375
06/27 03:41:45 PM   cls_loss = 1.1647771894931793
06/27 03:41:45 PM   eval_loss = 1.6604092121124268
06/27 03:41:45 PM   global_step = 64
06/27 03:41:45 PM   loss = 1.1647771894931793
06/27 03:41:45 PM ***** LOSS printing *****
06/27 03:41:45 PM loss
06/27 03:41:45 PM tensor(1.1783, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:45 PM ***** LOSS printing *****
06/27 03:41:45 PM loss
06/27 03:41:45 PM tensor(1.3824, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:46 PM ***** LOSS printing *****
06/27 03:41:46 PM loss
06/27 03:41:46 PM tensor(1.3981, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:46 PM ***** LOSS printing *****
06/27 03:41:46 PM loss
06/27 03:41:46 PM tensor(1.8731, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:46 PM ***** LOSS printing *****
06/27 03:41:46 PM loss
06/27 03:41:46 PM tensor(1.7908, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:46 PM ***** Running evaluation MLM *****
06/27 03:41:46 PM   Epoch = 5 iter 69 step
06/27 03:41:46 PM   Num examples = 16
06/27 03:41:46 PM   Batch size = 32
06/27 03:41:47 PM ***** Eval results *****
06/27 03:41:47 PM   acc = 0.9375
06/27 03:41:47 PM   cls_loss = 1.3646472030215793
06/27 03:41:47 PM   eval_loss = 1.2916464805603027
06/27 03:41:47 PM   global_step = 69
06/27 03:41:47 PM   loss = 1.3646472030215793
06/27 03:41:47 PM ***** LOSS printing *****
06/27 03:41:47 PM loss
06/27 03:41:47 PM tensor(2.0549, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:47 PM ***** LOSS printing *****
06/27 03:41:47 PM loss
06/27 03:41:47 PM tensor(1.5577, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:47 PM ***** LOSS printing *****
06/27 03:41:47 PM loss
06/27 03:41:47 PM tensor(1.3552, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:47 PM ***** LOSS printing *****
06/27 03:41:47 PM loss
06/27 03:41:47 PM tensor(1.4057, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:48 PM ***** LOSS printing *****
06/27 03:41:48 PM loss
06/27 03:41:48 PM tensor(1.2045, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:48 PM ***** Running evaluation MLM *****
06/27 03:41:48 PM   Epoch = 6 iter 74 step
06/27 03:41:48 PM   Num examples = 16
06/27 03:41:48 PM   Batch size = 32
06/27 03:41:48 PM ***** Eval results *****
06/27 03:41:48 PM   acc = 1.0
06/27 03:41:48 PM   cls_loss = 1.3050886392593384
06/27 03:41:48 PM   eval_loss = 1.6655720472335815
06/27 03:41:48 PM   global_step = 74
06/27 03:41:48 PM   loss = 1.3050886392593384
06/27 03:41:48 PM ***** LOSS printing *****
06/27 03:41:48 PM loss
06/27 03:41:48 PM tensor(0.8505, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:49 PM ***** LOSS printing *****
06/27 03:41:49 PM loss
06/27 03:41:49 PM tensor(1.7544, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:49 PM ***** LOSS printing *****
06/27 03:41:49 PM loss
06/27 03:41:49 PM tensor(1.5776, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:49 PM ***** LOSS printing *****
06/27 03:41:49 PM loss
06/27 03:41:49 PM tensor(1.1738, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:49 PM ***** LOSS printing *****
06/27 03:41:49 PM loss
06/27 03:41:49 PM tensor(1.2397, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:49 PM ***** Running evaluation MLM *****
06/27 03:41:49 PM   Epoch = 6 iter 79 step
06/27 03:41:49 PM   Num examples = 16
06/27 03:41:49 PM   Batch size = 32
06/27 03:41:50 PM ***** Eval results *****
06/27 03:41:50 PM   acc = 1.0
06/27 03:41:50 PM   cls_loss = 1.3151656559535436
06/27 03:41:50 PM   eval_loss = 1.9087296724319458
06/27 03:41:50 PM   global_step = 79
06/27 03:41:50 PM   loss = 1.3151656559535436
06/27 03:41:50 PM ***** LOSS printing *****
06/27 03:41:50 PM loss
06/27 03:41:50 PM tensor(1.4229, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:50 PM ***** LOSS printing *****
06/27 03:41:50 PM loss
06/27 03:41:50 PM tensor(1.6323, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:50 PM ***** LOSS printing *****
06/27 03:41:50 PM loss
06/27 03:41:50 PM tensor(1.8675, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:51 PM ***** LOSS printing *****
06/27 03:41:51 PM loss
06/27 03:41:51 PM tensor(1.2314, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:51 PM ***** LOSS printing *****
06/27 03:41:51 PM loss
06/27 03:41:51 PM tensor(1.4286, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:51 PM ***** Running evaluation MLM *****
06/27 03:41:51 PM   Epoch = 6 iter 84 step
06/27 03:41:51 PM   Num examples = 16
06/27 03:41:51 PM   Batch size = 32
06/27 03:41:52 PM ***** Eval results *****
06/27 03:41:52 PM   acc = 0.9375
06/27 03:41:52 PM   cls_loss = 1.3990680575370789
06/27 03:41:52 PM   eval_loss = 0.9308109283447266
06/27 03:41:52 PM   global_step = 84
06/27 03:41:52 PM   loss = 1.3990680575370789
06/27 03:41:52 PM ***** LOSS printing *****
06/27 03:41:52 PM loss
06/27 03:41:52 PM tensor(1.0785, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:52 PM ***** LOSS printing *****
06/27 03:41:52 PM loss
06/27 03:41:52 PM tensor(1.1954, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:52 PM ***** LOSS printing *****
06/27 03:41:52 PM loss
06/27 03:41:52 PM tensor(0.8404, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:52 PM ***** LOSS printing *****
06/27 03:41:52 PM loss
06/27 03:41:52 PM tensor(1.0956, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:52 PM ***** LOSS printing *****
06/27 03:41:52 PM loss
06/27 03:41:52 PM tensor(1.9224, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:53 PM ***** Running evaluation MLM *****
06/27 03:41:53 PM   Epoch = 7 iter 89 step
06/27 03:41:53 PM   Num examples = 16
06/27 03:41:53 PM   Batch size = 32
06/27 03:41:53 PM ***** Eval results *****
06/27 03:41:53 PM   acc = 0.9375
06/27 03:41:53 PM   cls_loss = 1.2264618396759033
06/27 03:41:53 PM   eval_loss = 1.2574867010116577
06/27 03:41:53 PM   global_step = 89
06/27 03:41:53 PM   loss = 1.2264618396759033
06/27 03:41:53 PM ***** LOSS printing *****
06/27 03:41:53 PM loss
06/27 03:41:53 PM tensor(1.7061, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:53 PM ***** LOSS printing *****
06/27 03:41:53 PM loss
06/27 03:41:53 PM tensor(1.3857, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:54 PM ***** LOSS printing *****
06/27 03:41:54 PM loss
06/27 03:41:54 PM tensor(2.5469, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:54 PM ***** LOSS printing *****
06/27 03:41:54 PM loss
06/27 03:41:54 PM tensor(0.9707, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:54 PM ***** LOSS printing *****
06/27 03:41:54 PM loss
06/27 03:41:54 PM tensor(2.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:54 PM ***** Running evaluation MLM *****
06/27 03:41:54 PM   Epoch = 7 iter 94 step
06/27 03:41:54 PM   Num examples = 16
06/27 03:41:54 PM   Batch size = 32
06/27 03:41:55 PM ***** Eval results *****
06/27 03:41:55 PM   acc = 0.9375
06/27 03:41:55 PM   cls_loss = 1.4822068750858306
06/27 03:41:55 PM   eval_loss = 1.6963520050048828
06/27 03:41:55 PM   global_step = 94
06/27 03:41:55 PM   loss = 1.4822068750858306
06/27 03:41:55 PM ***** LOSS printing *****
06/27 03:41:55 PM loss
06/27 03:41:55 PM tensor(1.3995, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:55 PM ***** LOSS printing *****
06/27 03:41:55 PM loss
06/27 03:41:55 PM tensor(1.3336, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:55 PM ***** LOSS printing *****
06/27 03:41:55 PM loss
06/27 03:41:55 PM tensor(1.9697, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:55 PM ***** LOSS printing *****
06/27 03:41:55 PM loss
06/27 03:41:55 PM tensor(1.0522, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:56 PM ***** LOSS printing *****
06/27 03:41:56 PM loss
06/27 03:41:56 PM tensor(1.3249, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:56 PM ***** Running evaluation MLM *****
06/27 03:41:56 PM   Epoch = 8 iter 99 step
06/27 03:41:56 PM   Num examples = 16
06/27 03:41:56 PM   Batch size = 32
06/27 03:41:56 PM ***** Eval results *****
06/27 03:41:56 PM   acc = 0.9375
06/27 03:41:56 PM   cls_loss = 1.4489264090855916
06/27 03:41:56 PM   eval_loss = 2.09269642829895
06/27 03:41:56 PM   global_step = 99
06/27 03:41:56 PM   loss = 1.4489264090855916
06/27 03:41:56 PM ***** LOSS printing *****
06/27 03:41:56 PM loss
06/27 03:41:56 PM tensor(1.2392, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:57 PM ***** LOSS printing *****
06/27 03:41:57 PM loss
06/27 03:41:57 PM tensor(0.3107, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:57 PM ***** LOSS printing *****
06/27 03:41:57 PM loss
06/27 03:41:57 PM tensor(2.2298, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:57 PM ***** LOSS printing *****
06/27 03:41:57 PM loss
06/27 03:41:57 PM tensor(1.8304, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:57 PM ***** LOSS printing *****
06/27 03:41:57 PM loss
06/27 03:41:57 PM tensor(1.2831, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:57 PM ***** Running evaluation MLM *****
06/27 03:41:57 PM   Epoch = 8 iter 104 step
06/27 03:41:57 PM   Num examples = 16
06/27 03:41:57 PM   Batch size = 32
06/27 03:41:58 PM ***** Eval results *****
06/27 03:41:58 PM   acc = 1.0
06/27 03:41:58 PM   cls_loss = 1.4049959629774094
06/27 03:41:58 PM   eval_loss = 1.564181923866272
06/27 03:41:58 PM   global_step = 104
06/27 03:41:58 PM   loss = 1.4049959629774094
06/27 03:41:58 PM ***** LOSS printing *****
06/27 03:41:58 PM loss
06/27 03:41:58 PM tensor(2.2675, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:58 PM ***** LOSS printing *****
06/27 03:41:58 PM loss
06/27 03:41:58 PM tensor(2.1993, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:58 PM ***** LOSS printing *****
06/27 03:41:58 PM loss
06/27 03:41:58 PM tensor(1.5837, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:59 PM ***** LOSS printing *****
06/27 03:41:59 PM loss
06/27 03:41:59 PM tensor(0.7676, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:59 PM ***** LOSS printing *****
06/27 03:41:59 PM loss
06/27 03:41:59 PM tensor(0.9804, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:41:59 PM ***** Running evaluation MLM *****
06/27 03:41:59 PM   Epoch = 9 iter 109 step
06/27 03:41:59 PM   Num examples = 16
06/27 03:41:59 PM   Batch size = 32
06/27 03:42:00 PM ***** Eval results *****
06/27 03:42:00 PM   acc = 1.0
06/27 03:42:00 PM   cls_loss = 0.9804151058197021
06/27 03:42:00 PM   eval_loss = 0.8220054507255554
06/27 03:42:00 PM   global_step = 109
06/27 03:42:00 PM   loss = 0.9804151058197021
06/27 03:42:00 PM ***** LOSS printing *****
06/27 03:42:00 PM loss
06/27 03:42:00 PM tensor(1.0394, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:00 PM ***** LOSS printing *****
06/27 03:42:00 PM loss
06/27 03:42:00 PM tensor(1.9121, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:00 PM ***** LOSS printing *****
06/27 03:42:00 PM loss
06/27 03:42:00 PM tensor(2.0729, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:00 PM ***** LOSS printing *****
06/27 03:42:00 PM loss
06/27 03:42:00 PM tensor(1.3189, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:00 PM ***** LOSS printing *****
06/27 03:42:00 PM loss
06/27 03:42:00 PM tensor(1.1776, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:01 PM ***** Running evaluation MLM *****
06/27 03:42:01 PM   Epoch = 9 iter 114 step
06/27 03:42:01 PM   Num examples = 16
06/27 03:42:01 PM   Batch size = 32
06/27 03:42:01 PM ***** Eval results *****
06/27 03:42:01 PM   acc = 1.0
06/27 03:42:01 PM   cls_loss = 1.4168589115142822
06/27 03:42:01 PM   eval_loss = 0.8082851767539978
06/27 03:42:01 PM   global_step = 114
06/27 03:42:01 PM   loss = 1.4168589115142822
06/27 03:42:01 PM ***** LOSS printing *****
06/27 03:42:01 PM loss
06/27 03:42:01 PM tensor(1.6922, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:01 PM ***** LOSS printing *****
06/27 03:42:01 PM loss
06/27 03:42:01 PM tensor(1.8718, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:02 PM ***** LOSS printing *****
06/27 03:42:02 PM loss
06/27 03:42:02 PM tensor(1.3472, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:02 PM ***** LOSS printing *****
06/27 03:42:02 PM loss
06/27 03:42:02 PM tensor(1.6024, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:02 PM ***** LOSS printing *****
06/27 03:42:02 PM loss
06/27 03:42:02 PM tensor(1.6201, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:42:02 PM ***** Running evaluation MLM *****
06/27 03:42:02 PM   Epoch = 9 iter 119 step
06/27 03:42:02 PM   Num examples = 16
06/27 03:42:02 PM   Batch size = 32
06/27 03:42:03 PM ***** Eval results *****
06/27 03:42:03 PM   acc = 1.0
06/27 03:42:03 PM   cls_loss = 1.5122641324996948
06/27 03:42:03 PM   eval_loss = 0.9001426100730896
06/27 03:42:03 PM   global_step = 119
06/27 03:42:03 PM   loss = 1.5122641324996948
06/27 03:42:03 PM ***** LOSS printing *****
06/27 03:42:03 PM loss
06/27 03:42:03 PM tensor(1.5197, device='cuda:0', grad_fn=<NllLossBackward0>)

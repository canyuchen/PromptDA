06/27 03:53:59 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-21/', data_seed=21, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 03:53:59 PM device: cuda n_gpu: 1
06/27 03:53:59 PM Writing example 0 of 48
06/27 03:53:59 PM *** Example ***
06/27 03:53:59 PM guid: train-1
06/27 03:53:59 PM tokens: <s> will Ġanyone Ġwho Ġisn 't Ġa Ġf ang oria Ġsubscriber Ġbe Ġexcited Ġthat Ġit Ġhasn 't Ġgone Ġstraight Ġto Ġvideo Ġ? </s> ĠIt Ġis <mask>
06/27 03:53:59 PM input_ids: 0 6677 1268 54 965 75 10 856 1097 7228 13707 28 2283 14 24 2282 75 1613 1359 7 569 17487 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM label: ['Ġright']
06/27 03:53:59 PM Writing example 0 of 16
06/27 03:53:59 PM *** Example ***
06/27 03:53:59 PM guid: dev-1
06/27 03:53:59 PM tokens: <s> any one Ġnot Ġinto Ġhigh - tech Ġspl atter f ests Ġis Ġadvised Ġto Ġtake Ġthe Ġwarning Ġliterally Ġ, Ġand Ġlog Ġon Ġto Ġsomething Ġmore Ġuser - friendly Ġ. </s> ĠIt Ġis <mask>
06/27 03:53:59 PM input_ids: 0 3785 1264 45 88 239 12 9406 11743 7933 506 10092 16 5578 7 185 5 2892 5909 2156 8 7425 15 7 402 55 3018 12 6928 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM label: ['Ġright']
06/27 03:53:59 PM Writing example 0 of 2000
06/27 03:53:59 PM *** Example ***
06/27 03:53:59 PM guid: dev-1
06/27 03:53:59 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 03:53:59 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 03:53:59 PM label: ['Ġright']
06/27 03:54:12 PM ***** Running training *****
06/27 03:54:12 PM   Num examples = 48
06/27 03:54:12 PM   Batch size = 4
06/27 03:54:12 PM   Num steps = 120
06/27 03:54:12 PM n: embeddings.word_embeddings.weight
06/27 03:54:12 PM n: embeddings.position_embeddings.weight
06/27 03:54:12 PM n: embeddings.token_type_embeddings.weight
06/27 03:54:12 PM n: embeddings.LayerNorm.weight
06/27 03:54:12 PM n: embeddings.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.0.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.0.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.0.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.0.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.0.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.0.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.0.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.0.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.0.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.0.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.0.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.0.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.1.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.1.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.1.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.1.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.1.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.1.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.1.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.1.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.1.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.1.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.1.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.1.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.2.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.2.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.2.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.2.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.2.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.2.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.2.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.2.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.2.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.2.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.2.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.2.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.3.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.3.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.3.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.3.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.3.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.3.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.3.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.3.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.3.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.3.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.3.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.3.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.4.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.4.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.4.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.4.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.4.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.4.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.4.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.4.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.4.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.4.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.4.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.4.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.5.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.5.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.5.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.5.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.5.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.5.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.5.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.5.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.5.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.5.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.5.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.5.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.6.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.6.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.6.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.6.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.6.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.6.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.6.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.6.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.6.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.6.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.6.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.6.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.7.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.7.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.7.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.7.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.7.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.7.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.7.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.7.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.7.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.7.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.7.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.7.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.8.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.8.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.8.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.8.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.8.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.8.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.8.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.8.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.8.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.8.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.8.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.8.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.9.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.9.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.9.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.9.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.9.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.9.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.9.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.9.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.9.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.9.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.9.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.9.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.10.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.10.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.10.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.10.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.10.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.10.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.10.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.10.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.10.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.10.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.10.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.10.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.11.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.11.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.11.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.11.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.11.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.11.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.11.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.11.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.11.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.11.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.11.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.11.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.12.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.12.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.12.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.12.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.12.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.12.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.12.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.12.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.12.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.12.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.12.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.12.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.13.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.13.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.13.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.13.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.13.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.13.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.13.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.13.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.13.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.13.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.13.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.13.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.14.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.14.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.14.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.14.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.14.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.14.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.14.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.14.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.14.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.14.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.14.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.14.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.15.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.15.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.15.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.15.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.15.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.15.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.15.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.15.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.15.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.15.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.15.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.15.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.16.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.16.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.16.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.16.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.16.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.16.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.16.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.16.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.16.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.16.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.16.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.16.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.17.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.17.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.17.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.17.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.17.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.17.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.17.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.17.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.17.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.17.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.17.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.17.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.18.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.18.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.18.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.18.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.18.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.18.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.18.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.18.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.18.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.18.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.18.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.18.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.19.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.19.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.19.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.19.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.19.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.19.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.19.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.19.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.19.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.19.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.19.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.19.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.20.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.20.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.20.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.20.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.20.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.20.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.20.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.20.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.20.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.20.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.20.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.20.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.21.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.21.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.21.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.21.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.21.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.21.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.21.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.21.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.21.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.21.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.21.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.21.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.22.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.22.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.22.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.22.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.22.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.22.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.22.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.22.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.22.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.22.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.22.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.22.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.23.attention.self.query.weight
06/27 03:54:12 PM n: encoder.layer.23.attention.self.query.bias
06/27 03:54:12 PM n: encoder.layer.23.attention.self.key.weight
06/27 03:54:12 PM n: encoder.layer.23.attention.self.key.bias
06/27 03:54:12 PM n: encoder.layer.23.attention.self.value.weight
06/27 03:54:12 PM n: encoder.layer.23.attention.self.value.bias
06/27 03:54:12 PM n: encoder.layer.23.attention.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.23.attention.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: encoder.layer.23.intermediate.dense.weight
06/27 03:54:12 PM n: encoder.layer.23.intermediate.dense.bias
06/27 03:54:12 PM n: encoder.layer.23.output.dense.weight
06/27 03:54:12 PM n: encoder.layer.23.output.dense.bias
06/27 03:54:12 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 03:54:12 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 03:54:12 PM n: pooler.dense.weight
06/27 03:54:12 PM n: pooler.dense.bias
06/27 03:54:12 PM n: roberta.embeddings.word_embeddings.weight
06/27 03:54:12 PM n: roberta.embeddings.position_embeddings.weight
06/27 03:54:12 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 03:54:12 PM n: roberta.embeddings.LayerNorm.weight
06/27 03:54:12 PM n: roberta.embeddings.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 03:54:12 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 03:54:12 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 03:54:12 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 03:54:12 PM n: roberta.pooler.dense.weight
06/27 03:54:12 PM n: roberta.pooler.dense.bias
06/27 03:54:12 PM n: lm_head.bias
06/27 03:54:12 PM n: lm_head.dense.weight
06/27 03:54:12 PM n: lm_head.dense.bias
06/27 03:54:12 PM n: lm_head.layer_norm.weight
06/27 03:54:12 PM n: lm_head.layer_norm.bias
06/27 03:54:12 PM n: lm_head.decoder.weight
06/27 03:54:12 PM Total parameters: 763292761
06/27 03:54:12 PM ***** LOSS printing *****
06/27 03:54:12 PM loss
06/27 03:54:12 PM tensor(17.9196, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:54:13 PM ***** LOSS printing *****
06/27 03:54:13 PM loss
06/27 03:54:13 PM tensor(12.0938, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:54:13 PM ***** LOSS printing *****
06/27 03:54:13 PM loss
06/27 03:54:13 PM tensor(7.0042, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:54:13 PM ***** LOSS printing *****
06/27 03:54:13 PM loss
06/27 03:54:13 PM tensor(4.2520, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:54:13 PM ***** Running evaluation MLM *****
06/27 03:54:13 PM   Epoch = 0 iter 4 step
06/27 03:54:13 PM   Num examples = 16
06/27 03:54:13 PM   Batch size = 32
06/27 03:54:14 PM ***** Eval results *****
06/27 03:54:14 PM   acc = 0.75
06/27 03:54:14 PM   cls_loss = 10.317375898361206
06/27 03:54:14 PM   eval_loss = 4.43378210067749
06/27 03:54:14 PM   global_step = 4
06/27 03:54:14 PM   loss = 10.317375898361206
06/27 03:54:14 PM ***** Save model *****
06/27 03:54:14 PM ***** Test Dataset Eval Result *****
06/27 03:55:17 PM ***** Eval results *****
06/27 03:55:17 PM   acc = 0.614
06/27 03:55:17 PM   cls_loss = 10.317375898361206
06/27 03:55:17 PM   eval_loss = 4.321627961264716
06/27 03:55:17 PM   global_step = 4
06/27 03:55:17 PM   loss = 10.317375898361206
06/27 03:55:21 PM ***** LOSS printing *****
06/27 03:55:21 PM loss
06/27 03:55:21 PM tensor(3.9443, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:22 PM ***** LOSS printing *****
06/27 03:55:22 PM loss
06/27 03:55:22 PM tensor(3.6164, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:22 PM ***** LOSS printing *****
06/27 03:55:22 PM loss
06/27 03:55:22 PM tensor(2.6170, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:22 PM ***** LOSS printing *****
06/27 03:55:22 PM loss
06/27 03:55:22 PM tensor(2.8200, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:22 PM ***** LOSS printing *****
06/27 03:55:22 PM loss
06/27 03:55:22 PM tensor(5.5600, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:22 PM ***** Running evaluation MLM *****
06/27 03:55:22 PM   Epoch = 0 iter 9 step
06/27 03:55:22 PM   Num examples = 16
06/27 03:55:22 PM   Batch size = 32
06/27 03:55:23 PM ***** Eval results *****
06/27 03:55:23 PM   acc = 0.5
06/27 03:55:23 PM   cls_loss = 6.647455824746026
06/27 03:55:23 PM   eval_loss = 3.0248570442199707
06/27 03:55:23 PM   global_step = 9
06/27 03:55:23 PM   loss = 6.647455824746026
06/27 03:55:23 PM ***** LOSS printing *****
06/27 03:55:23 PM loss
06/27 03:55:23 PM tensor(2.6191, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:23 PM ***** LOSS printing *****
06/27 03:55:23 PM loss
06/27 03:55:23 PM tensor(3.3115, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:23 PM ***** LOSS printing *****
06/27 03:55:23 PM loss
06/27 03:55:23 PM tensor(3.8271, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:24 PM ***** LOSS printing *****
06/27 03:55:24 PM loss
06/27 03:55:24 PM tensor(1.4269, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:24 PM ***** LOSS printing *****
06/27 03:55:24 PM loss
06/27 03:55:24 PM tensor(2.2176, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:24 PM ***** Running evaluation MLM *****
06/27 03:55:24 PM   Epoch = 1 iter 14 step
06/27 03:55:24 PM   Num examples = 16
06/27 03:55:24 PM   Batch size = 32
06/27 03:55:25 PM ***** Eval results *****
06/27 03:55:25 PM   acc = 0.5
06/27 03:55:25 PM   cls_loss = 1.8222612142562866
06/27 03:55:25 PM   eval_loss = 3.5386054515838623
06/27 03:55:25 PM   global_step = 14
06/27 03:55:25 PM   loss = 1.8222612142562866
06/27 03:55:25 PM ***** LOSS printing *****
06/27 03:55:25 PM loss
06/27 03:55:25 PM tensor(2.9839, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:25 PM ***** LOSS printing *****
06/27 03:55:25 PM loss
06/27 03:55:25 PM tensor(3.1646, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:25 PM ***** LOSS printing *****
06/27 03:55:25 PM loss
06/27 03:55:25 PM tensor(3.2717, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:25 PM ***** LOSS printing *****
06/27 03:55:25 PM loss
06/27 03:55:25 PM tensor(1.6101, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:25 PM ***** LOSS printing *****
06/27 03:55:25 PM loss
06/27 03:55:25 PM tensor(2.6098, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:26 PM ***** Running evaluation MLM *****
06/27 03:55:26 PM   Epoch = 1 iter 19 step
06/27 03:55:26 PM   Num examples = 16
06/27 03:55:26 PM   Batch size = 32
06/27 03:55:26 PM ***** Eval results *****
06/27 03:55:26 PM   acc = 0.5625
06/27 03:55:26 PM   cls_loss = 2.469222460474287
06/27 03:55:26 PM   eval_loss = 2.757960081100464
06/27 03:55:26 PM   global_step = 19
06/27 03:55:26 PM   loss = 2.469222460474287
06/27 03:55:26 PM ***** LOSS printing *****
06/27 03:55:26 PM loss
06/27 03:55:26 PM tensor(2.2781, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:26 PM ***** LOSS printing *****
06/27 03:55:26 PM loss
06/27 03:55:26 PM tensor(3.1862, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:27 PM ***** LOSS printing *****
06/27 03:55:27 PM loss
06/27 03:55:27 PM tensor(2.6544, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:27 PM ***** LOSS printing *****
06/27 03:55:27 PM loss
06/27 03:55:27 PM tensor(2.4045, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:27 PM ***** LOSS printing *****
06/27 03:55:27 PM loss
06/27 03:55:27 PM tensor(2.5700, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:27 PM ***** Running evaluation MLM *****
06/27 03:55:27 PM   Epoch = 1 iter 24 step
06/27 03:55:27 PM   Num examples = 16
06/27 03:55:27 PM   Batch size = 32
06/27 03:55:28 PM ***** Eval results *****
06/27 03:55:28 PM   acc = 0.6875
06/27 03:55:28 PM   cls_loss = 2.531481593847275
06/27 03:55:28 PM   eval_loss = 1.9931368827819824
06/27 03:55:28 PM   global_step = 24
06/27 03:55:28 PM   loss = 2.531481593847275
06/27 03:55:28 PM ***** LOSS printing *****
06/27 03:55:28 PM loss
06/27 03:55:28 PM tensor(1.4504, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:28 PM ***** LOSS printing *****
06/27 03:55:28 PM loss
06/27 03:55:28 PM tensor(1.3106, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:28 PM ***** LOSS printing *****
06/27 03:55:28 PM loss
06/27 03:55:28 PM tensor(1.7717, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:28 PM ***** LOSS printing *****
06/27 03:55:28 PM loss
06/27 03:55:28 PM tensor(2.1441, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:29 PM ***** LOSS printing *****
06/27 03:55:29 PM loss
06/27 03:55:29 PM tensor(1.8049, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:55:29 PM ***** Running evaluation MLM *****
06/27 03:55:29 PM   Epoch = 2 iter 29 step
06/27 03:55:29 PM   Num examples = 16
06/27 03:55:29 PM   Batch size = 32
06/27 03:55:29 PM ***** Eval results *****
06/27 03:55:29 PM   acc = 0.875
06/27 03:55:29 PM   cls_loss = 1.6963790893554687
06/27 03:55:29 PM   eval_loss = 2.1835453510284424
06/27 03:55:29 PM   global_step = 29
06/27 03:55:29 PM   loss = 1.6963790893554687
06/27 03:55:29 PM ***** Save model *****
06/27 03:55:29 PM ***** Test Dataset Eval Result *****
06/27 03:56:33 PM ***** Eval results *****
06/27 03:56:33 PM   acc = 0.859
06/27 03:56:33 PM   cls_loss = 1.6963790893554687
06/27 03:56:33 PM   eval_loss = 2.1207217403820584
06/27 03:56:33 PM   global_step = 29
06/27 03:56:33 PM   loss = 1.6963790893554687
06/27 03:56:37 PM ***** LOSS printing *****
06/27 03:56:37 PM loss
06/27 03:56:37 PM tensor(0.7877, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:37 PM ***** LOSS printing *****
06/27 03:56:37 PM loss
06/27 03:56:37 PM tensor(2.2304, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:37 PM ***** LOSS printing *****
06/27 03:56:37 PM loss
06/27 03:56:37 PM tensor(1.1378, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:37 PM ***** LOSS printing *****
06/27 03:56:37 PM loss
06/27 03:56:37 PM tensor(3.7182, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:38 PM ***** LOSS printing *****
06/27 03:56:38 PM loss
06/27 03:56:38 PM tensor(1.4689, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:38 PM ***** Running evaluation MLM *****
06/27 03:56:38 PM   Epoch = 2 iter 34 step
06/27 03:56:38 PM   Num examples = 16
06/27 03:56:38 PM   Batch size = 32
06/27 03:56:38 PM ***** Eval results *****
06/27 03:56:38 PM   acc = 0.8125
06/27 03:56:38 PM   cls_loss = 1.7824898719787599
06/27 03:56:38 PM   eval_loss = 1.613025188446045
06/27 03:56:38 PM   global_step = 34
06/27 03:56:38 PM   loss = 1.7824898719787599
06/27 03:56:38 PM ***** LOSS printing *****
06/27 03:56:38 PM loss
06/27 03:56:38 PM tensor(3.1813, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:39 PM ***** LOSS printing *****
06/27 03:56:39 PM loss
06/27 03:56:39 PM tensor(1.2788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:39 PM ***** LOSS printing *****
06/27 03:56:39 PM loss
06/27 03:56:39 PM tensor(2.3511, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:39 PM ***** LOSS printing *****
06/27 03:56:39 PM loss
06/27 03:56:39 PM tensor(1.0050, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:39 PM ***** LOSS printing *****
06/27 03:56:39 PM loss
06/27 03:56:39 PM tensor(2.1830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:40 PM ***** Running evaluation MLM *****
06/27 03:56:40 PM   Epoch = 3 iter 39 step
06/27 03:56:40 PM   Num examples = 16
06/27 03:56:40 PM   Batch size = 32
06/27 03:56:40 PM ***** Eval results *****
06/27 03:56:40 PM   acc = 0.875
06/27 03:56:40 PM   cls_loss = 1.8463598887125652
06/27 03:56:40 PM   eval_loss = 2.0367929935455322
06/27 03:56:40 PM   global_step = 39
06/27 03:56:40 PM   loss = 1.8463598887125652
06/27 03:56:40 PM ***** LOSS printing *****
06/27 03:56:40 PM loss
06/27 03:56:40 PM tensor(4.1475, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:40 PM ***** LOSS printing *****
06/27 03:56:40 PM loss
06/27 03:56:40 PM tensor(4.0692, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:41 PM ***** LOSS printing *****
06/27 03:56:41 PM loss
06/27 03:56:41 PM tensor(2.4147, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:41 PM ***** LOSS printing *****
06/27 03:56:41 PM loss
06/27 03:56:41 PM tensor(3.3367, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:41 PM ***** LOSS printing *****
06/27 03:56:41 PM loss
06/27 03:56:41 PM tensor(2.5831, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:41 PM ***** Running evaluation MLM *****
06/27 03:56:41 PM   Epoch = 3 iter 44 step
06/27 03:56:41 PM   Num examples = 16
06/27 03:56:41 PM   Batch size = 32
06/27 03:56:42 PM ***** Eval results *****
06/27 03:56:42 PM   acc = 0.8125
06/27 03:56:42 PM   cls_loss = 2.7612909972667694
06/27 03:56:42 PM   eval_loss = 1.6254708766937256
06/27 03:56:42 PM   global_step = 44
06/27 03:56:42 PM   loss = 2.7612909972667694
06/27 03:56:42 PM ***** LOSS printing *****
06/27 03:56:42 PM loss
06/27 03:56:42 PM tensor(1.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:42 PM ***** LOSS printing *****
06/27 03:56:42 PM loss
06/27 03:56:42 PM tensor(1.5399, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:42 PM ***** LOSS printing *****
06/27 03:56:42 PM loss
06/27 03:56:42 PM tensor(2.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:42 PM ***** LOSS printing *****
06/27 03:56:42 PM loss
06/27 03:56:42 PM tensor(1.0203, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:43 PM ***** LOSS printing *****
06/27 03:56:43 PM loss
06/27 03:56:43 PM tensor(1.0984, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:56:43 PM ***** Running evaluation MLM *****
06/27 03:56:43 PM   Epoch = 4 iter 49 step
06/27 03:56:43 PM   Num examples = 16
06/27 03:56:43 PM   Batch size = 32
06/27 03:56:43 PM ***** Eval results *****
06/27 03:56:43 PM   acc = 0.9375
06/27 03:56:43 PM   cls_loss = 1.0984324216842651
06/27 03:56:43 PM   eval_loss = 0.9692800641059875
06/27 03:56:43 PM   global_step = 49
06/27 03:56:43 PM   loss = 1.0984324216842651
06/27 03:56:43 PM ***** Save model *****
06/27 03:56:43 PM ***** Test Dataset Eval Result *****
06/27 03:57:47 PM ***** Eval results *****
06/27 03:57:47 PM   acc = 0.9135
06/27 03:57:47 PM   cls_loss = 1.0984324216842651
06/27 03:57:47 PM   eval_loss = 0.9592621544050792
06/27 03:57:47 PM   global_step = 49
06/27 03:57:47 PM   loss = 1.0984324216842651
06/27 03:57:51 PM ***** LOSS printing *****
06/27 03:57:51 PM loss
06/27 03:57:51 PM tensor(1.2370, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:51 PM ***** LOSS printing *****
06/27 03:57:51 PM loss
06/27 03:57:51 PM tensor(1.7595, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:51 PM ***** LOSS printing *****
06/27 03:57:51 PM loss
06/27 03:57:51 PM tensor(1.4144, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:51 PM ***** LOSS printing *****
06/27 03:57:51 PM loss
06/27 03:57:51 PM tensor(1.1895, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:52 PM ***** LOSS printing *****
06/27 03:57:52 PM loss
06/27 03:57:52 PM tensor(1.2377, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:52 PM ***** Running evaluation MLM *****
06/27 03:57:52 PM   Epoch = 4 iter 54 step
06/27 03:57:52 PM   Num examples = 16
06/27 03:57:52 PM   Batch size = 32
06/27 03:57:52 PM ***** Eval results *****
06/27 03:57:52 PM   acc = 0.875
06/27 03:57:52 PM   cls_loss = 1.322768251101176
06/27 03:57:52 PM   eval_loss = 1.2732151746749878
06/27 03:57:52 PM   global_step = 54
06/27 03:57:52 PM   loss = 1.322768251101176
06/27 03:57:52 PM ***** LOSS printing *****
06/27 03:57:52 PM loss
06/27 03:57:52 PM tensor(1.4706, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:53 PM ***** LOSS printing *****
06/27 03:57:53 PM loss
06/27 03:57:53 PM tensor(1.5167, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:53 PM ***** LOSS printing *****
06/27 03:57:53 PM loss
06/27 03:57:53 PM tensor(1.4704, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:53 PM ***** LOSS printing *****
06/27 03:57:53 PM loss
06/27 03:57:53 PM tensor(1.7081, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:53 PM ***** LOSS printing *****
06/27 03:57:53 PM loss
06/27 03:57:53 PM tensor(1.4217, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:53 PM ***** Running evaluation MLM *****
06/27 03:57:53 PM   Epoch = 4 iter 59 step
06/27 03:57:53 PM   Num examples = 16
06/27 03:57:53 PM   Batch size = 32
06/27 03:57:54 PM ***** Eval results *****
06/27 03:57:54 PM   acc = 0.9375
06/27 03:57:54 PM   cls_loss = 1.4112832871350376
06/27 03:57:54 PM   eval_loss = 0.9662204384803772
06/27 03:57:54 PM   global_step = 59
06/27 03:57:54 PM   loss = 1.4112832871350376
06/27 03:57:54 PM ***** LOSS printing *****
06/27 03:57:54 PM loss
06/27 03:57:54 PM tensor(1.3983, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:54 PM ***** LOSS printing *****
06/27 03:57:54 PM loss
06/27 03:57:54 PM tensor(0.8477, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:54 PM ***** LOSS printing *****
06/27 03:57:54 PM loss
06/27 03:57:54 PM tensor(1.2392, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:55 PM ***** LOSS printing *****
06/27 03:57:55 PM loss
06/27 03:57:55 PM tensor(1.7711, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:55 PM ***** LOSS printing *****
06/27 03:57:55 PM loss
06/27 03:57:55 PM tensor(1.1781, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:57:55 PM ***** Running evaluation MLM *****
06/27 03:57:55 PM   Epoch = 5 iter 64 step
06/27 03:57:55 PM   Num examples = 16
06/27 03:57:55 PM   Batch size = 32
06/27 03:57:56 PM ***** Eval results *****
06/27 03:57:56 PM   acc = 1.0
06/27 03:57:56 PM   cls_loss = 1.258999228477478
06/27 03:57:56 PM   eval_loss = 1.1714932918548584
06/27 03:57:56 PM   global_step = 64
06/27 03:57:56 PM   loss = 1.258999228477478
06/27 03:57:56 PM ***** Save model *****
06/27 03:57:56 PM ***** Test Dataset Eval Result *****
06/27 03:58:59 PM ***** Eval results *****
06/27 03:58:59 PM   acc = 0.9045
06/27 03:58:59 PM   cls_loss = 1.258999228477478
06/27 03:58:59 PM   eval_loss = 1.2796225509946308
06/27 03:58:59 PM   global_step = 64
06/27 03:58:59 PM   loss = 1.258999228477478
06/27 03:59:03 PM ***** LOSS printing *****
06/27 03:59:03 PM loss
06/27 03:59:03 PM tensor(1.6538, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:03 PM ***** LOSS printing *****
06/27 03:59:03 PM loss
06/27 03:59:03 PM tensor(1.4371, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:03 PM ***** LOSS printing *****
06/27 03:59:03 PM loss
06/27 03:59:03 PM tensor(1.5250, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:04 PM ***** LOSS printing *****
06/27 03:59:04 PM loss
06/27 03:59:04 PM tensor(1.2293, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:04 PM ***** LOSS printing *****
06/27 03:59:04 PM loss
06/27 03:59:04 PM tensor(1.3881, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:04 PM ***** Running evaluation MLM *****
06/27 03:59:04 PM   Epoch = 5 iter 69 step
06/27 03:59:04 PM   Num examples = 16
06/27 03:59:04 PM   Batch size = 32
06/27 03:59:05 PM ***** Eval results *****
06/27 03:59:05 PM   acc = 0.9375
06/27 03:59:05 PM   cls_loss = 1.3632395532396104
06/27 03:59:05 PM   eval_loss = 1.5997517108917236
06/27 03:59:05 PM   global_step = 69
06/27 03:59:05 PM   loss = 1.3632395532396104
06/27 03:59:05 PM ***** LOSS printing *****
06/27 03:59:05 PM loss
06/27 03:59:05 PM tensor(1.9348, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:05 PM ***** LOSS printing *****
06/27 03:59:05 PM loss
06/27 03:59:05 PM tensor(1.0871, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:05 PM ***** LOSS printing *****
06/27 03:59:05 PM loss
06/27 03:59:05 PM tensor(1.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:05 PM ***** LOSS printing *****
06/27 03:59:05 PM loss
06/27 03:59:05 PM tensor(1.1486, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:05 PM ***** LOSS printing *****
06/27 03:59:05 PM loss
06/27 03:59:05 PM tensor(1.3128, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:06 PM ***** Running evaluation MLM *****
06/27 03:59:06 PM   Epoch = 6 iter 74 step
06/27 03:59:06 PM   Num examples = 16
06/27 03:59:06 PM   Batch size = 32
06/27 03:59:06 PM ***** Eval results *****
06/27 03:59:06 PM   acc = 0.875
06/27 03:59:06 PM   cls_loss = 1.2306686639785767
06/27 03:59:06 PM   eval_loss = 1.65668785572052
06/27 03:59:06 PM   global_step = 74
06/27 03:59:06 PM   loss = 1.2306686639785767
06/27 03:59:06 PM ***** LOSS printing *****
06/27 03:59:06 PM loss
06/27 03:59:06 PM tensor(1.4510, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:06 PM ***** LOSS printing *****
06/27 03:59:06 PM loss
06/27 03:59:06 PM tensor(0.9285, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:07 PM ***** LOSS printing *****
06/27 03:59:07 PM loss
06/27 03:59:07 PM tensor(1.4045, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:07 PM ***** LOSS printing *****
06/27 03:59:07 PM loss
06/27 03:59:07 PM tensor(2.0637, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:07 PM ***** LOSS printing *****
06/27 03:59:07 PM loss
06/27 03:59:07 PM tensor(1.3260, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:07 PM ***** Running evaluation MLM *****
06/27 03:59:07 PM   Epoch = 6 iter 79 step
06/27 03:59:07 PM   Num examples = 16
06/27 03:59:07 PM   Batch size = 32
06/27 03:59:08 PM ***** Eval results *****
06/27 03:59:08 PM   acc = 0.875
06/27 03:59:08 PM   cls_loss = 1.3764425090381078
06/27 03:59:08 PM   eval_loss = 1.3125370740890503
06/27 03:59:08 PM   global_step = 79
06/27 03:59:08 PM   loss = 1.3764425090381078
06/27 03:59:08 PM ***** LOSS printing *****
06/27 03:59:08 PM loss
06/27 03:59:08 PM tensor(1.6636, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:08 PM ***** LOSS printing *****
06/27 03:59:08 PM loss
06/27 03:59:08 PM tensor(1.0779, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:08 PM ***** LOSS printing *****
06/27 03:59:08 PM loss
06/27 03:59:08 PM tensor(1.3161, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:08 PM ***** LOSS printing *****
06/27 03:59:08 PM loss
06/27 03:59:08 PM tensor(1.5061, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:09 PM ***** LOSS printing *****
06/27 03:59:09 PM loss
06/27 03:59:09 PM tensor(1.8651, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:09 PM ***** Running evaluation MLM *****
06/27 03:59:09 PM   Epoch = 6 iter 84 step
06/27 03:59:09 PM   Num examples = 16
06/27 03:59:09 PM   Batch size = 32
06/27 03:59:09 PM ***** Eval results *****
06/27 03:59:09 PM   acc = 0.9375
06/27 03:59:09 PM   cls_loss = 1.421991581718127
06/27 03:59:09 PM   eval_loss = 0.9501885771751404
06/27 03:59:09 PM   global_step = 84
06/27 03:59:09 PM   loss = 1.421991581718127
06/27 03:59:09 PM ***** LOSS printing *****
06/27 03:59:09 PM loss
06/27 03:59:09 PM tensor(0.9921, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:10 PM ***** LOSS printing *****
06/27 03:59:10 PM loss
06/27 03:59:10 PM tensor(1.8576, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:10 PM ***** LOSS printing *****
06/27 03:59:10 PM loss
06/27 03:59:10 PM tensor(1.2398, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:10 PM ***** LOSS printing *****
06/27 03:59:10 PM loss
06/27 03:59:10 PM tensor(1.6418, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:10 PM ***** LOSS printing *****
06/27 03:59:10 PM loss
06/27 03:59:10 PM tensor(1.6225, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:10 PM ***** Running evaluation MLM *****
06/27 03:59:10 PM   Epoch = 7 iter 89 step
06/27 03:59:10 PM   Num examples = 16
06/27 03:59:10 PM   Batch size = 32
06/27 03:59:11 PM ***** Eval results *****
06/27 03:59:11 PM   acc = 0.9375
06/27 03:59:11 PM   cls_loss = 1.4707658529281615
06/27 03:59:11 PM   eval_loss = 1.0685291290283203
06/27 03:59:11 PM   global_step = 89
06/27 03:59:11 PM   loss = 1.4707658529281615
06/27 03:59:11 PM ***** LOSS printing *****
06/27 03:59:11 PM loss
06/27 03:59:11 PM tensor(1.4889, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:11 PM ***** LOSS printing *****
06/27 03:59:11 PM loss
06/27 03:59:11 PM tensor(1.3278, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:11 PM ***** LOSS printing *****
06/27 03:59:11 PM loss
06/27 03:59:11 PM tensor(1.5162, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:12 PM ***** LOSS printing *****
06/27 03:59:12 PM loss
06/27 03:59:12 PM tensor(1.0978, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:12 PM ***** LOSS printing *****
06/27 03:59:12 PM loss
06/27 03:59:12 PM tensor(1.3316, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:12 PM ***** Running evaluation MLM *****
06/27 03:59:12 PM   Epoch = 7 iter 94 step
06/27 03:59:12 PM   Num examples = 16
06/27 03:59:12 PM   Batch size = 32
06/27 03:59:13 PM ***** Eval results *****
06/27 03:59:13 PM   acc = 1.0
06/27 03:59:13 PM   cls_loss = 1.4116250872612
06/27 03:59:13 PM   eval_loss = 1.6078133583068848
06/27 03:59:13 PM   global_step = 94
06/27 03:59:13 PM   loss = 1.4116250872612
06/27 03:59:13 PM ***** LOSS printing *****
06/27 03:59:13 PM loss
06/27 03:59:13 PM tensor(1.4137, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:13 PM ***** LOSS printing *****
06/27 03:59:13 PM loss
06/27 03:59:13 PM tensor(1.4530, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:13 PM ***** LOSS printing *****
06/27 03:59:13 PM loss
06/27 03:59:13 PM tensor(1.3232, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:13 PM ***** LOSS printing *****
06/27 03:59:13 PM loss
06/27 03:59:13 PM tensor(1.0101, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:14 PM ***** LOSS printing *****
06/27 03:59:14 PM loss
06/27 03:59:14 PM tensor(1.1482, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:14 PM ***** Running evaluation MLM *****
06/27 03:59:14 PM   Epoch = 8 iter 99 step
06/27 03:59:14 PM   Num examples = 16
06/27 03:59:14 PM   Batch size = 32
06/27 03:59:14 PM ***** Eval results *****
06/27 03:59:14 PM   acc = 0.875
06/27 03:59:14 PM   cls_loss = 1.1604989767074585
06/27 03:59:14 PM   eval_loss = 1.6924594640731812
06/27 03:59:14 PM   global_step = 99
06/27 03:59:14 PM   loss = 1.1604989767074585
06/27 03:59:14 PM ***** LOSS printing *****
06/27 03:59:14 PM loss
06/27 03:59:14 PM tensor(1.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:14 PM ***** LOSS printing *****
06/27 03:59:14 PM loss
06/27 03:59:14 PM tensor(0.7193, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:15 PM ***** LOSS printing *****
06/27 03:59:15 PM loss
06/27 03:59:15 PM tensor(1.7842, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:15 PM ***** LOSS printing *****
06/27 03:59:15 PM loss
06/27 03:59:15 PM tensor(1.6247, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:15 PM ***** LOSS printing *****
06/27 03:59:15 PM loss
06/27 03:59:15 PM tensor(1.3789, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:15 PM ***** Running evaluation MLM *****
06/27 03:59:15 PM   Epoch = 8 iter 104 step
06/27 03:59:15 PM   Num examples = 16
06/27 03:59:15 PM   Batch size = 32
06/27 03:59:16 PM ***** Eval results *****
06/27 03:59:16 PM   acc = 0.9375
06/27 03:59:16 PM   cls_loss = 1.2630748599767685
06/27 03:59:16 PM   eval_loss = 1.7114310264587402
06/27 03:59:16 PM   global_step = 104
06/27 03:59:16 PM   loss = 1.2630748599767685
06/27 03:59:16 PM ***** LOSS printing *****
06/27 03:59:16 PM loss
06/27 03:59:16 PM tensor(2.1874, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:16 PM ***** LOSS printing *****
06/27 03:59:16 PM loss
06/27 03:59:16 PM tensor(2.1034, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:16 PM ***** LOSS printing *****
06/27 03:59:16 PM loss
06/27 03:59:16 PM tensor(1.2892, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:17 PM ***** LOSS printing *****
06/27 03:59:17 PM loss
06/27 03:59:17 PM tensor(1.1716, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:17 PM ***** LOSS printing *****
06/27 03:59:17 PM loss
06/27 03:59:17 PM tensor(0.8898, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:17 PM ***** Running evaluation MLM *****
06/27 03:59:17 PM   Epoch = 9 iter 109 step
06/27 03:59:17 PM   Num examples = 16
06/27 03:59:17 PM   Batch size = 32
06/27 03:59:17 PM ***** Eval results *****
06/27 03:59:17 PM   acc = 0.9375
06/27 03:59:17 PM   cls_loss = 0.8897799849510193
06/27 03:59:17 PM   eval_loss = 0.9300538301467896
06/27 03:59:17 PM   global_step = 109
06/27 03:59:17 PM   loss = 0.8897799849510193
06/27 03:59:18 PM ***** LOSS printing *****
06/27 03:59:18 PM loss
06/27 03:59:18 PM tensor(1.1089, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:18 PM ***** LOSS printing *****
06/27 03:59:18 PM loss
06/27 03:59:18 PM tensor(1.5782, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:18 PM ***** LOSS printing *****
06/27 03:59:18 PM loss
06/27 03:59:18 PM tensor(1.4405, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:18 PM ***** LOSS printing *****
06/27 03:59:18 PM loss
06/27 03:59:18 PM tensor(1.2724, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:18 PM ***** LOSS printing *****
06/27 03:59:18 PM loss
06/27 03:59:18 PM tensor(1.7895, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:19 PM ***** Running evaluation MLM *****
06/27 03:59:19 PM   Epoch = 9 iter 114 step
06/27 03:59:19 PM   Num examples = 16
06/27 03:59:19 PM   Batch size = 32
06/27 03:59:19 PM ***** Eval results *****
06/27 03:59:19 PM   acc = 0.9375
06/27 03:59:19 PM   cls_loss = 1.3465561370054882
06/27 03:59:19 PM   eval_loss = 0.8829512596130371
06/27 03:59:19 PM   global_step = 114
06/27 03:59:19 PM   loss = 1.3465561370054882
06/27 03:59:19 PM ***** LOSS printing *****
06/27 03:59:19 PM loss
06/27 03:59:19 PM tensor(1.8787, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:19 PM ***** LOSS printing *****
06/27 03:59:19 PM loss
06/27 03:59:19 PM tensor(1.6158, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:20 PM ***** LOSS printing *****
06/27 03:59:20 PM loss
06/27 03:59:20 PM tensor(1.1309, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:20 PM ***** LOSS printing *****
06/27 03:59:20 PM loss
06/27 03:59:20 PM tensor(1.4920, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:20 PM ***** LOSS printing *****
06/27 03:59:20 PM loss
06/27 03:59:20 PM tensor(1.0958, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 03:59:20 PM ***** Running evaluation MLM *****
06/27 03:59:20 PM   Epoch = 9 iter 119 step
06/27 03:59:20 PM   Num examples = 16
06/27 03:59:20 PM   Batch size = 32
06/27 03:59:21 PM ***** Eval results *****
06/27 03:59:21 PM   acc = 0.9375
06/27 03:59:21 PM   cls_loss = 1.3902245380661704
06/27 03:59:21 PM   eval_loss = 1.0465672016143799
06/27 03:59:21 PM   global_step = 119
06/27 03:59:21 PM   loss = 1.3902245380661704
06/27 03:59:21 PM ***** LOSS printing *****
06/27 03:59:21 PM loss
06/27 03:59:21 PM tensor(1.2192, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 04:09:56 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/SST-2/8-87/', data_seed=87, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='sst-2', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 04:09:56 PM device: cuda n_gpu: 1
06/27 04:09:56 PM Writing example 0 of 48
06/27 04:09:56 PM *** Example ***
06/27 04:09:56 PM guid: train-1
06/27 04:09:56 PM tokens: <s> the Ġsatire Ġis Ġunfocused Ġ, Ġwhile Ġthe Ġstory Ġgoes Ġnowhere Ġ. </s> ĠIt Ġis <mask>
06/27 04:09:56 PM input_ids: 0 627 31368 16 47306 2156 150 5 527 1411 9261 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM label: ['Ġterrible']
06/27 04:09:56 PM Writing example 0 of 16
06/27 04:09:56 PM *** Example ***
06/27 04:09:56 PM guid: dev-1
06/27 04:09:56 PM tokens: <s> norm ally Ġ, Ġro h mer Ġ' s Ġtalk y Ġfilms Ġfasc inate Ġme Ġ, Ġbut Ġwhen Ġhe Ġmoves Ġhis Ġsetting Ġto Ġthe Ġpast Ġ, Ġand Ġrelies Ġon Ġa Ġhistorical Ġtext Ġ, Ġhe Ġloses Ġthe Ġrichness Ġof Ġcharacterization Ġthat Ġmakes Ġhis Ġfilms Ġso Ġmemorable Ġ. </s> ĠIt Ġis <mask>
06/27 04:09:56 PM input_ids: 0 42258 2368 2156 4533 298 2089 128 29 1067 219 3541 35439 13014 162 2156 53 77 37 3136 39 2749 7 5 375 2156 8 12438 15 10 4566 2788 2156 37 13585 5 38857 9 34934 14 817 39 3541 98 10132 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM label: ['Ġterrible']
06/27 04:09:56 PM Writing example 0 of 872
06/27 04:09:56 PM *** Example ***
06/27 04:09:56 PM guid: dev-1
06/27 04:09:56 PM tokens: <s> one Ġlong Ġstring Ġof Ġcl ic hes Ġ. </s> ĠIt Ġis <mask>
06/27 04:09:56 PM input_ids: 0 1264 251 6755 9 3741 636 5065 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:09:56 PM label: ['Ġterrible']
06/27 04:10:09 PM ***** Running training *****
06/27 04:10:09 PM   Num examples = 48
06/27 04:10:09 PM   Batch size = 4
06/27 04:10:09 PM   Num steps = 120
06/27 04:10:09 PM n: embeddings.word_embeddings.weight
06/27 04:10:09 PM n: embeddings.position_embeddings.weight
06/27 04:10:09 PM n: embeddings.token_type_embeddings.weight
06/27 04:10:09 PM n: embeddings.LayerNorm.weight
06/27 04:10:09 PM n: embeddings.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.0.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.0.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.0.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.0.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.0.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.0.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.0.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.0.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.0.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.0.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.0.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.0.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.1.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.1.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.1.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.1.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.1.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.1.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.1.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.1.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.1.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.1.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.1.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.1.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.2.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.2.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.2.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.2.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.2.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.2.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.2.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.2.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.2.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.2.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.2.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.2.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.3.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.3.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.3.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.3.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.3.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.3.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.3.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.3.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.3.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.3.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.3.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.3.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.4.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.4.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.4.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.4.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.4.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.4.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.4.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.4.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.4.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.4.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.4.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.4.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.5.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.5.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.5.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.5.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.5.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.5.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.5.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.5.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.5.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.5.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.5.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.5.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.6.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.6.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.6.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.6.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.6.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.6.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.6.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.6.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.6.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.6.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.6.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.6.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.7.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.7.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.7.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.7.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.7.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.7.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.7.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.7.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.7.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.7.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.7.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.7.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.8.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.8.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.8.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.8.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.8.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.8.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.8.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.8.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.8.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.8.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.8.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.8.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.9.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.9.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.9.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.9.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.9.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.9.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.9.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.9.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.9.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.9.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.9.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.9.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.10.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.10.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.10.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.10.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.10.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.10.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.10.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.10.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.10.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.10.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.10.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.10.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.11.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.11.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.11.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.11.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.11.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.11.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.11.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.11.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.11.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.11.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.11.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.11.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.12.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.12.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.12.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.12.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.12.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.12.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.12.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.12.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.12.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.12.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.12.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.12.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.13.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.13.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.13.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.13.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.13.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.13.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.13.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.13.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.13.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.13.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.13.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.13.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.14.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.14.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.14.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.14.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.14.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.14.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.14.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.14.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.14.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.14.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.14.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.14.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.15.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.15.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.15.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.15.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.15.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.15.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.15.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.15.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.15.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.15.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.15.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.15.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.16.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.16.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.16.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.16.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.16.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.16.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.16.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.16.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.16.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.16.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.16.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.16.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.17.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.17.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.17.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.17.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.17.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.17.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.17.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.17.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.17.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.17.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.17.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.17.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.18.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.18.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.18.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.18.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.18.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.18.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.18.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.18.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.18.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.18.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.18.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.18.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.19.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.19.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.19.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.19.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.19.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.19.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.19.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.19.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.19.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.19.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.19.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.19.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.20.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.20.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.20.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.20.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.20.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.20.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.20.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.20.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.20.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.20.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.20.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.20.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.21.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.21.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.21.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.21.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.21.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.21.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.21.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.21.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.21.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.21.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.21.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.21.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.22.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.22.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.22.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.22.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.22.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.22.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.22.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.22.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.22.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.22.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.22.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.22.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.23.attention.self.query.weight
06/27 04:10:09 PM n: encoder.layer.23.attention.self.query.bias
06/27 04:10:09 PM n: encoder.layer.23.attention.self.key.weight
06/27 04:10:09 PM n: encoder.layer.23.attention.self.key.bias
06/27 04:10:09 PM n: encoder.layer.23.attention.self.value.weight
06/27 04:10:09 PM n: encoder.layer.23.attention.self.value.bias
06/27 04:10:09 PM n: encoder.layer.23.attention.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.23.attention.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: encoder.layer.23.intermediate.dense.weight
06/27 04:10:09 PM n: encoder.layer.23.intermediate.dense.bias
06/27 04:10:09 PM n: encoder.layer.23.output.dense.weight
06/27 04:10:09 PM n: encoder.layer.23.output.dense.bias
06/27 04:10:09 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 04:10:09 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 04:10:09 PM n: pooler.dense.weight
06/27 04:10:09 PM n: pooler.dense.bias
06/27 04:10:09 PM n: roberta.embeddings.word_embeddings.weight
06/27 04:10:09 PM n: roberta.embeddings.position_embeddings.weight
06/27 04:10:09 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 04:10:09 PM n: roberta.embeddings.LayerNorm.weight
06/27 04:10:09 PM n: roberta.embeddings.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 04:10:09 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 04:10:09 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 04:10:09 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 04:10:09 PM n: roberta.pooler.dense.weight
06/27 04:10:09 PM n: roberta.pooler.dense.bias
06/27 04:10:09 PM n: lm_head.bias
06/27 04:10:09 PM n: lm_head.dense.weight
06/27 04:10:09 PM n: lm_head.dense.bias
06/27 04:10:09 PM n: lm_head.layer_norm.weight
06/27 04:10:09 PM n: lm_head.layer_norm.bias
06/27 04:10:09 PM n: lm_head.decoder.weight
06/27 04:10:09 PM Total parameters: 763292761
06/27 04:10:09 PM ***** LOSS printing *****
06/27 04:10:09 PM loss
06/27 04:10:09 PM tensor(20.5568, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:09 PM ***** LOSS printing *****
06/27 04:10:09 PM loss
06/27 04:10:09 PM tensor(13.7603, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:09 PM ***** LOSS printing *****
06/27 04:10:09 PM loss
06/27 04:10:09 PM tensor(8.6347, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:10 PM ***** LOSS printing *****
06/27 04:10:10 PM loss
06/27 04:10:10 PM tensor(5.4006, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:10 PM ***** Running evaluation MLM *****
06/27 04:10:10 PM   Epoch = 0 iter 4 step
06/27 04:10:10 PM   Num examples = 16
06/27 04:10:10 PM   Batch size = 32
06/27 04:10:10 PM ***** Eval results *****
06/27 04:10:10 PM   acc = 0.8125
06/27 04:10:10 PM   cls_loss = 12.088091135025024
06/27 04:10:10 PM   eval_loss = 3.3981950283050537
06/27 04:10:10 PM   global_step = 4
06/27 04:10:10 PM   loss = 12.088091135025024
06/27 04:10:10 PM ***** Save model *****
06/27 04:10:10 PM ***** Test Dataset Eval Result *****
06/27 04:10:38 PM ***** Eval results *****
06/27 04:10:38 PM   acc = 0.8669724770642202
06/27 04:10:38 PM   cls_loss = 12.088091135025024
06/27 04:10:38 PM   eval_loss = 3.1672912495476857
06/27 04:10:38 PM   global_step = 4
06/27 04:10:38 PM   loss = 12.088091135025024
06/27 04:10:41 PM ***** LOSS printing *****
06/27 04:10:41 PM loss
06/27 04:10:41 PM tensor(4.3484, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:42 PM ***** LOSS printing *****
06/27 04:10:42 PM loss
06/27 04:10:42 PM tensor(3.0213, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:42 PM ***** LOSS printing *****
06/27 04:10:42 PM loss
06/27 04:10:42 PM tensor(2.5305, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:42 PM ***** LOSS printing *****
06/27 04:10:42 PM loss
06/27 04:10:42 PM tensor(1.6266, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:42 PM ***** LOSS printing *****
06/27 04:10:42 PM loss
06/27 04:10:42 PM tensor(3.0107, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:10:42 PM ***** Running evaluation MLM *****
06/27 04:10:42 PM   Epoch = 0 iter 9 step
06/27 04:10:42 PM   Num examples = 16
06/27 04:10:42 PM   Batch size = 32
06/27 04:10:43 PM ***** Eval results *****
06/27 04:10:43 PM   acc = 1.0
06/27 04:10:43 PM   cls_loss = 6.987752728992039
06/27 04:10:43 PM   eval_loss = 1.4096614122390747
06/27 04:10:43 PM   global_step = 9
06/27 04:10:43 PM   loss = 6.987752728992039
06/27 04:10:43 PM ***** Save model *****
06/27 04:10:43 PM ***** Test Dataset Eval Result *****
06/27 04:11:10 PM ***** Eval results *****
06/27 04:11:10 PM   acc = 0.9059633027522935
06/27 04:11:10 PM   cls_loss = 6.987752728992039
06/27 04:11:10 PM   eval_loss = 1.33383409678936
06/27 04:11:10 PM   global_step = 9
06/27 04:11:10 PM   loss = 6.987752728992039
06/27 04:11:14 PM ***** LOSS printing *****
06/27 04:11:14 PM loss
06/27 04:11:14 PM tensor(2.5169, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:15 PM ***** LOSS printing *****
06/27 04:11:15 PM loss
06/27 04:11:15 PM tensor(3.2538, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:15 PM ***** LOSS printing *****
06/27 04:11:15 PM loss
06/27 04:11:15 PM tensor(2.8018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:15 PM ***** LOSS printing *****
06/27 04:11:15 PM loss
06/27 04:11:15 PM tensor(3.3238, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:15 PM ***** LOSS printing *****
06/27 04:11:15 PM loss
06/27 04:11:15 PM tensor(1.7513, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:15 PM ***** Running evaluation MLM *****
06/27 04:11:15 PM   Epoch = 1 iter 14 step
06/27 04:11:15 PM   Num examples = 16
06/27 04:11:15 PM   Batch size = 32
06/27 04:11:16 PM ***** Eval results *****
06/27 04:11:16 PM   acc = 0.6875
06/27 04:11:16 PM   cls_loss = 2.5375820994377136
06/27 04:11:16 PM   eval_loss = 1.1412200927734375
06/27 04:11:16 PM   global_step = 14
06/27 04:11:16 PM   loss = 2.5375820994377136
06/27 04:11:16 PM ***** LOSS printing *****
06/27 04:11:16 PM loss
06/27 04:11:16 PM tensor(2.3547, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:16 PM ***** LOSS printing *****
06/27 04:11:16 PM loss
06/27 04:11:16 PM tensor(2.0054, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:16 PM ***** LOSS printing *****
06/27 04:11:16 PM loss
06/27 04:11:16 PM tensor(2.7522, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:17 PM ***** LOSS printing *****
06/27 04:11:17 PM loss
06/27 04:11:17 PM tensor(4.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:17 PM ***** LOSS printing *****
06/27 04:11:17 PM loss
06/27 04:11:17 PM tensor(2.4666, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:17 PM ***** Running evaluation MLM *****
06/27 04:11:17 PM   Epoch = 1 iter 19 step
06/27 04:11:17 PM   Num examples = 16
06/27 04:11:17 PM   Batch size = 32
06/27 04:11:17 PM ***** Eval results *****
06/27 04:11:17 PM   acc = 0.8125
06/27 04:11:17 PM   cls_loss = 2.6725718123572215
06/27 04:11:17 PM   eval_loss = 2.3306984901428223
06/27 04:11:17 PM   global_step = 19
06/27 04:11:17 PM   loss = 2.6725718123572215
06/27 04:11:18 PM ***** LOSS printing *****
06/27 04:11:18 PM loss
06/27 04:11:18 PM tensor(1.2472, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:18 PM ***** LOSS printing *****
06/27 04:11:18 PM loss
06/27 04:11:18 PM tensor(0.8625, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:18 PM ***** LOSS printing *****
06/27 04:11:18 PM loss
06/27 04:11:18 PM tensor(1.4229, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:18 PM ***** LOSS printing *****
06/27 04:11:18 PM loss
06/27 04:11:18 PM tensor(2.6027, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:18 PM ***** LOSS printing *****
06/27 04:11:18 PM loss
06/27 04:11:18 PM tensor(3.1490, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:19 PM ***** Running evaluation MLM *****
06/27 04:11:19 PM   Epoch = 1 iter 24 step
06/27 04:11:19 PM   Num examples = 16
06/27 04:11:19 PM   Batch size = 32
06/27 04:11:19 PM ***** Eval results *****
06/27 04:11:19 PM   acc = 0.75
06/27 04:11:19 PM   cls_loss = 2.3326931993166604
06/27 04:11:19 PM   eval_loss = 2.6115338802337646
06/27 04:11:19 PM   global_step = 24
06/27 04:11:19 PM   loss = 2.3326931993166604
06/27 04:11:19 PM ***** LOSS printing *****
06/27 04:11:19 PM loss
06/27 04:11:19 PM tensor(2.7694, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:19 PM ***** LOSS printing *****
06/27 04:11:19 PM loss
06/27 04:11:19 PM tensor(1.0643, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:20 PM ***** LOSS printing *****
06/27 04:11:20 PM loss
06/27 04:11:20 PM tensor(0.6702, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:20 PM ***** LOSS printing *****
06/27 04:11:20 PM loss
06/27 04:11:20 PM tensor(2.7191, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:20 PM ***** LOSS printing *****
06/27 04:11:20 PM loss
06/27 04:11:20 PM tensor(2.9251, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:20 PM ***** Running evaluation MLM *****
06/27 04:11:20 PM   Epoch = 2 iter 29 step
06/27 04:11:20 PM   Num examples = 16
06/27 04:11:20 PM   Batch size = 32
06/27 04:11:21 PM ***** Eval results *****
06/27 04:11:21 PM   acc = 0.8125
06/27 04:11:21 PM   cls_loss = 2.0296326994895937
06/27 04:11:21 PM   eval_loss = 1.5283055305480957
06/27 04:11:21 PM   global_step = 29
06/27 04:11:21 PM   loss = 2.0296326994895937
06/27 04:11:21 PM ***** LOSS printing *****
06/27 04:11:21 PM loss
06/27 04:11:21 PM tensor(1.7974, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:21 PM ***** LOSS printing *****
06/27 04:11:21 PM loss
06/27 04:11:21 PM tensor(2.4117, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:21 PM ***** LOSS printing *****
06/27 04:11:21 PM loss
06/27 04:11:21 PM tensor(1.7882, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:21 PM ***** LOSS printing *****
06/27 04:11:21 PM loss
06/27 04:11:21 PM tensor(0.9545, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:22 PM ***** LOSS printing *****
06/27 04:11:22 PM loss
06/27 04:11:22 PM tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:22 PM ***** Running evaluation MLM *****
06/27 04:11:22 PM   Epoch = 2 iter 34 step
06/27 04:11:22 PM   Num examples = 16
06/27 04:11:22 PM   Batch size = 32
06/27 04:11:22 PM ***** Eval results *****
06/27 04:11:22 PM   acc = 0.8125
06/27 04:11:22 PM   cls_loss = 1.8749464929103852
06/27 04:11:22 PM   eval_loss = 1.4706521034240723
06/27 04:11:22 PM   global_step = 34
06/27 04:11:22 PM   loss = 1.8749464929103852
06/27 04:11:22 PM ***** LOSS printing *****
06/27 04:11:22 PM loss
06/27 04:11:22 PM tensor(2.4371, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:23 PM ***** LOSS printing *****
06/27 04:11:23 PM loss
06/27 04:11:23 PM tensor(2.1463, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:23 PM ***** LOSS printing *****
06/27 04:11:23 PM loss
06/27 04:11:23 PM tensor(1.7870, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:23 PM ***** LOSS printing *****
06/27 04:11:23 PM loss
06/27 04:11:23 PM tensor(1.8984, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:23 PM ***** LOSS printing *****
06/27 04:11:23 PM loss
06/27 04:11:23 PM tensor(1.9729, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:23 PM ***** Running evaluation MLM *****
06/27 04:11:23 PM   Epoch = 3 iter 39 step
06/27 04:11:23 PM   Num examples = 16
06/27 04:11:23 PM   Batch size = 32
06/27 04:11:24 PM ***** Eval results *****
06/27 04:11:24 PM   acc = 0.875
06/27 04:11:24 PM   cls_loss = 1.8861010074615479
06/27 04:11:24 PM   eval_loss = 1.717342495918274
06/27 04:11:24 PM   global_step = 39
06/27 04:11:24 PM   loss = 1.8861010074615479
06/27 04:11:24 PM ***** LOSS printing *****
06/27 04:11:24 PM loss
06/27 04:11:24 PM tensor(1.6317, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:24 PM ***** LOSS printing *****
06/27 04:11:24 PM loss
06/27 04:11:24 PM tensor(1.8047, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:24 PM ***** LOSS printing *****
06/27 04:11:24 PM loss
06/27 04:11:24 PM tensor(1.5871, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:25 PM ***** LOSS printing *****
06/27 04:11:25 PM loss
06/27 04:11:25 PM tensor(1.9838, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:25 PM ***** LOSS printing *****
06/27 04:11:25 PM loss
06/27 04:11:25 PM tensor(2.2589, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:25 PM ***** Running evaluation MLM *****
06/27 04:11:25 PM   Epoch = 3 iter 44 step
06/27 04:11:25 PM   Num examples = 16
06/27 04:11:25 PM   Batch size = 32
06/27 04:11:26 PM ***** Eval results *****
06/27 04:11:26 PM   acc = 0.8125
06/27 04:11:26 PM   cls_loss = 1.8655714988708496
06/27 04:11:26 PM   eval_loss = 1.9936542510986328
06/27 04:11:26 PM   global_step = 44
06/27 04:11:26 PM   loss = 1.8655714988708496
06/27 04:11:26 PM ***** LOSS printing *****
06/27 04:11:26 PM loss
06/27 04:11:26 PM tensor(1.1592, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:26 PM ***** LOSS printing *****
06/27 04:11:26 PM loss
06/27 04:11:26 PM tensor(1.6199, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:26 PM ***** LOSS printing *****
06/27 04:11:26 PM loss
06/27 04:11:26 PM tensor(1.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:26 PM ***** LOSS printing *****
06/27 04:11:26 PM loss
06/27 04:11:26 PM tensor(1.1682, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:26 PM ***** LOSS printing *****
06/27 04:11:26 PM loss
06/27 04:11:26 PM tensor(1.0096, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:27 PM ***** Running evaluation MLM *****
06/27 04:11:27 PM   Epoch = 4 iter 49 step
06/27 04:11:27 PM   Num examples = 16
06/27 04:11:27 PM   Batch size = 32
06/27 04:11:27 PM ***** Eval results *****
06/27 04:11:27 PM   acc = 0.8125
06/27 04:11:27 PM   cls_loss = 1.0096096992492676
06/27 04:11:27 PM   eval_loss = 1.573531985282898
06/27 04:11:27 PM   global_step = 49
06/27 04:11:27 PM   loss = 1.0096096992492676
06/27 04:11:27 PM ***** LOSS printing *****
06/27 04:11:27 PM loss
06/27 04:11:27 PM tensor(1.3442, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:27 PM ***** LOSS printing *****
06/27 04:11:27 PM loss
06/27 04:11:27 PM tensor(1.6512, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:28 PM ***** LOSS printing *****
06/27 04:11:28 PM loss
06/27 04:11:28 PM tensor(1.1499, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:28 PM ***** LOSS printing *****
06/27 04:11:28 PM loss
06/27 04:11:28 PM tensor(1.2527, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:28 PM ***** LOSS printing *****
06/27 04:11:28 PM loss
06/27 04:11:28 PM tensor(1.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:28 PM ***** Running evaluation MLM *****
06/27 04:11:28 PM   Epoch = 4 iter 54 step
06/27 04:11:28 PM   Num examples = 16
06/27 04:11:28 PM   Batch size = 32
06/27 04:11:29 PM ***** Eval results *****
06/27 04:11:29 PM   acc = 0.8125
06/27 04:11:29 PM   cls_loss = 1.2509557803471882
06/27 04:11:29 PM   eval_loss = 1.6167669296264648
06/27 04:11:29 PM   global_step = 54
06/27 04:11:29 PM   loss = 1.2509557803471882
06/27 04:11:29 PM ***** LOSS printing *****
06/27 04:11:29 PM loss
06/27 04:11:29 PM tensor(1.0699, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:29 PM ***** LOSS printing *****
06/27 04:11:29 PM loss
06/27 04:11:29 PM tensor(1.1413, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:29 PM ***** LOSS printing *****
06/27 04:11:29 PM loss
06/27 04:11:29 PM tensor(1.4639, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:29 PM ***** LOSS printing *****
06/27 04:11:29 PM loss
06/27 04:11:29 PM tensor(1.3712, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:30 PM ***** LOSS printing *****
06/27 04:11:30 PM loss
06/27 04:11:30 PM tensor(1.5416, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:30 PM ***** Running evaluation MLM *****
06/27 04:11:30 PM   Epoch = 4 iter 59 step
06/27 04:11:30 PM   Num examples = 16
06/27 04:11:30 PM   Batch size = 32
06/27 04:11:30 PM ***** Eval results *****
06/27 04:11:30 PM   acc = 0.6875
06/27 04:11:30 PM   cls_loss = 1.2812407125126233
06/27 04:11:30 PM   eval_loss = 1.489118218421936
06/27 04:11:30 PM   global_step = 59
06/27 04:11:30 PM   loss = 1.2812407125126233
06/27 04:11:30 PM ***** LOSS printing *****
06/27 04:11:30 PM loss
06/27 04:11:30 PM tensor(1.4321, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:31 PM ***** LOSS printing *****
06/27 04:11:31 PM loss
06/27 04:11:31 PM tensor(0.9286, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:31 PM ***** LOSS printing *****
06/27 04:11:31 PM loss
06/27 04:11:31 PM tensor(1.2358, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:31 PM ***** LOSS printing *****
06/27 04:11:31 PM loss
06/27 04:11:31 PM tensor(1.5465, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:31 PM ***** LOSS printing *****
06/27 04:11:31 PM loss
06/27 04:11:31 PM tensor(1.0686, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:31 PM ***** Running evaluation MLM *****
06/27 04:11:31 PM   Epoch = 5 iter 64 step
06/27 04:11:31 PM   Num examples = 16
06/27 04:11:31 PM   Batch size = 32
06/27 04:11:32 PM ***** Eval results *****
06/27 04:11:32 PM   acc = 0.75
06/27 04:11:32 PM   cls_loss = 1.1948632597923279
06/27 04:11:32 PM   eval_loss = 1.700121521949768
06/27 04:11:32 PM   global_step = 64
06/27 04:11:32 PM   loss = 1.1948632597923279
06/27 04:11:32 PM ***** LOSS printing *****
06/27 04:11:32 PM loss
06/27 04:11:32 PM tensor(1.8534, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:32 PM ***** LOSS printing *****
06/27 04:11:32 PM loss
06/27 04:11:32 PM tensor(1.3369, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:32 PM ***** LOSS printing *****
06/27 04:11:32 PM loss
06/27 04:11:32 PM tensor(1.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:33 PM ***** LOSS printing *****
06/27 04:11:33 PM loss
06/27 04:11:33 PM tensor(1.2917, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:33 PM ***** LOSS printing *****
06/27 04:11:33 PM loss
06/27 04:11:33 PM tensor(1.3727, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:33 PM ***** Running evaluation MLM *****
06/27 04:11:33 PM   Epoch = 5 iter 69 step
06/27 04:11:33 PM   Num examples = 16
06/27 04:11:33 PM   Batch size = 32
06/27 04:11:34 PM ***** Eval results *****
06/27 04:11:34 PM   acc = 0.8125
06/27 04:11:34 PM   cls_loss = 1.2963715394337971
06/27 04:11:34 PM   eval_loss = 2.105684757232666
06/27 04:11:34 PM   global_step = 69
06/27 04:11:34 PM   loss = 1.2963715394337971
06/27 04:11:34 PM ***** LOSS printing *****
06/27 04:11:34 PM loss
06/27 04:11:34 PM tensor(1.3714, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:34 PM ***** LOSS printing *****
06/27 04:11:34 PM loss
06/27 04:11:34 PM tensor(1.2809, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:34 PM ***** LOSS printing *****
06/27 04:11:34 PM loss
06/27 04:11:34 PM tensor(1.1758, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:34 PM ***** LOSS printing *****
06/27 04:11:34 PM loss
06/27 04:11:34 PM tensor(1.0761, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:35 PM ***** LOSS printing *****
06/27 04:11:35 PM loss
06/27 04:11:35 PM tensor(1.2263, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:35 PM ***** Running evaluation MLM *****
06/27 04:11:35 PM   Epoch = 6 iter 74 step
06/27 04:11:35 PM   Num examples = 16
06/27 04:11:35 PM   Batch size = 32
06/27 04:11:35 PM ***** Eval results *****
06/27 04:11:35 PM   acc = 0.8125
06/27 04:11:35 PM   cls_loss = 1.1512081623077393
06/27 04:11:35 PM   eval_loss = 2.1543045043945312
06/27 04:11:35 PM   global_step = 74
06/27 04:11:35 PM   loss = 1.1512081623077393
06/27 04:11:35 PM ***** LOSS printing *****
06/27 04:11:35 PM loss
06/27 04:11:35 PM tensor(0.9185, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:35 PM ***** LOSS printing *****
06/27 04:11:35 PM loss
06/27 04:11:35 PM tensor(0.8722, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:36 PM ***** LOSS printing *****
06/27 04:11:36 PM loss
06/27 04:11:36 PM tensor(1.5775, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:36 PM ***** LOSS printing *****
06/27 04:11:36 PM loss
06/27 04:11:36 PM tensor(1.4149, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:36 PM ***** LOSS printing *****
06/27 04:11:36 PM loss
06/27 04:11:36 PM tensor(1.9093, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:36 PM ***** Running evaluation MLM *****
06/27 04:11:36 PM   Epoch = 6 iter 79 step
06/27 04:11:36 PM   Num examples = 16
06/27 04:11:36 PM   Batch size = 32
06/27 04:11:37 PM ***** Eval results *****
06/27 04:11:37 PM   acc = 0.8125
06/27 04:11:37 PM   cls_loss = 1.2849707348006112
06/27 04:11:37 PM   eval_loss = 2.028040647506714
06/27 04:11:37 PM   global_step = 79
06/27 04:11:37 PM   loss = 1.2849707348006112
06/27 04:11:37 PM ***** LOSS printing *****
06/27 04:11:37 PM loss
06/27 04:11:37 PM tensor(2.0896, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:37 PM ***** LOSS printing *****
06/27 04:11:37 PM loss
06/27 04:11:37 PM tensor(1.3076, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:37 PM ***** LOSS printing *****
06/27 04:11:37 PM loss
06/27 04:11:37 PM tensor(1.0088, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:38 PM ***** LOSS printing *****
06/27 04:11:38 PM loss
06/27 04:11:38 PM tensor(1.9002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:38 PM ***** LOSS printing *****
06/27 04:11:38 PM loss
06/27 04:11:38 PM tensor(1.4088, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:38 PM ***** Running evaluation MLM *****
06/27 04:11:38 PM   Epoch = 6 iter 84 step
06/27 04:11:38 PM   Num examples = 16
06/27 04:11:38 PM   Batch size = 32
06/27 04:11:38 PM ***** Eval results *****
06/27 04:11:38 PM   acc = 0.875
06/27 04:11:38 PM   cls_loss = 1.3924823651711147
06/27 04:11:38 PM   eval_loss = 1.380397915840149
06/27 04:11:38 PM   global_step = 84
06/27 04:11:38 PM   loss = 1.3924823651711147
06/27 04:11:38 PM ***** LOSS printing *****
06/27 04:11:38 PM loss
06/27 04:11:38 PM tensor(1.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:39 PM ***** LOSS printing *****
06/27 04:11:39 PM loss
06/27 04:11:39 PM tensor(1.1929, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:39 PM ***** LOSS printing *****
06/27 04:11:39 PM loss
06/27 04:11:39 PM tensor(1.4249, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:39 PM ***** LOSS printing *****
06/27 04:11:39 PM loss
06/27 04:11:39 PM tensor(1.2945, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:39 PM ***** LOSS printing *****
06/27 04:11:39 PM loss
06/27 04:11:39 PM tensor(1.3769, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:40 PM ***** Running evaluation MLM *****
06/27 04:11:40 PM   Epoch = 7 iter 89 step
06/27 04:11:40 PM   Num examples = 16
06/27 04:11:40 PM   Batch size = 32
06/27 04:11:40 PM ***** Eval results *****
06/27 04:11:40 PM   acc = 0.875
06/27 04:11:40 PM   cls_loss = 1.2680905342102051
06/27 04:11:40 PM   eval_loss = 1.5569591522216797
06/27 04:11:40 PM   global_step = 89
06/27 04:11:40 PM   loss = 1.2680905342102051
06/27 04:11:40 PM ***** LOSS printing *****
06/27 04:11:40 PM loss
06/27 04:11:40 PM tensor(1.4200, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:40 PM ***** LOSS printing *****
06/27 04:11:40 PM loss
06/27 04:11:40 PM tensor(1.3660, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:41 PM ***** LOSS printing *****
06/27 04:11:41 PM loss
06/27 04:11:41 PM tensor(1.0906, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:41 PM ***** LOSS printing *****
06/27 04:11:41 PM loss
06/27 04:11:41 PM tensor(1.4401, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:41 PM ***** LOSS printing *****
06/27 04:11:41 PM loss
06/27 04:11:41 PM tensor(1.2746, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:41 PM ***** Running evaluation MLM *****
06/27 04:11:41 PM   Epoch = 7 iter 94 step
06/27 04:11:41 PM   Num examples = 16
06/27 04:11:41 PM   Batch size = 32
06/27 04:11:42 PM ***** Eval results *****
06/27 04:11:42 PM   acc = 0.8125
06/27 04:11:42 PM   cls_loss = 1.2931719303131104
06/27 04:11:42 PM   eval_loss = 1.6237900257110596
06/27 04:11:42 PM   global_step = 94
06/27 04:11:42 PM   loss = 1.2931719303131104
06/27 04:11:42 PM ***** LOSS printing *****
06/27 04:11:42 PM loss
06/27 04:11:42 PM tensor(1.1881, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:42 PM ***** LOSS printing *****
06/27 04:11:42 PM loss
06/27 04:11:42 PM tensor(1.2529, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:42 PM ***** LOSS printing *****
06/27 04:11:42 PM loss
06/27 04:11:42 PM tensor(1.4334, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:42 PM ***** LOSS printing *****
06/27 04:11:42 PM loss
06/27 04:11:42 PM tensor(0.9294, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:43 PM ***** LOSS printing *****
06/27 04:11:43 PM loss
06/27 04:11:43 PM tensor(1.4477, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:43 PM ***** Running evaluation MLM *****
06/27 04:11:43 PM   Epoch = 8 iter 99 step
06/27 04:11:43 PM   Num examples = 16
06/27 04:11:43 PM   Batch size = 32
06/27 04:11:43 PM ***** Eval results *****
06/27 04:11:43 PM   acc = 0.8125
06/27 04:11:43 PM   cls_loss = 1.2701875964800518
06/27 04:11:43 PM   eval_loss = 1.8414115905761719
06/27 04:11:43 PM   global_step = 99
06/27 04:11:43 PM   loss = 1.2701875964800518
06/27 04:11:43 PM ***** LOSS printing *****
06/27 04:11:43 PM loss
06/27 04:11:43 PM tensor(1.5917, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:44 PM ***** LOSS printing *****
06/27 04:11:44 PM loss
06/27 04:11:44 PM tensor(2.3683, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:44 PM ***** LOSS printing *****
06/27 04:11:44 PM loss
06/27 04:11:44 PM tensor(0.9691, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:44 PM ***** LOSS printing *****
06/27 04:11:44 PM loss
06/27 04:11:44 PM tensor(1.2908, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:44 PM ***** LOSS printing *****
06/27 04:11:44 PM loss
06/27 04:11:44 PM tensor(1.3780, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:44 PM ***** Running evaluation MLM *****
06/27 04:11:44 PM   Epoch = 8 iter 104 step
06/27 04:11:44 PM   Num examples = 16
06/27 04:11:44 PM   Batch size = 32
06/27 04:11:45 PM ***** Eval results *****
06/27 04:11:45 PM   acc = 0.875
06/27 04:11:45 PM   cls_loss = 1.426058568060398
06/27 04:11:45 PM   eval_loss = 1.565136194229126
06/27 04:11:45 PM   global_step = 104
06/27 04:11:45 PM   loss = 1.426058568060398
06/27 04:11:45 PM ***** LOSS printing *****
06/27 04:11:45 PM loss
06/27 04:11:45 PM tensor(1.4760, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:45 PM ***** LOSS printing *****
06/27 04:11:45 PM loss
06/27 04:11:45 PM tensor(1.5126, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:45 PM ***** LOSS printing *****
06/27 04:11:45 PM loss
06/27 04:11:45 PM tensor(1.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:46 PM ***** LOSS printing *****
06/27 04:11:46 PM loss
06/27 04:11:46 PM tensor(1.2523, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:46 PM ***** LOSS printing *****
06/27 04:11:46 PM loss
06/27 04:11:46 PM tensor(1.1076, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:46 PM ***** Running evaluation MLM *****
06/27 04:11:46 PM   Epoch = 9 iter 109 step
06/27 04:11:46 PM   Num examples = 16
06/27 04:11:46 PM   Batch size = 32
06/27 04:11:47 PM ***** Eval results *****
06/27 04:11:47 PM   acc = 0.9375
06/27 04:11:47 PM   cls_loss = 1.1076442003250122
06/27 04:11:47 PM   eval_loss = 1.502748966217041
06/27 04:11:47 PM   global_step = 109
06/27 04:11:47 PM   loss = 1.1076442003250122
06/27 04:11:47 PM ***** LOSS printing *****
06/27 04:11:47 PM loss
06/27 04:11:47 PM tensor(1.1818, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:47 PM ***** LOSS printing *****
06/27 04:11:47 PM loss
06/27 04:11:47 PM tensor(1.1225, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:47 PM ***** LOSS printing *****
06/27 04:11:47 PM loss
06/27 04:11:47 PM tensor(0.9789, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:47 PM ***** LOSS printing *****
06/27 04:11:47 PM loss
06/27 04:11:47 PM tensor(1.2027, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:47 PM ***** LOSS printing *****
06/27 04:11:47 PM loss
06/27 04:11:47 PM tensor(1.1679, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:48 PM ***** Running evaluation MLM *****
06/27 04:11:48 PM   Epoch = 9 iter 114 step
06/27 04:11:48 PM   Num examples = 16
06/27 04:11:48 PM   Batch size = 32
06/27 04:11:48 PM ***** Eval results *****
06/27 04:11:48 PM   acc = 0.9375
06/27 04:11:48 PM   cls_loss = 1.1269094149271648
06/27 04:11:48 PM   eval_loss = 1.4236528873443604
06/27 04:11:48 PM   global_step = 114
06/27 04:11:48 PM   loss = 1.1269094149271648
06/27 04:11:48 PM ***** LOSS printing *****
06/27 04:11:48 PM loss
06/27 04:11:48 PM tensor(1.2087, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:48 PM ***** LOSS printing *****
06/27 04:11:48 PM loss
06/27 04:11:48 PM tensor(1.1688, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:49 PM ***** LOSS printing *****
06/27 04:11:49 PM loss
06/27 04:11:49 PM tensor(1.2882, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:49 PM ***** LOSS printing *****
06/27 04:11:49 PM loss
06/27 04:11:49 PM tensor(1.4234, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:49 PM ***** LOSS printing *****
06/27 04:11:49 PM loss
06/27 04:11:49 PM tensor(1.4146, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:11:49 PM ***** Running evaluation MLM *****
06/27 04:11:49 PM   Epoch = 9 iter 119 step
06/27 04:11:49 PM   Num examples = 16
06/27 04:11:49 PM   Batch size = 32
06/27 04:11:50 PM ***** Eval results *****
06/27 04:11:50 PM   acc = 0.9375
06/27 04:11:50 PM   cls_loss = 1.2059226577932185
06/27 04:11:50 PM   eval_loss = 1.4897106885910034
06/27 04:11:50 PM   global_step = 119
06/27 04:11:50 PM   loss = 1.2059226577932185
06/27 04:11:50 PM ***** LOSS printing *****
06/27 04:11:50 PM loss
06/27 04:11:50 PM tensor(1.2792, device='cuda:0', grad_fn=<NllLossBackward0>)

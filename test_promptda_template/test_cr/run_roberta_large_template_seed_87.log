06/27 04:23:31 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-87/', data_seed=87, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 04:23:31 PM device: cuda n_gpu: 1
06/27 04:23:31 PM Writing example 0 of 48
06/27 04:23:31 PM *** Example ***
06/27 04:23:31 PM guid: train-1
06/27 04:23:31 PM tokens: <s> i Ġlove Ġthe Ġcontinuous Ġshot Ġmode Ġ, Ġwhich Ġallows Ġyou Ġto Ġtake Ġup Ġto Ġ16 Ġp ix Ġin Ġrapid Ġsuccession Ġ-- Ġgreat Ġfor Ġaction Ġshots Ġ. </s> ĠIt Ġis <mask>
06/27 04:23:31 PM input_ids: 0 118 657 5 11152 738 5745 2156 61 2386 47 7 185 62 7 545 181 3181 11 6379 15436 480 372 13 814 2347 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:31 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:31 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:31 PM label: ['Ġfantastic']
06/27 04:23:31 PM Writing example 0 of 16
06/27 04:23:31 PM *** Example ***
06/27 04:23:31 PM guid: dev-1
06/27 04:23:31 PM tokens: <s> i Ġlove Ġthis Ġproduct Ġ! Ġ. </s> ĠIt Ġis <mask>
06/27 04:23:31 PM input_ids: 0 118 657 42 1152 27785 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:31 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:31 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:31 PM label: ['Ġfantastic']
06/27 04:23:32 PM Writing example 0 of 2000
06/27 04:23:32 PM *** Example ***
06/27 04:23:32 PM guid: dev-1
06/27 04:23:32 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 04:23:32 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:32 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:32 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:23:32 PM label: ['Ġterrible']
06/27 04:23:45 PM ***** Running training *****
06/27 04:23:45 PM   Num examples = 48
06/27 04:23:45 PM   Batch size = 4
06/27 04:23:45 PM   Num steps = 120
06/27 04:23:45 PM n: embeddings.word_embeddings.weight
06/27 04:23:45 PM n: embeddings.position_embeddings.weight
06/27 04:23:45 PM n: embeddings.token_type_embeddings.weight
06/27 04:23:45 PM n: embeddings.LayerNorm.weight
06/27 04:23:45 PM n: embeddings.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.0.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.0.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.0.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.0.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.0.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.0.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.0.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.0.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.0.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.0.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.0.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.0.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.1.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.1.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.1.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.1.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.1.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.1.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.1.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.1.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.1.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.1.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.1.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.1.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.2.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.2.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.2.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.2.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.2.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.2.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.2.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.2.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.2.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.2.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.2.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.2.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.3.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.3.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.3.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.3.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.3.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.3.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.3.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.3.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.3.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.3.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.3.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.3.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.4.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.4.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.4.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.4.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.4.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.4.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.4.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.4.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.4.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.4.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.4.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.4.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.5.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.5.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.5.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.5.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.5.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.5.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.5.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.5.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.5.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.5.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.5.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.5.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.6.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.6.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.6.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.6.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.6.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.6.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.6.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.6.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.6.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.6.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.6.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.6.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.7.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.7.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.7.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.7.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.7.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.7.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.7.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.7.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.7.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.7.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.7.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.7.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.8.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.8.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.8.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.8.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.8.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.8.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.8.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.8.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.8.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.8.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.8.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.8.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.9.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.9.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.9.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.9.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.9.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.9.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.9.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.9.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.9.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.9.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.9.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.9.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.10.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.10.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.10.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.10.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.10.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.10.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.10.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.10.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.10.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.10.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.10.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.10.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.11.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.11.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.11.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.11.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.11.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.11.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.11.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.11.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.11.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.11.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.11.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.11.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.12.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.12.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.12.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.12.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.12.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.12.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.12.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.12.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.12.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.12.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.12.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.12.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.13.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.13.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.13.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.13.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.13.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.13.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.13.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.13.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.13.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.13.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.13.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.13.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.14.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.14.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.14.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.14.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.14.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.14.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.14.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.14.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.14.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.14.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.14.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.14.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.15.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.15.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.15.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.15.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.15.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.15.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.15.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.15.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.15.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.15.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.15.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.15.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.16.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.16.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.16.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.16.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.16.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.16.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.16.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.16.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.16.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.16.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.16.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.16.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.17.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.17.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.17.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.17.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.17.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.17.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.17.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.17.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.17.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.17.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.17.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.17.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.18.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.18.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.18.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.18.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.18.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.18.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.18.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.18.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.18.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.18.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.18.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.18.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.19.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.19.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.19.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.19.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.19.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.19.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.19.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.19.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.19.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.19.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.19.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.19.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.20.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.20.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.20.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.20.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.20.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.20.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.20.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.20.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.20.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.20.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.20.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.20.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.21.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.21.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.21.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.21.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.21.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.21.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.21.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.21.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.21.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.21.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.21.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.21.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.22.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.22.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.22.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.22.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.22.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.22.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.22.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.22.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.22.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.22.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.22.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.22.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.23.attention.self.query.weight
06/27 04:23:45 PM n: encoder.layer.23.attention.self.query.bias
06/27 04:23:45 PM n: encoder.layer.23.attention.self.key.weight
06/27 04:23:45 PM n: encoder.layer.23.attention.self.key.bias
06/27 04:23:45 PM n: encoder.layer.23.attention.self.value.weight
06/27 04:23:45 PM n: encoder.layer.23.attention.self.value.bias
06/27 04:23:45 PM n: encoder.layer.23.attention.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.23.attention.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: encoder.layer.23.intermediate.dense.weight
06/27 04:23:45 PM n: encoder.layer.23.intermediate.dense.bias
06/27 04:23:45 PM n: encoder.layer.23.output.dense.weight
06/27 04:23:45 PM n: encoder.layer.23.output.dense.bias
06/27 04:23:45 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 04:23:45 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 04:23:45 PM n: pooler.dense.weight
06/27 04:23:45 PM n: pooler.dense.bias
06/27 04:23:45 PM n: roberta.embeddings.word_embeddings.weight
06/27 04:23:45 PM n: roberta.embeddings.position_embeddings.weight
06/27 04:23:45 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 04:23:45 PM n: roberta.embeddings.LayerNorm.weight
06/27 04:23:45 PM n: roberta.embeddings.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 04:23:45 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 04:23:45 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 04:23:45 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 04:23:45 PM n: roberta.pooler.dense.weight
06/27 04:23:45 PM n: roberta.pooler.dense.bias
06/27 04:23:45 PM n: lm_head.bias
06/27 04:23:45 PM n: lm_head.dense.weight
06/27 04:23:45 PM n: lm_head.dense.bias
06/27 04:23:45 PM n: lm_head.layer_norm.weight
06/27 04:23:45 PM n: lm_head.layer_norm.bias
06/27 04:23:45 PM n: lm_head.decoder.weight
06/27 04:23:45 PM Total parameters: 763292761
06/27 04:23:45 PM ***** LOSS printing *****
06/27 04:23:45 PM loss
06/27 04:23:45 PM tensor(19.7783, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:23:45 PM ***** LOSS printing *****
06/27 04:23:45 PM loss
06/27 04:23:45 PM tensor(13.7580, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:23:45 PM ***** LOSS printing *****
06/27 04:23:45 PM loss
06/27 04:23:45 PM tensor(8.0930, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:23:45 PM ***** LOSS printing *****
06/27 04:23:45 PM loss
06/27 04:23:45 PM tensor(5.1095, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:23:46 PM ***** Running evaluation MLM *****
06/27 04:23:46 PM   Epoch = 0 iter 4 step
06/27 04:23:46 PM   Num examples = 16
06/27 04:23:46 PM   Batch size = 32
06/27 04:23:46 PM ***** Eval results *****
06/27 04:23:46 PM   acc = 0.8125
06/27 04:23:46 PM   cls_loss = 11.684680700302124
06/27 04:23:46 PM   eval_loss = 3.41949200630188
06/27 04:23:46 PM   global_step = 4
06/27 04:23:46 PM   loss = 11.684680700302124
06/27 04:23:46 PM ***** Save model *****
06/27 04:23:46 PM ***** Test Dataset Eval Result *****
06/27 04:24:49 PM ***** Eval results *****
06/27 04:24:49 PM   acc = 0.7915
06/27 04:24:49 PM   cls_loss = 11.684680700302124
06/27 04:24:49 PM   eval_loss = 3.4961169636438765
06/27 04:24:49 PM   global_step = 4
06/27 04:24:49 PM   loss = 11.684680700302124
06/27 04:24:53 PM ***** LOSS printing *****
06/27 04:24:53 PM loss
06/27 04:24:53 PM tensor(3.6953, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:24:53 PM ***** LOSS printing *****
06/27 04:24:53 PM loss
06/27 04:24:53 PM tensor(2.8883, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:24:53 PM ***** LOSS printing *****
06/27 04:24:53 PM loss
06/27 04:24:53 PM tensor(2.1806, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:24:54 PM ***** LOSS printing *****
06/27 04:24:54 PM loss
06/27 04:24:54 PM tensor(2.8804, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:24:54 PM ***** LOSS printing *****
06/27 04:24:54 PM loss
06/27 04:24:54 PM tensor(3.1424, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:24:54 PM ***** Running evaluation MLM *****
06/27 04:24:54 PM   Epoch = 0 iter 9 step
06/27 04:24:54 PM   Num examples = 16
06/27 04:24:54 PM   Batch size = 32
06/27 04:24:55 PM ***** Eval results *****
06/27 04:24:55 PM   acc = 0.875
06/27 04:24:55 PM   cls_loss = 6.836197084850735
06/27 04:24:55 PM   eval_loss = 1.6252052783966064
06/27 04:24:55 PM   global_step = 9
06/27 04:24:55 PM   loss = 6.836197084850735
06/27 04:24:55 PM ***** Save model *****
06/27 04:24:55 PM ***** Test Dataset Eval Result *****
06/27 04:25:57 PM ***** Eval results *****
06/27 04:25:57 PM   acc = 0.89
06/27 04:25:57 PM   cls_loss = 6.836197084850735
06/27 04:25:57 PM   eval_loss = 1.461424672414386
06/27 04:25:57 PM   global_step = 9
06/27 04:25:57 PM   loss = 6.836197084850735
06/27 04:26:01 PM ***** LOSS printing *****
06/27 04:26:01 PM loss
06/27 04:26:01 PM tensor(2.9079, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:01 PM ***** LOSS printing *****
06/27 04:26:01 PM loss
06/27 04:26:01 PM tensor(2.4629, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:02 PM ***** LOSS printing *****
06/27 04:26:02 PM loss
06/27 04:26:02 PM tensor(3.8307, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:02 PM ***** LOSS printing *****
06/27 04:26:02 PM loss
06/27 04:26:02 PM tensor(1.1376, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:02 PM ***** LOSS printing *****
06/27 04:26:02 PM loss
06/27 04:26:02 PM tensor(2.1469, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:02 PM ***** Running evaluation MLM *****
06/27 04:26:02 PM   Epoch = 1 iter 14 step
06/27 04:26:02 PM   Num examples = 16
06/27 04:26:02 PM   Batch size = 32
06/27 04:26:03 PM ***** Eval results *****
06/27 04:26:03 PM   acc = 0.8125
06/27 04:26:03 PM   cls_loss = 1.642258882522583
06/27 04:26:03 PM   eval_loss = 1.5539915561676025
06/27 04:26:03 PM   global_step = 14
06/27 04:26:03 PM   loss = 1.642258882522583
06/27 04:26:03 PM ***** LOSS printing *****
06/27 04:26:03 PM loss
06/27 04:26:03 PM tensor(1.6666, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:03 PM ***** LOSS printing *****
06/27 04:26:03 PM loss
06/27 04:26:03 PM tensor(2.8276, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:03 PM ***** LOSS printing *****
06/27 04:26:03 PM loss
06/27 04:26:03 PM tensor(2.9332, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:03 PM ***** LOSS printing *****
06/27 04:26:03 PM loss
06/27 04:26:03 PM tensor(2.3071, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:04 PM ***** LOSS printing *****
06/27 04:26:04 PM loss
06/27 04:26:04 PM tensor(2.0650, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:04 PM ***** Running evaluation MLM *****
06/27 04:26:04 PM   Epoch = 1 iter 19 step
06/27 04:26:04 PM   Num examples = 16
06/27 04:26:04 PM   Batch size = 32
06/27 04:26:04 PM ***** Eval results *****
06/27 04:26:04 PM   acc = 0.8125
06/27 04:26:04 PM   cls_loss = 2.15486695085253
06/27 04:26:04 PM   eval_loss = 2.8570499420166016
06/27 04:26:04 PM   global_step = 19
06/27 04:26:04 PM   loss = 2.15486695085253
06/27 04:26:04 PM ***** LOSS printing *****
06/27 04:26:04 PM loss
06/27 04:26:04 PM tensor(1.5420, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:05 PM ***** LOSS printing *****
06/27 04:26:05 PM loss
06/27 04:26:05 PM tensor(1.7985, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:05 PM ***** LOSS printing *****
06/27 04:26:05 PM loss
06/27 04:26:05 PM tensor(2.0074, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:05 PM ***** LOSS printing *****
06/27 04:26:05 PM loss
06/27 04:26:05 PM tensor(2.4003, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:05 PM ***** LOSS printing *****
06/27 04:26:05 PM loss
06/27 04:26:05 PM tensor(2.6316, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:05 PM ***** Running evaluation MLM *****
06/27 04:26:05 PM   Epoch = 1 iter 24 step
06/27 04:26:05 PM   Num examples = 16
06/27 04:26:05 PM   Batch size = 32
06/27 04:26:06 PM ***** Eval results *****
06/27 04:26:06 PM   acc = 0.8125
06/27 04:26:06 PM   cls_loss = 2.122004359960556
06/27 04:26:06 PM   eval_loss = 2.4998152256011963
06/27 04:26:06 PM   global_step = 24
06/27 04:26:06 PM   loss = 2.122004359960556
06/27 04:26:06 PM ***** LOSS printing *****
06/27 04:26:06 PM loss
06/27 04:26:06 PM tensor(1.7983, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:06 PM ***** LOSS printing *****
06/27 04:26:06 PM loss
06/27 04:26:06 PM tensor(1.2902, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:06 PM ***** LOSS printing *****
06/27 04:26:06 PM loss
06/27 04:26:06 PM tensor(0.9129, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:07 PM ***** LOSS printing *****
06/27 04:26:07 PM loss
06/27 04:26:07 PM tensor(2.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:07 PM ***** LOSS printing *****
06/27 04:26:07 PM loss
06/27 04:26:07 PM tensor(1.8958, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:07 PM ***** Running evaluation MLM *****
06/27 04:26:07 PM   Epoch = 2 iter 29 step
06/27 04:26:07 PM   Num examples = 16
06/27 04:26:07 PM   Batch size = 32
06/27 04:26:08 PM ***** Eval results *****
06/27 04:26:08 PM   acc = 0.8125
06/27 04:26:08 PM   cls_loss = 1.5837754249572753
06/27 04:26:08 PM   eval_loss = 1.2576345205307007
06/27 04:26:08 PM   global_step = 29
06/27 04:26:08 PM   loss = 1.5837754249572753
06/27 04:26:08 PM ***** LOSS printing *****
06/27 04:26:08 PM loss
06/27 04:26:08 PM tensor(1.1834, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:08 PM ***** LOSS printing *****
06/27 04:26:08 PM loss
06/27 04:26:08 PM tensor(1.4875, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:08 PM ***** LOSS printing *****
06/27 04:26:08 PM loss
06/27 04:26:08 PM tensor(1.9431, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:08 PM ***** LOSS printing *****
06/27 04:26:08 PM loss
06/27 04:26:08 PM tensor(0.9917, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:08 PM ***** LOSS printing *****
06/27 04:26:08 PM loss
06/27 04:26:08 PM tensor(1.8533, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:09 PM ***** Running evaluation MLM *****
06/27 04:26:09 PM   Epoch = 2 iter 34 step
06/27 04:26:09 PM   Num examples = 16
06/27 04:26:09 PM   Batch size = 32
06/27 04:26:09 PM ***** Eval results *****
06/27 04:26:09 PM   acc = 0.75
06/27 04:26:09 PM   cls_loss = 1.5377805829048157
06/27 04:26:09 PM   eval_loss = 1.1414737701416016
06/27 04:26:09 PM   global_step = 34
06/27 04:26:09 PM   loss = 1.5377805829048157
06/27 04:26:09 PM ***** LOSS printing *****
06/27 04:26:09 PM loss
06/27 04:26:09 PM tensor(2.5114, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:09 PM ***** LOSS printing *****
06/27 04:26:09 PM loss
06/27 04:26:09 PM tensor(2.1354, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:10 PM ***** LOSS printing *****
06/27 04:26:10 PM loss
06/27 04:26:10 PM tensor(2.3922, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:10 PM ***** LOSS printing *****
06/27 04:26:10 PM loss
06/27 04:26:10 PM tensor(1.6015, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:10 PM ***** LOSS printing *****
06/27 04:26:10 PM loss
06/27 04:26:10 PM tensor(2.4691, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:10 PM ***** Running evaluation MLM *****
06/27 04:26:10 PM   Epoch = 3 iter 39 step
06/27 04:26:10 PM   Num examples = 16
06/27 04:26:10 PM   Batch size = 32
06/27 04:26:11 PM ***** Eval results *****
06/27 04:26:11 PM   acc = 0.75
06/27 04:26:11 PM   cls_loss = 2.1542603572209678
06/27 04:26:11 PM   eval_loss = 1.6763453483581543
06/27 04:26:11 PM   global_step = 39
06/27 04:26:11 PM   loss = 2.1542603572209678
06/27 04:26:11 PM ***** LOSS printing *****
06/27 04:26:11 PM loss
06/27 04:26:11 PM tensor(1.9291, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:11 PM ***** LOSS printing *****
06/27 04:26:11 PM loss
06/27 04:26:11 PM tensor(2.0042, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:11 PM ***** LOSS printing *****
06/27 04:26:11 PM loss
06/27 04:26:11 PM tensor(1.5803, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:11 PM ***** LOSS printing *****
06/27 04:26:11 PM loss
06/27 04:26:11 PM tensor(1.7363, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:12 PM ***** LOSS printing *****
06/27 04:26:12 PM loss
06/27 04:26:12 PM tensor(1.6193, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:12 PM ***** Running evaluation MLM *****
06/27 04:26:12 PM   Epoch = 3 iter 44 step
06/27 04:26:12 PM   Num examples = 16
06/27 04:26:12 PM   Batch size = 32
06/27 04:26:12 PM ***** Eval results *****
06/27 04:26:12 PM   acc = 0.75
06/27 04:26:12 PM   cls_loss = 1.9164993911981583
06/27 04:26:12 PM   eval_loss = 2.3743231296539307
06/27 04:26:12 PM   global_step = 44
06/27 04:26:12 PM   loss = 1.9164993911981583
06/27 04:26:12 PM ***** LOSS printing *****
06/27 04:26:12 PM loss
06/27 04:26:12 PM tensor(1.4039, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:13 PM ***** LOSS printing *****
06/27 04:26:13 PM loss
06/27 04:26:13 PM tensor(1.9827, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:13 PM ***** LOSS printing *****
06/27 04:26:13 PM loss
06/27 04:26:13 PM tensor(1.2808, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:13 PM ***** LOSS printing *****
06/27 04:26:13 PM loss
06/27 04:26:13 PM tensor(1.5960, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:13 PM ***** LOSS printing *****
06/27 04:26:13 PM loss
06/27 04:26:13 PM tensor(1.0897, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:13 PM ***** Running evaluation MLM *****
06/27 04:26:13 PM   Epoch = 4 iter 49 step
06/27 04:26:13 PM   Num examples = 16
06/27 04:26:13 PM   Batch size = 32
06/27 04:26:14 PM ***** Eval results *****
06/27 04:26:14 PM   acc = 0.75
06/27 04:26:14 PM   cls_loss = 1.0897102355957031
06/27 04:26:14 PM   eval_loss = 1.6106703281402588
06/27 04:26:14 PM   global_step = 49
06/27 04:26:14 PM   loss = 1.0897102355957031
06/27 04:26:14 PM ***** LOSS printing *****
06/27 04:26:14 PM loss
06/27 04:26:14 PM tensor(1.3041, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:14 PM ***** LOSS printing *****
06/27 04:26:14 PM loss
06/27 04:26:14 PM tensor(1.2621, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:14 PM ***** LOSS printing *****
06/27 04:26:14 PM loss
06/27 04:26:14 PM tensor(1.3384, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:15 PM ***** LOSS printing *****
06/27 04:26:15 PM loss
06/27 04:26:15 PM tensor(1.3842, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:15 PM ***** LOSS printing *****
06/27 04:26:15 PM loss
06/27 04:26:15 PM tensor(1.3768, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:15 PM ***** Running evaluation MLM *****
06/27 04:26:15 PM   Epoch = 4 iter 54 step
06/27 04:26:15 PM   Num examples = 16
06/27 04:26:15 PM   Batch size = 32
06/27 04:26:16 PM ***** Eval results *****
06/27 04:26:16 PM   acc = 0.8125
06/27 04:26:16 PM   cls_loss = 1.2925474643707275
06/27 04:26:16 PM   eval_loss = 1.6369187831878662
06/27 04:26:16 PM   global_step = 54
06/27 04:26:16 PM   loss = 1.2925474643707275
06/27 04:26:16 PM ***** LOSS printing *****
06/27 04:26:16 PM loss
06/27 04:26:16 PM tensor(1.2515, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:16 PM ***** LOSS printing *****
06/27 04:26:16 PM loss
06/27 04:26:16 PM tensor(1.3997, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:16 PM ***** LOSS printing *****
06/27 04:26:16 PM loss
06/27 04:26:16 PM tensor(1.6650, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:16 PM ***** LOSS printing *****
06/27 04:26:16 PM loss
06/27 04:26:16 PM tensor(1.4221, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:17 PM ***** LOSS printing *****
06/27 04:26:17 PM loss
06/27 04:26:17 PM tensor(1.4357, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:17 PM ***** Running evaluation MLM *****
06/27 04:26:17 PM   Epoch = 4 iter 59 step
06/27 04:26:17 PM   Num examples = 16
06/27 04:26:17 PM   Batch size = 32
06/27 04:26:17 PM ***** Eval results *****
06/27 04:26:17 PM   acc = 0.8125
06/27 04:26:17 PM   cls_loss = 1.357210094278509
06/27 04:26:17 PM   eval_loss = 1.3351964950561523
06/27 04:26:17 PM   global_step = 59
06/27 04:26:17 PM   loss = 1.357210094278509
06/27 04:26:17 PM ***** LOSS printing *****
06/27 04:26:17 PM loss
06/27 04:26:17 PM tensor(1.3705, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:17 PM ***** LOSS printing *****
06/27 04:26:17 PM loss
06/27 04:26:17 PM tensor(1.1070, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:18 PM ***** LOSS printing *****
06/27 04:26:18 PM loss
06/27 04:26:18 PM tensor(1.1830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:18 PM ***** LOSS printing *****
06/27 04:26:18 PM loss
06/27 04:26:18 PM tensor(1.2585, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:18 PM ***** LOSS printing *****
06/27 04:26:18 PM loss
06/27 04:26:18 PM tensor(0.8725, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:26:18 PM ***** Running evaluation MLM *****
06/27 04:26:18 PM   Epoch = 5 iter 64 step
06/27 04:26:18 PM   Num examples = 16
06/27 04:26:18 PM   Batch size = 32
06/27 04:26:19 PM ***** Eval results *****
06/27 04:26:19 PM   acc = 0.9375
06/27 04:26:19 PM   cls_loss = 1.105246141552925
06/27 04:26:19 PM   eval_loss = 1.0693327188491821
06/27 04:26:19 PM   global_step = 64
06/27 04:26:19 PM   loss = 1.105246141552925
06/27 04:26:19 PM ***** Save model *****
06/27 04:26:19 PM ***** Test Dataset Eval Result *****
06/27 04:27:21 PM ***** Eval results *****
06/27 04:27:21 PM   acc = 0.919
06/27 04:27:21 PM   cls_loss = 1.105246141552925
06/27 04:27:21 PM   eval_loss = 1.0419214793613978
06/27 04:27:21 PM   global_step = 64
06/27 04:27:21 PM   loss = 1.105246141552925
06/27 04:27:25 PM ***** LOSS printing *****
06/27 04:27:25 PM loss
06/27 04:27:25 PM tensor(2.1545, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:25 PM ***** LOSS printing *****
06/27 04:27:25 PM loss
06/27 04:27:25 PM tensor(1.6353, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:25 PM ***** LOSS printing *****
06/27 04:27:25 PM loss
06/27 04:27:25 PM tensor(1.3397, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:26 PM ***** LOSS printing *****
06/27 04:27:26 PM loss
06/27 04:27:26 PM tensor(1.4368, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:26 PM ***** LOSS printing *****
06/27 04:27:26 PM loss
06/27 04:27:26 PM tensor(1.5315, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:26 PM ***** Running evaluation MLM *****
06/27 04:27:26 PM   Epoch = 5 iter 69 step
06/27 04:27:26 PM   Num examples = 16
06/27 04:27:26 PM   Batch size = 32
06/27 04:27:27 PM ***** Eval results *****
06/27 04:27:27 PM   acc = 0.9375
06/27 04:27:27 PM   cls_loss = 1.3909855220052931
06/27 04:27:27 PM   eval_loss = 1.4346898794174194
06/27 04:27:27 PM   global_step = 69
06/27 04:27:27 PM   loss = 1.3909855220052931
06/27 04:27:27 PM ***** LOSS printing *****
06/27 04:27:27 PM loss
06/27 04:27:27 PM tensor(1.2352, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:27 PM ***** LOSS printing *****
06/27 04:27:27 PM loss
06/27 04:27:27 PM tensor(1.4548, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:27 PM ***** LOSS printing *****
06/27 04:27:27 PM loss
06/27 04:27:27 PM tensor(1.4219, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:27 PM ***** LOSS printing *****
06/27 04:27:27 PM loss
06/27 04:27:27 PM tensor(1.2294, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:28 PM ***** LOSS printing *****
06/27 04:27:28 PM loss
06/27 04:27:28 PM tensor(0.9579, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:28 PM ***** Running evaluation MLM *****
06/27 04:27:28 PM   Epoch = 6 iter 74 step
06/27 04:27:28 PM   Num examples = 16
06/27 04:27:28 PM   Batch size = 32
06/27 04:27:28 PM ***** Eval results *****
06/27 04:27:28 PM   acc = 0.9375
06/27 04:27:28 PM   cls_loss = 1.0936530232429504
06/27 04:27:28 PM   eval_loss = 1.9395654201507568
06/27 04:27:28 PM   global_step = 74
06/27 04:27:28 PM   loss = 1.0936530232429504
06/27 04:27:28 PM ***** LOSS printing *****
06/27 04:27:28 PM loss
06/27 04:27:28 PM tensor(1.2023, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:28 PM ***** LOSS printing *****
06/27 04:27:28 PM loss
06/27 04:27:28 PM tensor(2.5201, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:29 PM ***** LOSS printing *****
06/27 04:27:29 PM loss
06/27 04:27:29 PM tensor(1.9423, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:29 PM ***** LOSS printing *****
06/27 04:27:29 PM loss
06/27 04:27:29 PM tensor(3.3099, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:29 PM ***** LOSS printing *****
06/27 04:27:29 PM loss
06/27 04:27:29 PM tensor(1.5207, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:29 PM ***** Running evaluation MLM *****
06/27 04:27:29 PM   Epoch = 6 iter 79 step
06/27 04:27:29 PM   Num examples = 16
06/27 04:27:29 PM   Batch size = 32
06/27 04:27:30 PM ***** Eval results *****
06/27 04:27:30 PM   acc = 0.9375
06/27 04:27:30 PM   cls_loss = 1.8118100336619787
06/27 04:27:30 PM   eval_loss = 1.9455993175506592
06/27 04:27:30 PM   global_step = 79
06/27 04:27:30 PM   loss = 1.8118100336619787
06/27 04:27:30 PM ***** LOSS printing *****
06/27 04:27:30 PM loss
06/27 04:27:30 PM tensor(1.2066, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:30 PM ***** LOSS printing *****
06/27 04:27:30 PM loss
06/27 04:27:30 PM tensor(1.6792, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:30 PM ***** LOSS printing *****
06/27 04:27:30 PM loss
06/27 04:27:30 PM tensor(1.2608, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:31 PM ***** LOSS printing *****
06/27 04:27:31 PM loss
06/27 04:27:31 PM tensor(1.1863, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:31 PM ***** LOSS printing *****
06/27 04:27:31 PM loss
06/27 04:27:31 PM tensor(1.3865, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:31 PM ***** Running evaluation MLM *****
06/27 04:27:31 PM   Epoch = 6 iter 84 step
06/27 04:27:31 PM   Num examples = 16
06/27 04:27:31 PM   Batch size = 32
06/27 04:27:31 PM ***** Eval results *****
06/27 04:27:31 PM   acc = 0.875
06/27 04:27:31 PM   cls_loss = 1.616842250029246
06/27 04:27:31 PM   eval_loss = 1.4212480783462524
06/27 04:27:31 PM   global_step = 84
06/27 04:27:31 PM   loss = 1.616842250029246
06/27 04:27:31 PM ***** LOSS printing *****
06/27 04:27:31 PM loss
06/27 04:27:31 PM tensor(0.9485, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:32 PM ***** LOSS printing *****
06/27 04:27:32 PM loss
06/27 04:27:32 PM tensor(1.2857, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:32 PM ***** LOSS printing *****
06/27 04:27:32 PM loss
06/27 04:27:32 PM tensor(1.4299, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:32 PM ***** LOSS printing *****
06/27 04:27:32 PM loss
06/27 04:27:32 PM tensor(1.4782, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:32 PM ***** LOSS printing *****
06/27 04:27:32 PM loss
06/27 04:27:32 PM tensor(1.5898, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:33 PM ***** Running evaluation MLM *****
06/27 04:27:33 PM   Epoch = 7 iter 89 step
06/27 04:27:33 PM   Num examples = 16
06/27 04:27:33 PM   Batch size = 32
06/27 04:27:33 PM ***** Eval results *****
06/27 04:27:33 PM   acc = 0.8125
06/27 04:27:33 PM   cls_loss = 1.346402609348297
06/27 04:27:33 PM   eval_loss = 1.1729075908660889
06/27 04:27:33 PM   global_step = 89
06/27 04:27:33 PM   loss = 1.346402609348297
06/27 04:27:33 PM ***** LOSS printing *****
06/27 04:27:33 PM loss
06/27 04:27:33 PM tensor(1.3132, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:33 PM ***** LOSS printing *****
06/27 04:27:33 PM loss
06/27 04:27:33 PM tensor(1.2540, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:34 PM ***** LOSS printing *****
06/27 04:27:34 PM loss
06/27 04:27:34 PM tensor(1.1222, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:34 PM ***** LOSS printing *****
06/27 04:27:34 PM loss
06/27 04:27:34 PM tensor(1.1627, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:34 PM ***** LOSS printing *****
06/27 04:27:34 PM loss
06/27 04:27:34 PM tensor(1.1369, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:34 PM ***** Running evaluation MLM *****
06/27 04:27:34 PM   Epoch = 7 iter 94 step
06/27 04:27:34 PM   Num examples = 16
06/27 04:27:34 PM   Batch size = 32
06/27 04:27:35 PM ***** Eval results *****
06/27 04:27:35 PM   acc = 0.75
06/27 04:27:35 PM   cls_loss = 1.2721039593219756
06/27 04:27:35 PM   eval_loss = 1.7498291730880737
06/27 04:27:35 PM   global_step = 94
06/27 04:27:35 PM   loss = 1.2721039593219756
06/27 04:27:35 PM ***** LOSS printing *****
06/27 04:27:35 PM loss
06/27 04:27:35 PM tensor(1.5058, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:35 PM ***** LOSS printing *****
06/27 04:27:35 PM loss
06/27 04:27:35 PM tensor(1.4516, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:35 PM ***** LOSS printing *****
06/27 04:27:35 PM loss
06/27 04:27:35 PM tensor(1.3959, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:35 PM ***** LOSS printing *****
06/27 04:27:35 PM loss
06/27 04:27:35 PM tensor(1.5679, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:36 PM ***** LOSS printing *****
06/27 04:27:36 PM loss
06/27 04:27:36 PM tensor(1.2960, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:36 PM ***** Running evaluation MLM *****
06/27 04:27:36 PM   Epoch = 8 iter 99 step
06/27 04:27:36 PM   Num examples = 16
06/27 04:27:36 PM   Batch size = 32
06/27 04:27:36 PM ***** Eval results *****
06/27 04:27:36 PM   acc = 0.75
06/27 04:27:36 PM   cls_loss = 1.419947584470113
06/27 04:27:36 PM   eval_loss = 2.089006185531616
06/27 04:27:36 PM   global_step = 99
06/27 04:27:36 PM   loss = 1.419947584470113
06/27 04:27:36 PM ***** LOSS printing *****
06/27 04:27:36 PM loss
06/27 04:27:36 PM tensor(1.2723, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:37 PM ***** LOSS printing *****
06/27 04:27:37 PM loss
06/27 04:27:37 PM tensor(1.1166, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:37 PM ***** LOSS printing *****
06/27 04:27:37 PM loss
06/27 04:27:37 PM tensor(1.6517, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:37 PM ***** LOSS printing *****
06/27 04:27:37 PM loss
06/27 04:27:37 PM tensor(1.1641, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:37 PM ***** LOSS printing *****
06/27 04:27:37 PM loss
06/27 04:27:37 PM tensor(1.0920, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:37 PM ***** Running evaluation MLM *****
06/27 04:27:37 PM   Epoch = 8 iter 104 step
06/27 04:27:37 PM   Num examples = 16
06/27 04:27:37 PM   Batch size = 32
06/27 04:27:38 PM ***** Eval results *****
06/27 04:27:38 PM   acc = 0.875
06/27 04:27:38 PM   cls_loss = 1.3195699006319046
06/27 04:27:38 PM   eval_loss = 1.7004653215408325
06/27 04:27:38 PM   global_step = 104
06/27 04:27:38 PM   loss = 1.3195699006319046
06/27 04:27:38 PM ***** LOSS printing *****
06/27 04:27:38 PM loss
06/27 04:27:38 PM tensor(1.0590, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:38 PM ***** LOSS printing *****
06/27 04:27:38 PM loss
06/27 04:27:38 PM tensor(1.3080, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:38 PM ***** LOSS printing *****
06/27 04:27:38 PM loss
06/27 04:27:38 PM tensor(1.5719, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:39 PM ***** LOSS printing *****
06/27 04:27:39 PM loss
06/27 04:27:39 PM tensor(1.2813, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:39 PM ***** LOSS printing *****
06/27 04:27:39 PM loss
06/27 04:27:39 PM tensor(1.0105, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:39 PM ***** Running evaluation MLM *****
06/27 04:27:39 PM   Epoch = 9 iter 109 step
06/27 04:27:39 PM   Num examples = 16
06/27 04:27:39 PM   Batch size = 32
06/27 04:27:39 PM ***** Eval results *****
06/27 04:27:39 PM   acc = 0.875
06/27 04:27:39 PM   cls_loss = 1.0105009078979492
06/27 04:27:39 PM   eval_loss = 1.7815916538238525
06/27 04:27:39 PM   global_step = 109
06/27 04:27:39 PM   loss = 1.0105009078979492
06/27 04:27:40 PM ***** LOSS printing *****
06/27 04:27:40 PM loss
06/27 04:27:40 PM tensor(1.3503, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:40 PM ***** LOSS printing *****
06/27 04:27:40 PM loss
06/27 04:27:40 PM tensor(1.2892, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:40 PM ***** LOSS printing *****
06/27 04:27:40 PM loss
06/27 04:27:40 PM tensor(1.1611, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:40 PM ***** LOSS printing *****
06/27 04:27:40 PM loss
06/27 04:27:40 PM tensor(1.2484, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:40 PM ***** LOSS printing *****
06/27 04:27:40 PM loss
06/27 04:27:40 PM tensor(1.2437, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:41 PM ***** Running evaluation MLM *****
06/27 04:27:41 PM   Epoch = 9 iter 114 step
06/27 04:27:41 PM   Num examples = 16
06/27 04:27:41 PM   Batch size = 32
06/27 04:27:41 PM ***** Eval results *****
06/27 04:27:41 PM   acc = 0.875
06/27 04:27:41 PM   cls_loss = 1.2172127763430278
06/27 04:27:41 PM   eval_loss = 1.449908971786499
06/27 04:27:41 PM   global_step = 114
06/27 04:27:41 PM   loss = 1.2172127763430278
06/27 04:27:41 PM ***** LOSS printing *****
06/27 04:27:41 PM loss
06/27 04:27:41 PM tensor(1.4924, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:41 PM ***** LOSS printing *****
06/27 04:27:41 PM loss
06/27 04:27:41 PM tensor(1.2404, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:42 PM ***** LOSS printing *****
06/27 04:27:42 PM loss
06/27 04:27:42 PM tensor(1.0551, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:42 PM ***** LOSS printing *****
06/27 04:27:42 PM loss
06/27 04:27:42 PM tensor(1.1029, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:42 PM ***** LOSS printing *****
06/27 04:27:42 PM loss
06/27 04:27:42 PM tensor(1.6943, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:42 PM ***** Running evaluation MLM *****
06/27 04:27:42 PM   Epoch = 9 iter 119 step
06/27 04:27:42 PM   Num examples = 16
06/27 04:27:42 PM   Batch size = 32
06/27 04:27:43 PM ***** Eval results *****
06/27 04:27:43 PM   acc = 0.875
06/27 04:27:43 PM   cls_loss = 1.2625874280929565
06/27 04:27:43 PM   eval_loss = 1.5811421871185303
06/27 04:27:43 PM   global_step = 119
06/27 04:27:43 PM   loss = 1.2625874280929565
06/27 04:27:43 PM ***** LOSS printing *****
06/27 04:27:43 PM loss
06/27 04:27:43 PM tensor(1.4813, device='cuda:0', grad_fn=<NllLossBackward0>)

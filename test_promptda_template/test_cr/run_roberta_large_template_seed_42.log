06/27 04:27:44 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-42/', data_seed=42, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 04:27:44 PM device: cuda n_gpu: 1
06/27 04:27:44 PM Writing example 0 of 48
06/27 04:27:44 PM *** Example ***
06/27 04:27:44 PM guid: train-1
06/27 04:27:44 PM tokens: <s> also Ġthe Ġbattery Ġlife Ġisn Ġ' t Ġgreat Ġbut Ġit Ġ' s Ġsufficient Ġfor Ġmy Ġneeds Ġ. </s> ĠIt Ġis <mask>
06/27 04:27:44 PM input_ids: 0 19726 5 3822 301 965 128 90 372 53 24 128 29 7719 13 127 782 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:44 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:44 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:44 PM label: ['Ġbad']
06/27 04:27:44 PM Writing example 0 of 16
06/27 04:27:44 PM *** Example ***
06/27 04:27:44 PM guid: dev-1
06/27 04:27:44 PM tokens: <s> negative Ġ: Ġimp ossibly Ġtiny Ġand Ġdifficult Ġto Ġoperate Ġ, Ġbarely Ġvisible Ġ, Ġpower Ġbutton Ġ. </s> ĠIt Ġis <mask>
06/27 04:27:44 PM input_ids: 0 33407 4832 4023 39890 5262 8 1202 7 4303 2156 6254 7097 2156 476 6148 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:44 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:44 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:44 PM label: ['Ġbad']
06/27 04:27:45 PM Writing example 0 of 2000
06/27 04:27:45 PM *** Example ***
06/27 04:27:45 PM guid: dev-1
06/27 04:27:45 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 04:27:45 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:45 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:45 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:27:45 PM label: ['Ġbad']
06/27 04:27:58 PM ***** Running training *****
06/27 04:27:58 PM   Num examples = 48
06/27 04:27:58 PM   Batch size = 4
06/27 04:27:58 PM   Num steps = 120
06/27 04:27:58 PM n: embeddings.word_embeddings.weight
06/27 04:27:58 PM n: embeddings.position_embeddings.weight
06/27 04:27:58 PM n: embeddings.token_type_embeddings.weight
06/27 04:27:58 PM n: embeddings.LayerNorm.weight
06/27 04:27:58 PM n: embeddings.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.0.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.0.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.0.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.0.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.0.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.0.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.0.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.0.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.0.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.0.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.0.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.0.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.1.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.1.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.1.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.1.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.1.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.1.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.1.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.1.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.1.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.1.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.1.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.1.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.2.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.2.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.2.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.2.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.2.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.2.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.2.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.2.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.2.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.2.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.2.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.2.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.3.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.3.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.3.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.3.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.3.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.3.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.3.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.3.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.3.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.3.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.3.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.3.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.4.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.4.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.4.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.4.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.4.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.4.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.4.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.4.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.4.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.4.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.4.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.4.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.5.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.5.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.5.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.5.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.5.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.5.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.5.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.5.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.5.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.5.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.5.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.5.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.6.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.6.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.6.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.6.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.6.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.6.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.6.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.6.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.6.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.6.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.6.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.6.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.7.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.7.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.7.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.7.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.7.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.7.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.7.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.7.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.7.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.7.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.7.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.7.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.8.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.8.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.8.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.8.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.8.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.8.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.8.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.8.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.8.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.8.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.8.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.8.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.9.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.9.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.9.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.9.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.9.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.9.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.9.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.9.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.9.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.9.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.9.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.9.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.10.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.10.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.10.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.10.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.10.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.10.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.10.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.10.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.10.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.10.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.10.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.10.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.11.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.11.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.11.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.11.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.11.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.11.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.11.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.11.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.11.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.11.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.11.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.11.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.12.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.12.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.12.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.12.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.12.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.12.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.12.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.12.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.12.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.12.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.12.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.12.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.13.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.13.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.13.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.13.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.13.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.13.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.13.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.13.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.13.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.13.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.13.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.13.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.14.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.14.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.14.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.14.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.14.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.14.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.14.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.14.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.14.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.14.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.14.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.14.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.15.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.15.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.15.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.15.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.15.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.15.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.15.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.15.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.15.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.15.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.15.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.15.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.16.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.16.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.16.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.16.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.16.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.16.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.16.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.16.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.16.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.16.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.16.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.16.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.17.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.17.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.17.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.17.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.17.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.17.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.17.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.17.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.17.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.17.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.17.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.17.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.18.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.18.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.18.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.18.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.18.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.18.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.18.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.18.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.18.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.18.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.18.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.18.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.19.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.19.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.19.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.19.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.19.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.19.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.19.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.19.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.19.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.19.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.19.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.19.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.20.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.20.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.20.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.20.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.20.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.20.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.20.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.20.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.20.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.20.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.20.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.20.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.21.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.21.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.21.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.21.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.21.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.21.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.21.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.21.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.21.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.21.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.21.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.21.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.22.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.22.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.22.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.22.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.22.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.22.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.22.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.22.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.22.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.22.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.22.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.22.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.23.attention.self.query.weight
06/27 04:27:58 PM n: encoder.layer.23.attention.self.query.bias
06/27 04:27:58 PM n: encoder.layer.23.attention.self.key.weight
06/27 04:27:58 PM n: encoder.layer.23.attention.self.key.bias
06/27 04:27:58 PM n: encoder.layer.23.attention.self.value.weight
06/27 04:27:58 PM n: encoder.layer.23.attention.self.value.bias
06/27 04:27:58 PM n: encoder.layer.23.attention.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.23.attention.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: encoder.layer.23.intermediate.dense.weight
06/27 04:27:58 PM n: encoder.layer.23.intermediate.dense.bias
06/27 04:27:58 PM n: encoder.layer.23.output.dense.weight
06/27 04:27:58 PM n: encoder.layer.23.output.dense.bias
06/27 04:27:58 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 04:27:58 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 04:27:58 PM n: pooler.dense.weight
06/27 04:27:58 PM n: pooler.dense.bias
06/27 04:27:58 PM n: roberta.embeddings.word_embeddings.weight
06/27 04:27:58 PM n: roberta.embeddings.position_embeddings.weight
06/27 04:27:58 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 04:27:58 PM n: roberta.embeddings.LayerNorm.weight
06/27 04:27:58 PM n: roberta.embeddings.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 04:27:58 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 04:27:58 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 04:27:58 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 04:27:58 PM n: roberta.pooler.dense.weight
06/27 04:27:58 PM n: roberta.pooler.dense.bias
06/27 04:27:58 PM n: lm_head.bias
06/27 04:27:58 PM n: lm_head.dense.weight
06/27 04:27:58 PM n: lm_head.dense.bias
06/27 04:27:58 PM n: lm_head.layer_norm.weight
06/27 04:27:58 PM n: lm_head.layer_norm.bias
06/27 04:27:58 PM n: lm_head.decoder.weight
06/27 04:27:58 PM Total parameters: 763292761
06/27 04:27:58 PM ***** LOSS printing *****
06/27 04:27:58 PM loss
06/27 04:27:58 PM tensor(18.8029, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:58 PM ***** LOSS printing *****
06/27 04:27:58 PM loss
06/27 04:27:58 PM tensor(13.2629, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:58 PM ***** LOSS printing *****
06/27 04:27:58 PM loss
06/27 04:27:58 PM tensor(8.1571, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:59 PM ***** LOSS printing *****
06/27 04:27:59 PM loss
06/27 04:27:59 PM tensor(5.3129, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:27:59 PM ***** Running evaluation MLM *****
06/27 04:27:59 PM   Epoch = 0 iter 4 step
06/27 04:27:59 PM   Num examples = 16
06/27 04:27:59 PM   Batch size = 32
06/27 04:27:59 PM ***** Eval results *****
06/27 04:27:59 PM   acc = 0.5625
06/27 04:27:59 PM   cls_loss = 11.383960604667664
06/27 04:27:59 PM   eval_loss = 4.199136734008789
06/27 04:27:59 PM   global_step = 4
06/27 04:27:59 PM   loss = 11.383960604667664
06/27 04:27:59 PM ***** Save model *****
06/27 04:27:59 PM ***** Test Dataset Eval Result *****
06/27 04:29:02 PM ***** Eval results *****
06/27 04:29:02 PM   acc = 0.596
06/27 04:29:02 PM   cls_loss = 11.383960604667664
06/27 04:29:02 PM   eval_loss = 4.512586661747524
06/27 04:29:02 PM   global_step = 4
06/27 04:29:02 PM   loss = 11.383960604667664
06/27 04:29:06 PM ***** LOSS printing *****
06/27 04:29:06 PM loss
06/27 04:29:06 PM tensor(4.6812, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:29:06 PM ***** LOSS printing *****
06/27 04:29:06 PM loss
06/27 04:29:06 PM tensor(2.5725, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:29:07 PM ***** LOSS printing *****
06/27 04:29:07 PM loss
06/27 04:29:07 PM tensor(1.9453, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:29:07 PM ***** LOSS printing *****
06/27 04:29:07 PM loss
06/27 04:29:07 PM tensor(3.6859, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:29:07 PM ***** LOSS printing *****
06/27 04:29:07 PM loss
06/27 04:29:07 PM tensor(5.3275, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:29:07 PM ***** Running evaluation MLM *****
06/27 04:29:07 PM   Epoch = 0 iter 9 step
06/27 04:29:07 PM   Num examples = 16
06/27 04:29:07 PM   Batch size = 32
06/27 04:29:08 PM ***** Eval results *****
06/27 04:29:08 PM   acc = 0.8125
06/27 04:29:08 PM   cls_loss = 7.083133207427131
06/27 04:29:08 PM   eval_loss = 2.382714033126831
06/27 04:29:08 PM   global_step = 9
06/27 04:29:08 PM   loss = 7.083133207427131
06/27 04:29:08 PM ***** Save model *****
06/27 04:29:08 PM ***** Test Dataset Eval Result *****
06/27 04:30:10 PM ***** Eval results *****
06/27 04:30:10 PM   acc = 0.8785
06/27 04:30:10 PM   cls_loss = 7.083133207427131
06/27 04:30:10 PM   eval_loss = 2.5127442072308255
06/27 04:30:10 PM   global_step = 9
06/27 04:30:10 PM   loss = 7.083133207427131
06/27 04:30:14 PM ***** LOSS printing *****
06/27 04:30:14 PM loss
06/27 04:30:14 PM tensor(2.6661, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:30:15 PM ***** LOSS printing *****
06/27 04:30:15 PM loss
06/27 04:30:15 PM tensor(3.7366, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:30:15 PM ***** LOSS printing *****
06/27 04:30:15 PM loss
06/27 04:30:15 PM tensor(4.6524, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:30:15 PM ***** LOSS printing *****
06/27 04:30:15 PM loss
06/27 04:30:15 PM tensor(2.2945, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:30:15 PM ***** LOSS printing *****
06/27 04:30:15 PM loss
06/27 04:30:15 PM tensor(2.0468, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:30:15 PM ***** Running evaluation MLM *****
06/27 04:30:15 PM   Epoch = 1 iter 14 step
06/27 04:30:15 PM   Num examples = 16
06/27 04:30:15 PM   Batch size = 32
06/27 04:30:16 PM ***** Eval results *****
06/27 04:30:16 PM   acc = 0.875
06/27 04:30:16 PM   cls_loss = 2.1706467866897583
06/27 04:30:16 PM   eval_loss = 1.399449348449707
06/27 04:30:16 PM   global_step = 14
06/27 04:30:16 PM   loss = 2.1706467866897583
06/27 04:30:16 PM ***** Save model *****
06/27 04:30:16 PM ***** Test Dataset Eval Result *****
06/27 04:31:18 PM ***** Eval results *****
06/27 04:31:18 PM   acc = 0.8965
06/27 04:31:18 PM   cls_loss = 2.1706467866897583
06/27 04:31:18 PM   eval_loss = 1.249547297046298
06/27 04:31:18 PM   global_step = 14
06/27 04:31:18 PM   loss = 2.1706467866897583
06/27 04:31:23 PM ***** LOSS printing *****
06/27 04:31:23 PM loss
06/27 04:31:23 PM tensor(1.3928, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:23 PM ***** LOSS printing *****
06/27 04:31:23 PM loss
06/27 04:31:23 PM tensor(3.0942, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:23 PM ***** LOSS printing *****
06/27 04:31:23 PM loss
06/27 04:31:23 PM tensor(3.6887, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:24 PM ***** LOSS printing *****
06/27 04:31:24 PM loss
06/27 04:31:24 PM tensor(2.2897, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:24 PM ***** LOSS printing *****
06/27 04:31:24 PM loss
06/27 04:31:24 PM tensor(1.8577, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:24 PM ***** Running evaluation MLM *****
06/27 04:31:24 PM   Epoch = 1 iter 19 step
06/27 04:31:24 PM   Num examples = 16
06/27 04:31:24 PM   Batch size = 32
06/27 04:31:24 PM ***** Eval results *****
06/27 04:31:24 PM   acc = 0.875
06/27 04:31:24 PM   cls_loss = 2.380621177809579
06/27 04:31:24 PM   eval_loss = 3.329611301422119
06/27 04:31:24 PM   global_step = 19
06/27 04:31:24 PM   loss = 2.380621177809579
06/27 04:31:24 PM ***** LOSS printing *****
06/27 04:31:24 PM loss
06/27 04:31:24 PM tensor(1.5042, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:25 PM ***** LOSS printing *****
06/27 04:31:25 PM loss
06/27 04:31:25 PM tensor(1.1559, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:25 PM ***** LOSS printing *****
06/27 04:31:25 PM loss
06/27 04:31:25 PM tensor(1.5537, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:25 PM ***** LOSS printing *****
06/27 04:31:25 PM loss
06/27 04:31:25 PM tensor(3.1928, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:25 PM ***** LOSS printing *****
06/27 04:31:25 PM loss
06/27 04:31:25 PM tensor(4.4781, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:26 PM ***** Running evaluation MLM *****
06/27 04:31:26 PM   Epoch = 1 iter 24 step
06/27 04:31:26 PM   Num examples = 16
06/27 04:31:26 PM   Batch size = 32
06/27 04:31:26 PM ***** Eval results *****
06/27 04:31:26 PM   acc = 0.8125
06/27 04:31:26 PM   cls_loss = 2.3790825506051383
06/27 04:31:26 PM   eval_loss = 3.0002732276916504
06/27 04:31:26 PM   global_step = 24
06/27 04:31:26 PM   loss = 2.3790825506051383
06/27 04:31:26 PM ***** LOSS printing *****
06/27 04:31:26 PM loss
06/27 04:31:26 PM tensor(2.3108, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:26 PM ***** LOSS printing *****
06/27 04:31:26 PM loss
06/27 04:31:26 PM tensor(2.8663, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:27 PM ***** LOSS printing *****
06/27 04:31:27 PM loss
06/27 04:31:27 PM tensor(0.9839, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:27 PM ***** LOSS printing *****
06/27 04:31:27 PM loss
06/27 04:31:27 PM tensor(0.7929, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:27 PM ***** LOSS printing *****
06/27 04:31:27 PM loss
06/27 04:31:27 PM tensor(2.4529, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:27 PM ***** Running evaluation MLM *****
06/27 04:31:27 PM   Epoch = 2 iter 29 step
06/27 04:31:27 PM   Num examples = 16
06/27 04:31:27 PM   Batch size = 32
06/27 04:31:28 PM ***** Eval results *****
06/27 04:31:28 PM   acc = 0.8125
06/27 04:31:28 PM   cls_loss = 1.8813306212425231
06/27 04:31:28 PM   eval_loss = 1.0632747411727905
06/27 04:31:28 PM   global_step = 29
06/27 04:31:28 PM   loss = 1.8813306212425231
06/27 04:31:28 PM ***** LOSS printing *****
06/27 04:31:28 PM loss
06/27 04:31:28 PM tensor(2.1519, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:28 PM ***** LOSS printing *****
06/27 04:31:28 PM loss
06/27 04:31:28 PM tensor(0.8247, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:28 PM ***** LOSS printing *****
06/27 04:31:28 PM loss
06/27 04:31:28 PM tensor(2.4882, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:28 PM ***** LOSS printing *****
06/27 04:31:28 PM loss
06/27 04:31:28 PM tensor(2.5535, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:29 PM ***** LOSS printing *****
06/27 04:31:29 PM loss
06/27 04:31:29 PM tensor(1.7502, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:29 PM ***** Running evaluation MLM *****
06/27 04:31:29 PM   Epoch = 2 iter 34 step
06/27 04:31:29 PM   Num examples = 16
06/27 04:31:29 PM   Batch size = 32
06/27 04:31:29 PM ***** Eval results *****
06/27 04:31:29 PM   acc = 0.8125
06/27 04:31:29 PM   cls_loss = 1.9175271213054657
06/27 04:31:29 PM   eval_loss = 1.1492350101470947
06/27 04:31:29 PM   global_step = 34
06/27 04:31:29 PM   loss = 1.9175271213054657
06/27 04:31:29 PM ***** LOSS printing *****
06/27 04:31:29 PM loss
06/27 04:31:29 PM tensor(1.9616, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:30 PM ***** LOSS printing *****
06/27 04:31:30 PM loss
06/27 04:31:30 PM tensor(2.3861, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:30 PM ***** LOSS printing *****
06/27 04:31:30 PM loss
06/27 04:31:30 PM tensor(1.0698, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:30 PM ***** LOSS printing *****
06/27 04:31:30 PM loss
06/27 04:31:30 PM tensor(1.4505, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:30 PM ***** LOSS printing *****
06/27 04:31:30 PM loss
06/27 04:31:30 PM tensor(1.9884, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:30 PM ***** Running evaluation MLM *****
06/27 04:31:30 PM   Epoch = 3 iter 39 step
06/27 04:31:30 PM   Num examples = 16
06/27 04:31:30 PM   Batch size = 32
06/27 04:31:31 PM ***** Eval results *****
06/27 04:31:31 PM   acc = 0.75
06/27 04:31:31 PM   cls_loss = 1.5028938849767048
06/27 04:31:31 PM   eval_loss = 3.354677200317383
06/27 04:31:31 PM   global_step = 39
06/27 04:31:31 PM   loss = 1.5028938849767048
06/27 04:31:31 PM ***** LOSS printing *****
06/27 04:31:31 PM loss
06/27 04:31:31 PM tensor(1.6874, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:31 PM ***** LOSS printing *****
06/27 04:31:31 PM loss
06/27 04:31:31 PM tensor(2.2524, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:31 PM ***** LOSS printing *****
06/27 04:31:31 PM loss
06/27 04:31:31 PM tensor(2.3545, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:32 PM ***** LOSS printing *****
06/27 04:31:32 PM loss
06/27 04:31:32 PM tensor(2.5275, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:32 PM ***** LOSS printing *****
06/27 04:31:32 PM loss
06/27 04:31:32 PM tensor(2.0639, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:32 PM ***** Running evaluation MLM *****
06/27 04:31:32 PM   Epoch = 3 iter 44 step
06/27 04:31:32 PM   Num examples = 16
06/27 04:31:32 PM   Batch size = 32
06/27 04:31:33 PM ***** Eval results *****
06/27 04:31:33 PM   acc = 0.75
06/27 04:31:33 PM   cls_loss = 1.9242884367704391
06/27 04:31:33 PM   eval_loss = 3.3148369789123535
06/27 04:31:33 PM   global_step = 44
06/27 04:31:33 PM   loss = 1.9242884367704391
06/27 04:31:33 PM ***** LOSS printing *****
06/27 04:31:33 PM loss
06/27 04:31:33 PM tensor(0.8364, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:33 PM ***** LOSS printing *****
06/27 04:31:33 PM loss
06/27 04:31:33 PM tensor(1.5356, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:33 PM ***** LOSS printing *****
06/27 04:31:33 PM loss
06/27 04:31:33 PM tensor(2.0726, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:33 PM ***** LOSS printing *****
06/27 04:31:33 PM loss
06/27 04:31:33 PM tensor(2.1138, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:33 PM ***** LOSS printing *****
06/27 04:31:33 PM loss
06/27 04:31:33 PM tensor(1.1008, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:34 PM ***** Running evaluation MLM *****
06/27 04:31:34 PM   Epoch = 4 iter 49 step
06/27 04:31:34 PM   Num examples = 16
06/27 04:31:34 PM   Batch size = 32
06/27 04:31:34 PM ***** Eval results *****
06/27 04:31:34 PM   acc = 0.8125
06/27 04:31:34 PM   cls_loss = 1.1008219718933105
06/27 04:31:34 PM   eval_loss = 2.475639820098877
06/27 04:31:34 PM   global_step = 49
06/27 04:31:34 PM   loss = 1.1008219718933105
06/27 04:31:34 PM ***** LOSS printing *****
06/27 04:31:34 PM loss
06/27 04:31:34 PM tensor(1.7200, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:34 PM ***** LOSS printing *****
06/27 04:31:34 PM loss
06/27 04:31:34 PM tensor(2.1025, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:35 PM ***** LOSS printing *****
06/27 04:31:35 PM loss
06/27 04:31:35 PM tensor(1.4047, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:35 PM ***** LOSS printing *****
06/27 04:31:35 PM loss
06/27 04:31:35 PM tensor(1.0961, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:35 PM ***** LOSS printing *****
06/27 04:31:35 PM loss
06/27 04:31:35 PM tensor(1.7258, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:35 PM ***** Running evaluation MLM *****
06/27 04:31:35 PM   Epoch = 4 iter 54 step
06/27 04:31:35 PM   Num examples = 16
06/27 04:31:35 PM   Batch size = 32
06/27 04:31:36 PM ***** Eval results *****
06/27 04:31:36 PM   acc = 0.875
06/27 04:31:36 PM   cls_loss = 1.524983326594035
06/27 04:31:36 PM   eval_loss = 1.270851492881775
06/27 04:31:36 PM   global_step = 54
06/27 04:31:36 PM   loss = 1.524983326594035
06/27 04:31:36 PM ***** LOSS printing *****
06/27 04:31:36 PM loss
06/27 04:31:36 PM tensor(1.1501, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:36 PM ***** LOSS printing *****
06/27 04:31:36 PM loss
06/27 04:31:36 PM tensor(2.1428, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:36 PM ***** LOSS printing *****
06/27 04:31:36 PM loss
06/27 04:31:36 PM tensor(1.9130, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:36 PM ***** LOSS printing *****
06/27 04:31:36 PM loss
06/27 04:31:36 PM tensor(1.6634, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:37 PM ***** LOSS printing *****
06/27 04:31:37 PM loss
06/27 04:31:37 PM tensor(1.5337, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:37 PM ***** Running evaluation MLM *****
06/27 04:31:37 PM   Epoch = 4 iter 59 step
06/27 04:31:37 PM   Num examples = 16
06/27 04:31:37 PM   Batch size = 32
06/27 04:31:37 PM ***** Eval results *****
06/27 04:31:37 PM   acc = 0.875
06/27 04:31:37 PM   cls_loss = 1.5957124016501687
06/27 04:31:37 PM   eval_loss = 1.1762040853500366
06/27 04:31:37 PM   global_step = 59
06/27 04:31:37 PM   loss = 1.5957124016501687
06/27 04:31:37 PM ***** LOSS printing *****
06/27 04:31:37 PM loss
06/27 04:31:37 PM tensor(1.6638, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:38 PM ***** LOSS printing *****
06/27 04:31:38 PM loss
06/27 04:31:38 PM tensor(1.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:38 PM ***** LOSS printing *****
06/27 04:31:38 PM loss
06/27 04:31:38 PM tensor(1.0982, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:38 PM ***** LOSS printing *****
06/27 04:31:38 PM loss
06/27 04:31:38 PM tensor(1.3577, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:38 PM ***** LOSS printing *****
06/27 04:31:38 PM loss
06/27 04:31:38 PM tensor(1.0850, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:38 PM ***** Running evaluation MLM *****
06/27 04:31:38 PM   Epoch = 5 iter 64 step
06/27 04:31:38 PM   Num examples = 16
06/27 04:31:38 PM   Batch size = 32
06/27 04:31:39 PM ***** Eval results *****
06/27 04:31:39 PM   acc = 0.875
06/27 04:31:39 PM   cls_loss = 1.1425735652446747
06/27 04:31:39 PM   eval_loss = 1.9612504243850708
06/27 04:31:39 PM   global_step = 64
06/27 04:31:39 PM   loss = 1.1425735652446747
06/27 04:31:39 PM ***** LOSS printing *****
06/27 04:31:39 PM loss
06/27 04:31:39 PM tensor(1.4924, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:39 PM ***** LOSS printing *****
06/27 04:31:39 PM loss
06/27 04:31:39 PM tensor(1.0690, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:39 PM ***** LOSS printing *****
06/27 04:31:39 PM loss
06/27 04:31:39 PM tensor(1.1951, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:40 PM ***** LOSS printing *****
06/27 04:31:40 PM loss
06/27 04:31:40 PM tensor(0.7306, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:40 PM ***** LOSS printing *****
06/27 04:31:40 PM loss
06/27 04:31:40 PM tensor(1.0391, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:40 PM ***** Running evaluation MLM *****
06/27 04:31:40 PM   Epoch = 5 iter 69 step
06/27 04:31:40 PM   Num examples = 16
06/27 04:31:40 PM   Batch size = 32
06/27 04:31:41 PM ***** Eval results *****
06/27 04:31:41 PM   acc = 0.8125
06/27 04:31:41 PM   cls_loss = 1.1218313773473103
06/27 04:31:41 PM   eval_loss = 2.836754560470581
06/27 04:31:41 PM   global_step = 69
06/27 04:31:41 PM   loss = 1.1218313773473103
06/27 04:31:41 PM ***** LOSS printing *****
06/27 04:31:41 PM loss
06/27 04:31:41 PM tensor(2.2422, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:41 PM ***** LOSS printing *****
06/27 04:31:41 PM loss
06/27 04:31:41 PM tensor(1.9989, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:41 PM ***** LOSS printing *****
06/27 04:31:41 PM loss
06/27 04:31:41 PM tensor(1.3041, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:41 PM ***** LOSS printing *****
06/27 04:31:41 PM loss
06/27 04:31:41 PM tensor(1.5362, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:41 PM ***** LOSS printing *****
06/27 04:31:41 PM loss
06/27 04:31:41 PM tensor(0.7130, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:42 PM ***** Running evaluation MLM *****
06/27 04:31:42 PM   Epoch = 6 iter 74 step
06/27 04:31:42 PM   Num examples = 16
06/27 04:31:42 PM   Batch size = 32
06/27 04:31:42 PM ***** Eval results *****
06/27 04:31:42 PM   acc = 0.875
06/27 04:31:42 PM   cls_loss = 1.1245890259742737
06/27 04:31:42 PM   eval_loss = 2.5799763202667236
06/27 04:31:42 PM   global_step = 74
06/27 04:31:42 PM   loss = 1.1245890259742737
06/27 04:31:42 PM ***** LOSS printing *****
06/27 04:31:42 PM loss
06/27 04:31:42 PM tensor(1.6013, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:42 PM ***** LOSS printing *****
06/27 04:31:42 PM loss
06/27 04:31:42 PM tensor(1.7657, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:43 PM ***** LOSS printing *****
06/27 04:31:43 PM loss
06/27 04:31:43 PM tensor(1.8963, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:43 PM ***** LOSS printing *****
06/27 04:31:43 PM loss
06/27 04:31:43 PM tensor(1.7280, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:43 PM ***** LOSS printing *****
06/27 04:31:43 PM loss
06/27 04:31:43 PM tensor(1.2318, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:43 PM ***** Running evaluation MLM *****
06/27 04:31:43 PM   Epoch = 6 iter 79 step
06/27 04:31:43 PM   Num examples = 16
06/27 04:31:43 PM   Batch size = 32
06/27 04:31:44 PM ***** Eval results *****
06/27 04:31:44 PM   acc = 0.875
06/27 04:31:44 PM   cls_loss = 1.4960515328816004
06/27 04:31:44 PM   eval_loss = 2.108121156692505
06/27 04:31:44 PM   global_step = 79
06/27 04:31:44 PM   loss = 1.4960515328816004
06/27 04:31:44 PM ***** LOSS printing *****
06/27 04:31:44 PM loss
06/27 04:31:44 PM tensor(1.2188, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:44 PM ***** LOSS printing *****
06/27 04:31:44 PM loss
06/27 04:31:44 PM tensor(1.5497, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:44 PM ***** LOSS printing *****
06/27 04:31:44 PM loss
06/27 04:31:44 PM tensor(1.4809, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:44 PM ***** LOSS printing *****
06/27 04:31:44 PM loss
06/27 04:31:44 PM tensor(1.2149, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:45 PM ***** LOSS printing *****
06/27 04:31:45 PM loss
06/27 04:31:45 PM tensor(1.7398, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:45 PM ***** Running evaluation MLM *****
06/27 04:31:45 PM   Epoch = 6 iter 84 step
06/27 04:31:45 PM   Num examples = 16
06/27 04:31:45 PM   Batch size = 32
06/27 04:31:45 PM ***** Eval results *****
06/27 04:31:45 PM   acc = 0.875
06/27 04:31:45 PM   cls_loss = 1.4730291465918224
06/27 04:31:45 PM   eval_loss = 1.5663813352584839
06/27 04:31:45 PM   global_step = 84
06/27 04:31:45 PM   loss = 1.4730291465918224
06/27 04:31:45 PM ***** LOSS printing *****
06/27 04:31:45 PM loss
06/27 04:31:45 PM tensor(0.9863, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:46 PM ***** LOSS printing *****
06/27 04:31:46 PM loss
06/27 04:31:46 PM tensor(1.3273, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:46 PM ***** LOSS printing *****
06/27 04:31:46 PM loss
06/27 04:31:46 PM tensor(1.0819, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:46 PM ***** LOSS printing *****
06/27 04:31:46 PM loss
06/27 04:31:46 PM tensor(1.5647, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:46 PM ***** LOSS printing *****
06/27 04:31:46 PM loss
06/27 04:31:46 PM tensor(1.2252, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:46 PM ***** Running evaluation MLM *****
06/27 04:31:46 PM   Epoch = 7 iter 89 step
06/27 04:31:46 PM   Num examples = 16
06/27 04:31:46 PM   Batch size = 32
06/27 04:31:47 PM ***** Eval results *****
06/27 04:31:47 PM   acc = 0.875
06/27 04:31:47 PM   cls_loss = 1.2370903253555299
06/27 04:31:47 PM   eval_loss = 1.1436240673065186
06/27 04:31:47 PM   global_step = 89
06/27 04:31:47 PM   loss = 1.2370903253555299
06/27 04:31:47 PM ***** LOSS printing *****
06/27 04:31:47 PM loss
06/27 04:31:47 PM tensor(1.6586, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:47 PM ***** LOSS printing *****
06/27 04:31:47 PM loss
06/27 04:31:47 PM tensor(1.6396, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:47 PM ***** LOSS printing *****
06/27 04:31:47 PM loss
06/27 04:31:47 PM tensor(1.1648, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:48 PM ***** LOSS printing *****
06/27 04:31:48 PM loss
06/27 04:31:48 PM tensor(0.9230, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:48 PM ***** LOSS printing *****
06/27 04:31:48 PM loss
06/27 04:31:48 PM tensor(1.2503, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:48 PM ***** Running evaluation MLM *****
06/27 04:31:48 PM   Epoch = 7 iter 94 step
06/27 04:31:48 PM   Num examples = 16
06/27 04:31:48 PM   Batch size = 32
06/27 04:31:49 PM ***** Eval results *****
06/27 04:31:49 PM   acc = 0.8125
06/27 04:31:49 PM   cls_loss = 1.2821779370307922
06/27 04:31:49 PM   eval_loss = 1.3937647342681885
06/27 04:31:49 PM   global_step = 94
06/27 04:31:49 PM   loss = 1.2821779370307922
06/27 04:31:49 PM ***** LOSS printing *****
06/27 04:31:49 PM loss
06/27 04:31:49 PM tensor(1.6414, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:49 PM ***** LOSS printing *****
06/27 04:31:49 PM loss
06/27 04:31:49 PM tensor(1.8338, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:49 PM ***** LOSS printing *****
06/27 04:31:49 PM loss
06/27 04:31:49 PM tensor(1.3849, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:49 PM ***** LOSS printing *****
06/27 04:31:49 PM loss
06/27 04:31:49 PM tensor(1.1545, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:50 PM ***** LOSS printing *****
06/27 04:31:50 PM loss
06/27 04:31:50 PM tensor(1.1692, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:50 PM ***** Running evaluation MLM *****
06/27 04:31:50 PM   Epoch = 8 iter 99 step
06/27 04:31:50 PM   Num examples = 16
06/27 04:31:50 PM   Batch size = 32
06/27 04:31:50 PM ***** Eval results *****
06/27 04:31:50 PM   acc = 0.875
06/27 04:31:50 PM   cls_loss = 1.2361764113108318
06/27 04:31:50 PM   eval_loss = 2.145298957824707
06/27 04:31:50 PM   global_step = 99
06/27 04:31:50 PM   loss = 1.2361764113108318
06/27 04:31:50 PM ***** LOSS printing *****
06/27 04:31:50 PM loss
06/27 04:31:50 PM tensor(1.3892, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:50 PM ***** LOSS printing *****
06/27 04:31:50 PM loss
06/27 04:31:50 PM tensor(1.2315, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:51 PM ***** LOSS printing *****
06/27 04:31:51 PM loss
06/27 04:31:51 PM tensor(1.9652, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:51 PM ***** LOSS printing *****
06/27 04:31:51 PM loss
06/27 04:31:51 PM tensor(1.0456, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:51 PM ***** LOSS printing *****
06/27 04:31:51 PM loss
06/27 04:31:51 PM tensor(1.4671, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:51 PM ***** Running evaluation MLM *****
06/27 04:31:51 PM   Epoch = 8 iter 104 step
06/27 04:31:51 PM   Num examples = 16
06/27 04:31:51 PM   Batch size = 32
06/27 04:31:52 PM ***** Eval results *****
06/27 04:31:52 PM   acc = 0.8125
06/27 04:31:52 PM   cls_loss = 1.3508902788162231
06/27 04:31:52 PM   eval_loss = 2.3321263790130615
06/27 04:31:52 PM   global_step = 104
06/27 04:31:52 PM   loss = 1.3508902788162231
06/27 04:31:52 PM ***** LOSS printing *****
06/27 04:31:52 PM loss
06/27 04:31:52 PM tensor(0.9488, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:52 PM ***** LOSS printing *****
06/27 04:31:52 PM loss
06/27 04:31:52 PM tensor(1.5466, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:52 PM ***** LOSS printing *****
06/27 04:31:52 PM loss
06/27 04:31:52 PM tensor(1.7581, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:53 PM ***** LOSS printing *****
06/27 04:31:53 PM loss
06/27 04:31:53 PM tensor(1.2896, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:53 PM ***** LOSS printing *****
06/27 04:31:53 PM loss
06/27 04:31:53 PM tensor(1.3361, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:53 PM ***** Running evaluation MLM *****
06/27 04:31:53 PM   Epoch = 9 iter 109 step
06/27 04:31:53 PM   Num examples = 16
06/27 04:31:53 PM   Batch size = 32
06/27 04:31:53 PM ***** Eval results *****
06/27 04:31:53 PM   acc = 0.8125
06/27 04:31:53 PM   cls_loss = 1.336143136024475
06/27 04:31:53 PM   eval_loss = 2.3952059745788574
06/27 04:31:53 PM   global_step = 109
06/27 04:31:53 PM   loss = 1.336143136024475
06/27 04:31:53 PM ***** LOSS printing *****
06/27 04:31:53 PM loss
06/27 04:31:53 PM tensor(1.5453, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:54 PM ***** LOSS printing *****
06/27 04:31:54 PM loss
06/27 04:31:54 PM tensor(1.6196, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:54 PM ***** LOSS printing *****
06/27 04:31:54 PM loss
06/27 04:31:54 PM tensor(1.3276, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:54 PM ***** LOSS printing *****
06/27 04:31:54 PM loss
06/27 04:31:54 PM tensor(1.6329, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:54 PM ***** LOSS printing *****
06/27 04:31:54 PM loss
06/27 04:31:54 PM tensor(1.2068, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:55 PM ***** Running evaluation MLM *****
06/27 04:31:55 PM   Epoch = 9 iter 114 step
06/27 04:31:55 PM   Num examples = 16
06/27 04:31:55 PM   Batch size = 32
06/27 04:31:55 PM ***** Eval results *****
06/27 04:31:55 PM   acc = 0.8125
06/27 04:31:55 PM   cls_loss = 1.4447211225827534
06/27 04:31:55 PM   eval_loss = 1.8275835514068604
06/27 04:31:55 PM   global_step = 114
06/27 04:31:55 PM   loss = 1.4447211225827534
06/27 04:31:55 PM ***** LOSS printing *****
06/27 04:31:55 PM loss
06/27 04:31:55 PM tensor(1.1885, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:55 PM ***** LOSS printing *****
06/27 04:31:55 PM loss
06/27 04:31:55 PM tensor(1.0615, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:56 PM ***** LOSS printing *****
06/27 04:31:56 PM loss
06/27 04:31:56 PM tensor(1.1618, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:56 PM ***** LOSS printing *****
06/27 04:31:56 PM loss
06/27 04:31:56 PM tensor(1.3739, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:56 PM ***** LOSS printing *****
06/27 04:31:56 PM loss
06/27 04:31:56 PM tensor(1.5314, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:31:56 PM ***** Running evaluation MLM *****
06/27 04:31:56 PM   Epoch = 9 iter 119 step
06/27 04:31:56 PM   Num examples = 16
06/27 04:31:56 PM   Batch size = 32
06/27 04:31:57 PM ***** Eval results *****
06/27 04:31:57 PM   acc = 0.8125
06/27 04:31:57 PM   cls_loss = 1.3623094667087903
06/27 04:31:57 PM   eval_loss = 1.7654434442520142
06/27 04:31:57 PM   global_step = 119
06/27 04:31:57 PM   loss = 1.3623094667087903
06/27 04:31:57 PM ***** LOSS printing *****
06/27 04:31:57 PM loss
06/27 04:31:57 PM tensor(1.0978, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 04:31:58 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-21/', data_seed=21, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 04:31:58 PM device: cuda n_gpu: 1
06/27 04:31:58 PM Writing example 0 of 48
06/27 04:31:58 PM *** Example ***
06/27 04:31:58 PM guid: train-1
06/27 04:31:58 PM tokens: <s> the Ġn okia Ġ66 00 Ġis Ġa Ġdecent Ġextension Ġof Ġthe Ġsmart Ġphone Ġline Ġ. </s> ĠIt Ġis <mask>
06/27 04:31:58 PM input_ids: 0 627 295 43946 5138 612 16 10 7297 5064 9 5 2793 1028 516 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:58 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:58 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:58 PM label: ['Ġexcellent']
06/27 04:31:59 PM Writing example 0 of 16
06/27 04:31:59 PM *** Example ***
06/27 04:31:59 PM guid: dev-1
06/27 04:31:59 PM tokens: <s> the Ġclarity Ġis Ġunbelievable Ġ, Ġwhich Ġmeans Ġyou Ġcan Ġcram Ġalmost Ġthe Ġfull Ġ2 Ġ, Ġ5 Ġ! Ġ. </s> ĠIt Ġis <mask>
06/27 04:31:59 PM input_ids: 0 627 10498 16 14011 2156 61 839 47 64 39011 818 5 455 132 2156 195 27785 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:59 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:59 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:59 PM label: ['Ġexcellent']
06/27 04:31:59 PM Writing example 0 of 2000
06/27 04:31:59 PM *** Example ***
06/27 04:31:59 PM guid: dev-1
06/27 04:31:59 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 04:31:59 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:59 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:59 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:31:59 PM label: ['Ġawful']
06/27 04:32:12 PM ***** Running training *****
06/27 04:32:12 PM   Num examples = 48
06/27 04:32:12 PM   Batch size = 4
06/27 04:32:12 PM   Num steps = 120
06/27 04:32:12 PM n: embeddings.word_embeddings.weight
06/27 04:32:12 PM n: embeddings.position_embeddings.weight
06/27 04:32:12 PM n: embeddings.token_type_embeddings.weight
06/27 04:32:12 PM n: embeddings.LayerNorm.weight
06/27 04:32:12 PM n: embeddings.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.0.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.0.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.0.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.0.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.0.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.0.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.0.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.0.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.0.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.0.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.0.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.0.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.1.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.1.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.1.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.1.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.1.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.1.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.1.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.1.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.1.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.1.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.1.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.1.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.2.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.2.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.2.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.2.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.2.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.2.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.2.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.2.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.2.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.2.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.2.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.2.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.3.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.3.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.3.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.3.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.3.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.3.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.3.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.3.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.3.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.3.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.3.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.3.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.4.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.4.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.4.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.4.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.4.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.4.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.4.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.4.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.4.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.4.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.4.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.4.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.5.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.5.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.5.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.5.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.5.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.5.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.5.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.5.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.5.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.5.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.5.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.5.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.6.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.6.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.6.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.6.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.6.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.6.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.6.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.6.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.6.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.6.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.6.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.6.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.7.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.7.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.7.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.7.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.7.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.7.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.7.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.7.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.7.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.7.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.7.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.7.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.8.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.8.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.8.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.8.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.8.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.8.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.8.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.8.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.8.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.8.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.8.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.8.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.9.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.9.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.9.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.9.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.9.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.9.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.9.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.9.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.9.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.9.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.9.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.9.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.10.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.10.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.10.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.10.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.10.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.10.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.10.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.10.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.10.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.10.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.10.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.10.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.11.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.11.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.11.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.11.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.11.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.11.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.11.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.11.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.11.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.11.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.11.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.11.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.12.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.12.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.12.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.12.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.12.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.12.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.12.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.12.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.12.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.12.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.12.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.12.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.13.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.13.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.13.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.13.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.13.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.13.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.13.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.13.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.13.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.13.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.13.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.13.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.14.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.14.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.14.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.14.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.14.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.14.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.14.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.14.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.14.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.14.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.14.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.14.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.15.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.15.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.15.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.15.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.15.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.15.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.15.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.15.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.15.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.15.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.15.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.15.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.16.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.16.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.16.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.16.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.16.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.16.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.16.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.16.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.16.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.16.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.16.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.16.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.17.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.17.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.17.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.17.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.17.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.17.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.17.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.17.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.17.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.17.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.17.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.17.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.18.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.18.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.18.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.18.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.18.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.18.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.18.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.18.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.18.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.18.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.18.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.18.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.19.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.19.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.19.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.19.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.19.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.19.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.19.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.19.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.19.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.19.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.19.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.19.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.20.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.20.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.20.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.20.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.20.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.20.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.20.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.20.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.20.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.20.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.20.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.20.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.21.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.21.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.21.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.21.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.21.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.21.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.21.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.21.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.21.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.21.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.21.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.21.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.22.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.22.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.22.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.22.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.22.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.22.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.22.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.22.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.22.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.22.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.22.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.22.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.23.attention.self.query.weight
06/27 04:32:12 PM n: encoder.layer.23.attention.self.query.bias
06/27 04:32:12 PM n: encoder.layer.23.attention.self.key.weight
06/27 04:32:12 PM n: encoder.layer.23.attention.self.key.bias
06/27 04:32:12 PM n: encoder.layer.23.attention.self.value.weight
06/27 04:32:12 PM n: encoder.layer.23.attention.self.value.bias
06/27 04:32:12 PM n: encoder.layer.23.attention.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.23.attention.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: encoder.layer.23.intermediate.dense.weight
06/27 04:32:12 PM n: encoder.layer.23.intermediate.dense.bias
06/27 04:32:12 PM n: encoder.layer.23.output.dense.weight
06/27 04:32:12 PM n: encoder.layer.23.output.dense.bias
06/27 04:32:12 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 04:32:12 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 04:32:12 PM n: pooler.dense.weight
06/27 04:32:12 PM n: pooler.dense.bias
06/27 04:32:12 PM n: roberta.embeddings.word_embeddings.weight
06/27 04:32:12 PM n: roberta.embeddings.position_embeddings.weight
06/27 04:32:12 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 04:32:12 PM n: roberta.embeddings.LayerNorm.weight
06/27 04:32:12 PM n: roberta.embeddings.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 04:32:12 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 04:32:12 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 04:32:12 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 04:32:12 PM n: roberta.pooler.dense.weight
06/27 04:32:12 PM n: roberta.pooler.dense.bias
06/27 04:32:12 PM n: lm_head.bias
06/27 04:32:12 PM n: lm_head.dense.weight
06/27 04:32:12 PM n: lm_head.dense.bias
06/27 04:32:12 PM n: lm_head.layer_norm.weight
06/27 04:32:12 PM n: lm_head.layer_norm.bias
06/27 04:32:12 PM n: lm_head.decoder.weight
06/27 04:32:12 PM Total parameters: 763292761
06/27 04:32:12 PM ***** LOSS printing *****
06/27 04:32:12 PM loss
06/27 04:32:12 PM tensor(18.8559, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:32:12 PM ***** LOSS printing *****
06/27 04:32:12 PM loss
06/27 04:32:12 PM tensor(14.9711, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:32:12 PM ***** LOSS printing *****
06/27 04:32:12 PM loss
06/27 04:32:12 PM tensor(8.9709, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:32:13 PM ***** LOSS printing *****
06/27 04:32:13 PM loss
06/27 04:32:13 PM tensor(5.7280, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:32:13 PM ***** Running evaluation MLM *****
06/27 04:32:13 PM   Epoch = 0 iter 4 step
06/27 04:32:13 PM   Num examples = 16
06/27 04:32:13 PM   Batch size = 32
06/27 04:32:13 PM ***** Eval results *****
06/27 04:32:13 PM   acc = 0.5625
06/27 04:32:13 PM   cls_loss = 12.131502270698547
06/27 04:32:13 PM   eval_loss = 4.208447456359863
06/27 04:32:13 PM   global_step = 4
06/27 04:32:13 PM   loss = 12.131502270698547
06/27 04:32:13 PM ***** Save model *****
06/27 04:32:13 PM ***** Test Dataset Eval Result *****
06/27 04:33:16 PM ***** Eval results *****
06/27 04:33:16 PM   acc = 0.7285
06/27 04:33:16 PM   cls_loss = 12.131502270698547
06/27 04:33:16 PM   eval_loss = 4.367527076176235
06/27 04:33:16 PM   global_step = 4
06/27 04:33:16 PM   loss = 12.131502270698547
06/27 04:33:20 PM ***** LOSS printing *****
06/27 04:33:20 PM loss
06/27 04:33:20 PM tensor(3.6828, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:33:20 PM ***** LOSS printing *****
06/27 04:33:20 PM loss
06/27 04:33:20 PM tensor(2.5727, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:33:20 PM ***** LOSS printing *****
06/27 04:33:20 PM loss
06/27 04:33:20 PM tensor(1.8883, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:33:21 PM ***** LOSS printing *****
06/27 04:33:21 PM loss
06/27 04:33:21 PM tensor(3.4917, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:33:21 PM ***** LOSS printing *****
06/27 04:33:21 PM loss
06/27 04:33:21 PM tensor(2.2400, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:33:21 PM ***** Running evaluation MLM *****
06/27 04:33:21 PM   Epoch = 0 iter 9 step
06/27 04:33:21 PM   Num examples = 16
06/27 04:33:21 PM   Batch size = 32
06/27 04:33:21 PM ***** Eval results *****
06/27 04:33:21 PM   acc = 0.875
06/27 04:33:21 PM   cls_loss = 6.933500819736057
06/27 04:33:21 PM   eval_loss = 1.653497576713562
06/27 04:33:21 PM   global_step = 9
06/27 04:33:21 PM   loss = 6.933500819736057
06/27 04:33:21 PM ***** Save model *****
06/27 04:33:21 PM ***** Test Dataset Eval Result *****
06/27 04:34:24 PM ***** Eval results *****
06/27 04:34:24 PM   acc = 0.815
06/27 04:34:24 PM   cls_loss = 6.933500819736057
06/27 04:34:24 PM   eval_loss = 1.62789119236053
06/27 04:34:24 PM   global_step = 9
06/27 04:34:24 PM   loss = 6.933500819736057
06/27 04:34:28 PM ***** LOSS printing *****
06/27 04:34:28 PM loss
06/27 04:34:28 PM tensor(1.6193, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:29 PM ***** LOSS printing *****
06/27 04:34:29 PM loss
06/27 04:34:29 PM tensor(4.0737, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:29 PM ***** LOSS printing *****
06/27 04:34:29 PM loss
06/27 04:34:29 PM tensor(5.6642, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:29 PM ***** LOSS printing *****
06/27 04:34:29 PM loss
06/27 04:34:29 PM tensor(2.2590, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:29 PM ***** LOSS printing *****
06/27 04:34:29 PM loss
06/27 04:34:29 PM tensor(1.8251, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:29 PM ***** Running evaluation MLM *****
06/27 04:34:29 PM   Epoch = 1 iter 14 step
06/27 04:34:29 PM   Num examples = 16
06/27 04:34:29 PM   Batch size = 32
06/27 04:34:30 PM ***** Eval results *****
06/27 04:34:30 PM   acc = 0.8125
06/27 04:34:30 PM   cls_loss = 2.0420608520507812
06/27 04:34:30 PM   eval_loss = 1.2888859510421753
06/27 04:34:30 PM   global_step = 14
06/27 04:34:30 PM   loss = 2.0420608520507812
06/27 04:34:30 PM ***** LOSS printing *****
06/27 04:34:30 PM loss
06/27 04:34:30 PM tensor(1.2100, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:30 PM ***** LOSS printing *****
06/27 04:34:30 PM loss
06/27 04:34:30 PM tensor(2.2355, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:30 PM ***** LOSS printing *****
06/27 04:34:30 PM loss
06/27 04:34:30 PM tensor(3.8960, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:31 PM ***** LOSS printing *****
06/27 04:34:31 PM loss
06/27 04:34:31 PM tensor(2.6329, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:31 PM ***** LOSS printing *****
06/27 04:34:31 PM loss
06/27 04:34:31 PM tensor(3.1529, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:31 PM ***** Running evaluation MLM *****
06/27 04:34:31 PM   Epoch = 1 iter 19 step
06/27 04:34:31 PM   Num examples = 16
06/27 04:34:31 PM   Batch size = 32
06/27 04:34:32 PM ***** Eval results *****
06/27 04:34:32 PM   acc = 0.75
06/27 04:34:32 PM   cls_loss = 2.458765455654689
06/27 04:34:32 PM   eval_loss = 3.002533197402954
06/27 04:34:32 PM   global_step = 19
06/27 04:34:32 PM   loss = 2.458765455654689
06/27 04:34:32 PM ***** LOSS printing *****
06/27 04:34:32 PM loss
06/27 04:34:32 PM tensor(2.0638, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:32 PM ***** LOSS printing *****
06/27 04:34:32 PM loss
06/27 04:34:32 PM tensor(2.2578, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:32 PM ***** LOSS printing *****
06/27 04:34:32 PM loss
06/27 04:34:32 PM tensor(1.9744, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:32 PM ***** LOSS printing *****
06/27 04:34:32 PM loss
06/27 04:34:32 PM tensor(1.3169, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:32 PM ***** LOSS printing *****
06/27 04:34:32 PM loss
06/27 04:34:32 PM tensor(2.6463, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:33 PM ***** Running evaluation MLM *****
06/27 04:34:33 PM   Epoch = 1 iter 24 step
06/27 04:34:33 PM   Num examples = 16
06/27 04:34:33 PM   Batch size = 32
06/27 04:34:33 PM ***** Eval results *****
06/27 04:34:33 PM   acc = 0.75
06/27 04:34:33 PM   cls_loss = 2.289207547903061
06/27 04:34:33 PM   eval_loss = 1.8824794292449951
06/27 04:34:33 PM   global_step = 24
06/27 04:34:33 PM   loss = 2.289207547903061
06/27 04:34:33 PM ***** LOSS printing *****
06/27 04:34:33 PM loss
06/27 04:34:33 PM tensor(1.8571, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:33 PM ***** LOSS printing *****
06/27 04:34:33 PM loss
06/27 04:34:33 PM tensor(1.5044, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:34 PM ***** LOSS printing *****
06/27 04:34:34 PM loss
06/27 04:34:34 PM tensor(1.1946, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:34 PM ***** LOSS printing *****
06/27 04:34:34 PM loss
06/27 04:34:34 PM tensor(2.4046, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:34 PM ***** LOSS printing *****
06/27 04:34:34 PM loss
06/27 04:34:34 PM tensor(1.8947, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:34 PM ***** Running evaluation MLM *****
06/27 04:34:34 PM   Epoch = 2 iter 29 step
06/27 04:34:34 PM   Num examples = 16
06/27 04:34:34 PM   Batch size = 32
06/27 04:34:35 PM ***** Eval results *****
06/27 04:34:35 PM   acc = 0.75
06/27 04:34:35 PM   cls_loss = 1.771083950996399
06/27 04:34:35 PM   eval_loss = 1.4600071907043457
06/27 04:34:35 PM   global_step = 29
06/27 04:34:35 PM   loss = 1.771083950996399
06/27 04:34:35 PM ***** LOSS printing *****
06/27 04:34:35 PM loss
06/27 04:34:35 PM tensor(1.7257, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:35 PM ***** LOSS printing *****
06/27 04:34:35 PM loss
06/27 04:34:35 PM tensor(1.0542, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:35 PM ***** LOSS printing *****
06/27 04:34:35 PM loss
06/27 04:34:35 PM tensor(1.7346, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:35 PM ***** LOSS printing *****
06/27 04:34:35 PM loss
06/27 04:34:35 PM tensor(1.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:36 PM ***** LOSS printing *****
06/27 04:34:36 PM loss
06/27 04:34:36 PM tensor(1.9109, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:36 PM ***** Running evaluation MLM *****
06/27 04:34:36 PM   Epoch = 2 iter 34 step
06/27 04:34:36 PM   Num examples = 16
06/27 04:34:36 PM   Batch size = 32
06/27 04:34:36 PM ***** Eval results *****
06/27 04:34:36 PM   acc = 0.6875
06/27 04:34:36 PM   cls_loss = 1.6334813356399536
06/27 04:34:36 PM   eval_loss = 2.322986602783203
06/27 04:34:36 PM   global_step = 34
06/27 04:34:36 PM   loss = 1.6334813356399536
06/27 04:34:36 PM ***** LOSS printing *****
06/27 04:34:36 PM loss
06/27 04:34:36 PM tensor(3.0093, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:37 PM ***** LOSS printing *****
06/27 04:34:37 PM loss
06/27 04:34:37 PM tensor(1.9293, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:37 PM ***** LOSS printing *****
06/27 04:34:37 PM loss
06/27 04:34:37 PM tensor(1.8947, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:37 PM ***** LOSS printing *****
06/27 04:34:37 PM loss
06/27 04:34:37 PM tensor(1.2564, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:37 PM ***** LOSS printing *****
06/27 04:34:37 PM loss
06/27 04:34:37 PM tensor(1.3583, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:37 PM ***** Running evaluation MLM *****
06/27 04:34:37 PM   Epoch = 3 iter 39 step
06/27 04:34:37 PM   Num examples = 16
06/27 04:34:37 PM   Batch size = 32
06/27 04:34:38 PM ***** Eval results *****
06/27 04:34:38 PM   acc = 0.75
06/27 04:34:38 PM   cls_loss = 1.5031379461288452
06/27 04:34:38 PM   eval_loss = 1.672720193862915
06/27 04:34:38 PM   global_step = 39
06/27 04:34:38 PM   loss = 1.5031379461288452
06/27 04:34:38 PM ***** LOSS printing *****
06/27 04:34:38 PM loss
06/27 04:34:38 PM tensor(1.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:38 PM ***** LOSS printing *****
06/27 04:34:38 PM loss
06/27 04:34:38 PM tensor(1.7262, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:38 PM ***** LOSS printing *****
06/27 04:34:38 PM loss
06/27 04:34:38 PM tensor(1.3112, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:39 PM ***** LOSS printing *****
06/27 04:34:39 PM loss
06/27 04:34:39 PM tensor(3.5481, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:39 PM ***** LOSS printing *****
06/27 04:34:39 PM loss
06/27 04:34:39 PM tensor(2.6925, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:39 PM ***** Running evaluation MLM *****
06/27 04:34:39 PM   Epoch = 3 iter 44 step
06/27 04:34:39 PM   Num examples = 16
06/27 04:34:39 PM   Batch size = 32
06/27 04:34:40 PM ***** Eval results *****
06/27 04:34:40 PM   acc = 0.75
06/27 04:34:40 PM   cls_loss = 1.9259827733039856
06/27 04:34:40 PM   eval_loss = 1.7532322406768799
06/27 04:34:40 PM   global_step = 44
06/27 04:34:40 PM   loss = 1.9259827733039856
06/27 04:34:40 PM ***** LOSS printing *****
06/27 04:34:40 PM loss
06/27 04:34:40 PM tensor(1.0262, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:40 PM ***** LOSS printing *****
06/27 04:34:40 PM loss
06/27 04:34:40 PM tensor(1.5762, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:40 PM ***** LOSS printing *****
06/27 04:34:40 PM loss
06/27 04:34:40 PM tensor(1.5383, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:40 PM ***** LOSS printing *****
06/27 04:34:40 PM loss
06/27 04:34:40 PM tensor(0.7931, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:40 PM ***** LOSS printing *****
06/27 04:34:40 PM loss
06/27 04:34:40 PM tensor(1.2457, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:41 PM ***** Running evaluation MLM *****
06/27 04:34:41 PM   Epoch = 4 iter 49 step
06/27 04:34:41 PM   Num examples = 16
06/27 04:34:41 PM   Batch size = 32
06/27 04:34:41 PM ***** Eval results *****
06/27 04:34:41 PM   acc = 0.8125
06/27 04:34:41 PM   cls_loss = 1.245664358139038
06/27 04:34:41 PM   eval_loss = 1.261641025543213
06/27 04:34:41 PM   global_step = 49
06/27 04:34:41 PM   loss = 1.245664358139038
06/27 04:34:41 PM ***** LOSS printing *****
06/27 04:34:41 PM loss
06/27 04:34:41 PM tensor(1.4723, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:41 PM ***** LOSS printing *****
06/27 04:34:41 PM loss
06/27 04:34:41 PM tensor(1.7601, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:42 PM ***** LOSS printing *****
06/27 04:34:42 PM loss
06/27 04:34:42 PM tensor(1.1953, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:42 PM ***** LOSS printing *****
06/27 04:34:42 PM loss
06/27 04:34:42 PM tensor(0.9209, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:42 PM ***** LOSS printing *****
06/27 04:34:42 PM loss
06/27 04:34:42 PM tensor(1.4460, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:42 PM ***** Running evaluation MLM *****
06/27 04:34:42 PM   Epoch = 4 iter 54 step
06/27 04:34:42 PM   Num examples = 16
06/27 04:34:42 PM   Batch size = 32
06/27 04:34:43 PM ***** Eval results *****
06/27 04:34:43 PM   acc = 0.8125
06/27 04:34:43 PM   cls_loss = 1.3400554656982422
06/27 04:34:43 PM   eval_loss = 1.6526553630828857
06/27 04:34:43 PM   global_step = 54
06/27 04:34:43 PM   loss = 1.3400554656982422
06/27 04:34:43 PM ***** LOSS printing *****
06/27 04:34:43 PM loss
06/27 04:34:43 PM tensor(1.4891, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:43 PM ***** LOSS printing *****
06/27 04:34:43 PM loss
06/27 04:34:43 PM tensor(1.5343, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:43 PM ***** LOSS printing *****
06/27 04:34:43 PM loss
06/27 04:34:43 PM tensor(1.0323, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:43 PM ***** LOSS printing *****
06/27 04:34:43 PM loss
06/27 04:34:43 PM tensor(1.3549, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:44 PM ***** LOSS printing *****
06/27 04:34:44 PM loss
06/27 04:34:44 PM tensor(1.4462, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:44 PM ***** Running evaluation MLM *****
06/27 04:34:44 PM   Epoch = 4 iter 59 step
06/27 04:34:44 PM   Num examples = 16
06/27 04:34:44 PM   Batch size = 32
06/27 04:34:44 PM ***** Eval results *****
06/27 04:34:44 PM   acc = 0.75
06/27 04:34:44 PM   cls_loss = 1.35428663817319
06/27 04:34:44 PM   eval_loss = 1.723585844039917
06/27 04:34:44 PM   global_step = 59
06/27 04:34:44 PM   loss = 1.35428663817319
06/27 04:34:44 PM ***** LOSS printing *****
06/27 04:34:44 PM loss
06/27 04:34:44 PM tensor(1.2873, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:45 PM ***** LOSS printing *****
06/27 04:34:45 PM loss
06/27 04:34:45 PM tensor(0.8857, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:45 PM ***** LOSS printing *****
06/27 04:34:45 PM loss
06/27 04:34:45 PM tensor(1.1143, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:45 PM ***** LOSS printing *****
06/27 04:34:45 PM loss
06/27 04:34:45 PM tensor(1.2653, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:45 PM ***** LOSS printing *****
06/27 04:34:45 PM loss
06/27 04:34:45 PM tensor(1.5221, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:45 PM ***** Running evaluation MLM *****
06/27 04:34:45 PM   Epoch = 5 iter 64 step
06/27 04:34:45 PM   Num examples = 16
06/27 04:34:45 PM   Batch size = 32
06/27 04:34:46 PM ***** Eval results *****
06/27 04:34:46 PM   acc = 0.6875
06/27 04:34:46 PM   cls_loss = 1.1968584656715393
06/27 04:34:46 PM   eval_loss = 1.8893805742263794
06/27 04:34:46 PM   global_step = 64
06/27 04:34:46 PM   loss = 1.1968584656715393
06/27 04:34:46 PM ***** LOSS printing *****
06/27 04:34:46 PM loss
06/27 04:34:46 PM tensor(1.7769, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:46 PM ***** LOSS printing *****
06/27 04:34:46 PM loss
06/27 04:34:46 PM tensor(1.2097, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:46 PM ***** LOSS printing *****
06/27 04:34:46 PM loss
06/27 04:34:46 PM tensor(1.1094, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:47 PM ***** LOSS printing *****
06/27 04:34:47 PM loss
06/27 04:34:47 PM tensor(1.2271, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:47 PM ***** LOSS printing *****
06/27 04:34:47 PM loss
06/27 04:34:47 PM tensor(1.3134, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:47 PM ***** Running evaluation MLM *****
06/27 04:34:47 PM   Epoch = 5 iter 69 step
06/27 04:34:47 PM   Num examples = 16
06/27 04:34:47 PM   Batch size = 32
06/27 04:34:48 PM ***** Eval results *****
06/27 04:34:48 PM   acc = 0.75
06/27 04:34:48 PM   cls_loss = 1.2693151368035211
06/27 04:34:48 PM   eval_loss = 2.0710020065307617
06/27 04:34:48 PM   global_step = 69
06/27 04:34:48 PM   loss = 1.2693151368035211
06/27 04:34:48 PM ***** LOSS printing *****
06/27 04:34:48 PM loss
06/27 04:34:48 PM tensor(1.1206, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:48 PM ***** LOSS printing *****
06/27 04:34:48 PM loss
06/27 04:34:48 PM tensor(1.2125, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:48 PM ***** LOSS printing *****
06/27 04:34:48 PM loss
06/27 04:34:48 PM tensor(1.0978, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:48 PM ***** LOSS printing *****
06/27 04:34:48 PM loss
06/27 04:34:48 PM tensor(1.0356, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:49 PM ***** LOSS printing *****
06/27 04:34:49 PM loss
06/27 04:34:49 PM tensor(1.0795, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:49 PM ***** Running evaluation MLM *****
06/27 04:34:49 PM   Epoch = 6 iter 74 step
06/27 04:34:49 PM   Num examples = 16
06/27 04:34:49 PM   Batch size = 32
06/27 04:34:49 PM ***** Eval results *****
06/27 04:34:49 PM   acc = 0.75
06/27 04:34:49 PM   cls_loss = 1.0575514435768127
06/27 04:34:49 PM   eval_loss = 2.000230312347412
06/27 04:34:49 PM   global_step = 74
06/27 04:34:49 PM   loss = 1.0575514435768127
06/27 04:34:49 PM ***** LOSS printing *****
06/27 04:34:49 PM loss
06/27 04:34:49 PM tensor(1.4292, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:49 PM ***** LOSS printing *****
06/27 04:34:49 PM loss
06/27 04:34:49 PM tensor(0.9725, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:50 PM ***** LOSS printing *****
06/27 04:34:50 PM loss
06/27 04:34:50 PM tensor(1.7355, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:50 PM ***** LOSS printing *****
06/27 04:34:50 PM loss
06/27 04:34:50 PM tensor(1.1762, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:50 PM ***** LOSS printing *****
06/27 04:34:50 PM loss
06/27 04:34:50 PM tensor(1.5905, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:50 PM ***** Running evaluation MLM *****
06/27 04:34:50 PM   Epoch = 6 iter 79 step
06/27 04:34:50 PM   Num examples = 16
06/27 04:34:50 PM   Batch size = 32
06/27 04:34:51 PM ***** Eval results *****
06/27 04:34:51 PM   acc = 0.75
06/27 04:34:51 PM   cls_loss = 1.2884242619786943
06/27 04:34:51 PM   eval_loss = 2.202986478805542
06/27 04:34:51 PM   global_step = 79
06/27 04:34:51 PM   loss = 1.2884242619786943
06/27 04:34:51 PM ***** LOSS printing *****
06/27 04:34:51 PM loss
06/27 04:34:51 PM tensor(1.4360, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:51 PM ***** LOSS printing *****
06/27 04:34:51 PM loss
06/27 04:34:51 PM tensor(2.0574, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:51 PM ***** LOSS printing *****
06/27 04:34:51 PM loss
06/27 04:34:51 PM tensor(1.0032, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:52 PM ***** LOSS printing *****
06/27 04:34:52 PM loss
06/27 04:34:52 PM tensor(1.3493, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:52 PM ***** LOSS printing *****
06/27 04:34:52 PM loss
06/27 04:34:52 PM tensor(1.7778, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:52 PM ***** Running evaluation MLM *****
06/27 04:34:52 PM   Epoch = 6 iter 84 step
06/27 04:34:52 PM   Num examples = 16
06/27 04:34:52 PM   Batch size = 32
06/27 04:34:52 PM ***** Eval results *****
06/27 04:34:52 PM   acc = 0.75
06/27 04:34:52 PM   cls_loss = 1.3868914991617203
06/27 04:34:52 PM   eval_loss = 2.5966954231262207
06/27 04:34:52 PM   global_step = 84
06/27 04:34:52 PM   loss = 1.3868914991617203
06/27 04:34:52 PM ***** LOSS printing *****
06/27 04:34:52 PM loss
06/27 04:34:52 PM tensor(0.8075, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:53 PM ***** LOSS printing *****
06/27 04:34:53 PM loss
06/27 04:34:53 PM tensor(1.6226, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:53 PM ***** LOSS printing *****
06/27 04:34:53 PM loss
06/27 04:34:53 PM tensor(1.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:53 PM ***** LOSS printing *****
06/27 04:34:53 PM loss
06/27 04:34:53 PM tensor(1.1637, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:53 PM ***** LOSS printing *****
06/27 04:34:53 PM loss
06/27 04:34:53 PM tensor(0.9821, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:54 PM ***** Running evaluation MLM *****
06/27 04:34:54 PM   Epoch = 7 iter 89 step
06/27 04:34:54 PM   Num examples = 16
06/27 04:34:54 PM   Batch size = 32
06/27 04:34:54 PM ***** Eval results *****
06/27 04:34:54 PM   acc = 0.75
06/27 04:34:54 PM   cls_loss = 1.129616904258728
06/27 04:34:54 PM   eval_loss = 2.714567184448242
06/27 04:34:54 PM   global_step = 89
06/27 04:34:54 PM   loss = 1.129616904258728
06/27 04:34:54 PM ***** LOSS printing *****
06/27 04:34:54 PM loss
06/27 04:34:54 PM tensor(1.7064, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:54 PM ***** LOSS printing *****
06/27 04:34:54 PM loss
06/27 04:34:54 PM tensor(1.4307, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:55 PM ***** LOSS printing *****
06/27 04:34:55 PM loss
06/27 04:34:55 PM tensor(1.3685, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:55 PM ***** LOSS printing *****
06/27 04:34:55 PM loss
06/27 04:34:55 PM tensor(1.0600, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:55 PM ***** LOSS printing *****
06/27 04:34:55 PM loss
06/27 04:34:55 PM tensor(0.9361, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:55 PM ***** Running evaluation MLM *****
06/27 04:34:55 PM   Epoch = 7 iter 94 step
06/27 04:34:55 PM   Num examples = 16
06/27 04:34:55 PM   Batch size = 32
06/27 04:34:56 PM ***** Eval results *****
06/27 04:34:56 PM   acc = 0.75
06/27 04:34:56 PM   cls_loss = 1.2149708330631257
06/27 04:34:56 PM   eval_loss = 2.2197234630584717
06/27 04:34:56 PM   global_step = 94
06/27 04:34:56 PM   loss = 1.2149708330631257
06/27 04:34:56 PM ***** LOSS printing *****
06/27 04:34:56 PM loss
06/27 04:34:56 PM tensor(1.2226, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:56 PM ***** LOSS printing *****
06/27 04:34:56 PM loss
06/27 04:34:56 PM tensor(1.2788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:56 PM ***** LOSS printing *****
06/27 04:34:56 PM loss
06/27 04:34:56 PM tensor(1.2362, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:56 PM ***** LOSS printing *****
06/27 04:34:56 PM loss
06/27 04:34:56 PM tensor(1.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:57 PM ***** LOSS printing *****
06/27 04:34:57 PM loss
06/27 04:34:57 PM tensor(1.6009, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:57 PM ***** Running evaluation MLM *****
06/27 04:34:57 PM   Epoch = 8 iter 99 step
06/27 04:34:57 PM   Num examples = 16
06/27 04:34:57 PM   Batch size = 32
06/27 04:34:57 PM ***** Eval results *****
06/27 04:34:57 PM   acc = 0.75
06/27 04:34:57 PM   cls_loss = 1.2888123989105225
06/27 04:34:57 PM   eval_loss = 2.0359630584716797
06/27 04:34:57 PM   global_step = 99
06/27 04:34:57 PM   loss = 1.2888123989105225
06/27 04:34:57 PM ***** LOSS printing *****
06/27 04:34:57 PM loss
06/27 04:34:57 PM tensor(1.5174, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:58 PM ***** LOSS printing *****
06/27 04:34:58 PM loss
06/27 04:34:58 PM tensor(1.3971, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:58 PM ***** LOSS printing *****
06/27 04:34:58 PM loss
06/27 04:34:58 PM tensor(0.7824, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:58 PM ***** LOSS printing *****
06/27 04:34:58 PM loss
06/27 04:34:58 PM tensor(1.0666, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:58 PM ***** LOSS printing *****
06/27 04:34:58 PM loss
06/27 04:34:58 PM tensor(1.1160, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:58 PM ***** Running evaluation MLM *****
06/27 04:34:58 PM   Epoch = 8 iter 104 step
06/27 04:34:58 PM   Num examples = 16
06/27 04:34:58 PM   Batch size = 32
06/27 04:34:59 PM ***** Eval results *****
06/27 04:34:59 PM   acc = 0.75
06/27 04:34:59 PM   cls_loss = 1.2182393819093704
06/27 04:34:59 PM   eval_loss = 1.9040248394012451
06/27 04:34:59 PM   global_step = 104
06/27 04:34:59 PM   loss = 1.2182393819093704
06/27 04:34:59 PM ***** LOSS printing *****
06/27 04:34:59 PM loss
06/27 04:34:59 PM tensor(1.8555, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:59 PM ***** LOSS printing *****
06/27 04:34:59 PM loss
06/27 04:34:59 PM tensor(1.6267, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:34:59 PM ***** LOSS printing *****
06/27 04:34:59 PM loss
06/27 04:34:59 PM tensor(1.1155, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:00 PM ***** LOSS printing *****
06/27 04:35:00 PM loss
06/27 04:35:00 PM tensor(1.3965, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:00 PM ***** LOSS printing *****
06/27 04:35:00 PM loss
06/27 04:35:00 PM tensor(1.3295, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:00 PM ***** Running evaluation MLM *****
06/27 04:35:00 PM   Epoch = 9 iter 109 step
06/27 04:35:00 PM   Num examples = 16
06/27 04:35:00 PM   Batch size = 32
06/27 04:35:01 PM ***** Eval results *****
06/27 04:35:01 PM   acc = 0.75
06/27 04:35:01 PM   cls_loss = 1.3295040130615234
06/27 04:35:01 PM   eval_loss = 2.178740978240967
06/27 04:35:01 PM   global_step = 109
06/27 04:35:01 PM   loss = 1.3295040130615234
06/27 04:35:01 PM ***** LOSS printing *****
06/27 04:35:01 PM loss
06/27 04:35:01 PM tensor(1.2318, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:01 PM ***** LOSS printing *****
06/27 04:35:01 PM loss
06/27 04:35:01 PM tensor(1.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:01 PM ***** LOSS printing *****
06/27 04:35:01 PM loss
06/27 04:35:01 PM tensor(0.9526, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:01 PM ***** LOSS printing *****
06/27 04:35:01 PM loss
06/27 04:35:01 PM tensor(1.3086, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:01 PM ***** LOSS printing *****
06/27 04:35:01 PM loss
06/27 04:35:01 PM tensor(1.1170, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:02 PM ***** Running evaluation MLM *****
06/27 04:35:02 PM   Epoch = 9 iter 114 step
06/27 04:35:02 PM   Num examples = 16
06/27 04:35:02 PM   Batch size = 32
06/27 04:35:02 PM ***** Eval results *****
06/27 04:35:02 PM   acc = 0.75
06/27 04:35:02 PM   cls_loss = 1.1626748243967693
06/27 04:35:02 PM   eval_loss = 2.54724383354187
06/27 04:35:02 PM   global_step = 114
06/27 04:35:02 PM   loss = 1.1626748243967693
06/27 04:35:02 PM ***** LOSS printing *****
06/27 04:35:02 PM loss
06/27 04:35:02 PM tensor(1.0804, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:02 PM ***** LOSS printing *****
06/27 04:35:02 PM loss
06/27 04:35:02 PM tensor(1.2791, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:03 PM ***** LOSS printing *****
06/27 04:35:03 PM loss
06/27 04:35:03 PM tensor(1.2282, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:03 PM ***** LOSS printing *****
06/27 04:35:03 PM loss
06/27 04:35:03 PM tensor(1.5152, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:03 PM ***** LOSS printing *****
06/27 04:35:03 PM loss
06/27 04:35:03 PM tensor(1.4939, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:35:03 PM ***** Running evaluation MLM *****
06/27 04:35:03 PM   Epoch = 9 iter 119 step
06/27 04:35:03 PM   Num examples = 16
06/27 04:35:03 PM   Batch size = 32
06/27 04:35:04 PM ***** Eval results *****
06/27 04:35:04 PM   acc = 0.75
06/27 04:35:04 PM   cls_loss = 1.2339056730270386
06/27 04:35:04 PM   eval_loss = 2.6479272842407227
06/27 04:35:04 PM   global_step = 119
06/27 04:35:04 PM   loss = 1.2339056730270386
06/27 04:35:04 PM ***** LOSS printing *****
06/27 04:35:04 PM loss
06/27 04:35:04 PM tensor(1.0335, device='cuda:0', grad_fn=<NllLossBackward0>)

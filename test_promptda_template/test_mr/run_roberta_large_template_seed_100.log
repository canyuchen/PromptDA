06/27 04:38:13 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/mr/8-100/', data_seed=100, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='mr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 04:38:13 PM device: cuda n_gpu: 1
06/27 04:38:13 PM Writing example 0 of 48
06/27 04:38:13 PM *** Example ***
06/27 04:38:13 PM guid: train-1
06/27 04:38:13 PM tokens: <s> supp osed ly Ġbased Ġupon Ġreal Ġ, Ġor Ġat Ġleast Ġsober ly Ġreported Ġincidents Ġ, Ġthe Ġfilm Ġends Ġwith Ġa Ġlarge Ġhuman Ġtragedy Ġ. Ġalas Ġ, Ġgetting Ġthere Ġis Ġnot Ġeven Ġhalf Ġthe Ġinterest Ġ. </s> ĠIt Ġis <mask>
06/27 04:38:13 PM input_ids: 0 16714 7878 352 716 2115 588 2156 50 23 513 17333 352 431 4495 2156 5 822 3587 19 10 739 1050 6906 479 40463 2156 562 89 16 45 190 457 5 773 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM label: ['Ġterrible']
06/27 04:38:13 PM Writing example 0 of 16
06/27 04:38:13 PM *** Example ***
06/27 04:38:13 PM guid: dev-1
06/27 04:38:13 PM tokens: <s> although Ġbased Ġon Ġa Ġreal - life Ġperson Ġ, Ġjohn Ġ, Ġin Ġthe Ġmovie Ġ, Ġis Ġa Ġrather Ġdull Ġperson Ġto Ġbe Ġstuck Ġwith Ġfor Ġtwo Ġhours Ġ. </s> ĠIt Ġis <mask>
06/27 04:38:13 PM input_ids: 0 24648 716 15 10 588 12 5367 621 2156 41906 2156 11 5 1569 2156 16 10 1195 22018 621 7 28 4889 19 13 80 722 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM label: ['Ġterrible']
06/27 04:38:13 PM Writing example 0 of 2000
06/27 04:38:13 PM *** Example ***
06/27 04:38:13 PM guid: dev-1
06/27 04:38:13 PM tokens: <s> sim pl istic Ġ, Ġsilly Ġand Ġtedious Ġ. </s> ĠIt Ġis <mask>
06/27 04:38:13 PM input_ids: 0 13092 2911 5580 2156 15470 8 35138 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 04:38:13 PM label: ['Ġterrible']
06/27 04:38:26 PM ***** Running training *****
06/27 04:38:26 PM   Num examples = 48
06/27 04:38:26 PM   Batch size = 4
06/27 04:38:26 PM   Num steps = 120
06/27 04:38:26 PM n: embeddings.word_embeddings.weight
06/27 04:38:26 PM n: embeddings.position_embeddings.weight
06/27 04:38:26 PM n: embeddings.token_type_embeddings.weight
06/27 04:38:26 PM n: embeddings.LayerNorm.weight
06/27 04:38:26 PM n: embeddings.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.0.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.0.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.0.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.0.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.0.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.0.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.0.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.0.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.0.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.0.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.0.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.0.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.1.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.1.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.1.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.1.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.1.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.1.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.1.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.1.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.1.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.1.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.1.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.1.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.2.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.2.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.2.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.2.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.2.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.2.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.2.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.2.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.2.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.2.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.2.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.2.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.3.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.3.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.3.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.3.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.3.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.3.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.3.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.3.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.3.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.3.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.3.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.3.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.4.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.4.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.4.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.4.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.4.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.4.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.4.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.4.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.4.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.4.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.4.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.4.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.5.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.5.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.5.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.5.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.5.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.5.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.5.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.5.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.5.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.5.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.5.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.5.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.6.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.6.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.6.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.6.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.6.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.6.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.6.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.6.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.6.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.6.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.6.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.6.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.7.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.7.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.7.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.7.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.7.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.7.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.7.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.7.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.7.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.7.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.7.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.7.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.8.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.8.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.8.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.8.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.8.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.8.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.8.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.8.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.8.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.8.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.8.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.8.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.9.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.9.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.9.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.9.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.9.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.9.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.9.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.9.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.9.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.9.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.9.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.9.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.10.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.10.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.10.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.10.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.10.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.10.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.10.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.10.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.10.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.10.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.10.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.10.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.11.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.11.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.11.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.11.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.11.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.11.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.11.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.11.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.11.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.11.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.11.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.11.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.12.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.12.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.12.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.12.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.12.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.12.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.12.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.12.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.12.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.12.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.12.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.12.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.13.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.13.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.13.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.13.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.13.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.13.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.13.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.13.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.13.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.13.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.13.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.13.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.14.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.14.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.14.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.14.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.14.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.14.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.14.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.14.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.14.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.14.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.14.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.14.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.15.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.15.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.15.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.15.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.15.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.15.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.15.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.15.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.15.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.15.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.15.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.15.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.16.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.16.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.16.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.16.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.16.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.16.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.16.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.16.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.16.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.16.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.16.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.16.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.17.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.17.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.17.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.17.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.17.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.17.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.17.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.17.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.17.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.17.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.17.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.17.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.18.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.18.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.18.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.18.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.18.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.18.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.18.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.18.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.18.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.18.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.18.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.18.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.19.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.19.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.19.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.19.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.19.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.19.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.19.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.19.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.19.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.19.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.19.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.19.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.20.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.20.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.20.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.20.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.20.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.20.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.20.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.20.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.20.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.20.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.20.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.20.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.21.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.21.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.21.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.21.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.21.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.21.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.21.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.21.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.21.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.21.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.21.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.21.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.22.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.22.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.22.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.22.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.22.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.22.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.22.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.22.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.22.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.22.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.22.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.22.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.23.attention.self.query.weight
06/27 04:38:26 PM n: encoder.layer.23.attention.self.query.bias
06/27 04:38:26 PM n: encoder.layer.23.attention.self.key.weight
06/27 04:38:26 PM n: encoder.layer.23.attention.self.key.bias
06/27 04:38:26 PM n: encoder.layer.23.attention.self.value.weight
06/27 04:38:26 PM n: encoder.layer.23.attention.self.value.bias
06/27 04:38:26 PM n: encoder.layer.23.attention.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.23.attention.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: encoder.layer.23.intermediate.dense.weight
06/27 04:38:26 PM n: encoder.layer.23.intermediate.dense.bias
06/27 04:38:26 PM n: encoder.layer.23.output.dense.weight
06/27 04:38:26 PM n: encoder.layer.23.output.dense.bias
06/27 04:38:26 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 04:38:26 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 04:38:26 PM n: pooler.dense.weight
06/27 04:38:26 PM n: pooler.dense.bias
06/27 04:38:26 PM n: roberta.embeddings.word_embeddings.weight
06/27 04:38:26 PM n: roberta.embeddings.position_embeddings.weight
06/27 04:38:26 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 04:38:26 PM n: roberta.embeddings.LayerNorm.weight
06/27 04:38:26 PM n: roberta.embeddings.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 04:38:26 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 04:38:26 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 04:38:26 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 04:38:26 PM n: roberta.pooler.dense.weight
06/27 04:38:26 PM n: roberta.pooler.dense.bias
06/27 04:38:26 PM n: lm_head.bias
06/27 04:38:26 PM n: lm_head.dense.weight
06/27 04:38:26 PM n: lm_head.dense.bias
06/27 04:38:26 PM n: lm_head.layer_norm.weight
06/27 04:38:26 PM n: lm_head.layer_norm.bias
06/27 04:38:26 PM n: lm_head.decoder.weight
06/27 04:38:26 PM Total parameters: 763292761
06/27 04:38:27 PM ***** LOSS printing *****
06/27 04:38:27 PM loss
06/27 04:38:27 PM tensor(18.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:38:27 PM ***** LOSS printing *****
06/27 04:38:27 PM loss
06/27 04:38:27 PM tensor(11.2176, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:38:27 PM ***** LOSS printing *****
06/27 04:38:27 PM loss
06/27 04:38:27 PM tensor(7.6021, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:38:27 PM ***** LOSS printing *****
06/27 04:38:27 PM loss
06/27 04:38:27 PM tensor(5.2290, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:38:27 PM ***** Running evaluation MLM *****
06/27 04:38:27 PM   Epoch = 0 iter 4 step
06/27 04:38:27 PM   Num examples = 16
06/27 04:38:27 PM   Batch size = 32
06/27 04:38:28 PM ***** Eval results *****
06/27 04:38:28 PM   acc = 0.8125
06/27 04:38:28 PM   cls_loss = 10.521360158920288
06/27 04:38:28 PM   eval_loss = 3.9201176166534424
06/27 04:38:28 PM   global_step = 4
06/27 04:38:28 PM   loss = 10.521360158920288
06/27 04:38:28 PM ***** Save model *****
06/27 04:38:28 PM ***** Test Dataset Eval Result *****
06/27 04:39:31 PM ***** Eval results *****
06/27 04:39:31 PM   acc = 0.7815
06/27 04:39:31 PM   cls_loss = 10.521360158920288
06/27 04:39:31 PM   eval_loss = 4.014605964933123
06/27 04:39:31 PM   global_step = 4
06/27 04:39:31 PM   loss = 10.521360158920288
06/27 04:39:35 PM ***** LOSS printing *****
06/27 04:39:35 PM loss
06/27 04:39:35 PM tensor(3.7964, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:39:36 PM ***** LOSS printing *****
06/27 04:39:36 PM loss
06/27 04:39:36 PM tensor(3.0488, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:39:36 PM ***** LOSS printing *****
06/27 04:39:36 PM loss
06/27 04:39:36 PM tensor(2.9677, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:39:36 PM ***** LOSS printing *****
06/27 04:39:36 PM loss
06/27 04:39:36 PM tensor(2.8055, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:39:36 PM ***** LOSS printing *****
06/27 04:39:36 PM loss
06/27 04:39:36 PM tensor(5.2484, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:39:37 PM ***** Running evaluation MLM *****
06/27 04:39:37 PM   Epoch = 0 iter 9 step
06/27 04:39:37 PM   Num examples = 16
06/27 04:39:37 PM   Batch size = 32
06/27 04:39:37 PM ***** Eval results *****
06/27 04:39:37 PM   acc = 0.875
06/27 04:39:37 PM   cls_loss = 6.661369429694282
06/27 04:39:37 PM   eval_loss = 2.3507683277130127
06/27 04:39:37 PM   global_step = 9
06/27 04:39:37 PM   loss = 6.661369429694282
06/27 04:39:37 PM ***** Save model *****
06/27 04:39:37 PM ***** Test Dataset Eval Result *****
06/27 04:40:40 PM ***** Eval results *****
06/27 04:40:40 PM   acc = 0.835
06/27 04:40:40 PM   cls_loss = 6.661369429694282
06/27 04:40:40 PM   eval_loss = 2.50511191950904
06/27 04:40:40 PM   global_step = 9
06/27 04:40:40 PM   loss = 6.661369429694282
06/27 04:40:45 PM ***** LOSS printing *****
06/27 04:40:45 PM loss
06/27 04:40:45 PM tensor(1.9076, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:45 PM ***** LOSS printing *****
06/27 04:40:45 PM loss
06/27 04:40:45 PM tensor(3.2148, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:45 PM ***** LOSS printing *****
06/27 04:40:45 PM loss
06/27 04:40:45 PM tensor(7.4066, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:46 PM ***** LOSS printing *****
06/27 04:40:46 PM loss
06/27 04:40:46 PM tensor(2.2967, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:46 PM ***** LOSS printing *****
06/27 04:40:46 PM loss
06/27 04:40:46 PM tensor(2.0881, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:46 PM ***** Running evaluation MLM *****
06/27 04:40:46 PM   Epoch = 1 iter 14 step
06/27 04:40:46 PM   Num examples = 16
06/27 04:40:46 PM   Batch size = 32
06/27 04:40:46 PM ***** Eval results *****
06/27 04:40:46 PM   acc = 0.875
06/27 04:40:46 PM   cls_loss = 2.192394733428955
06/27 04:40:46 PM   eval_loss = 1.2740038633346558
06/27 04:40:46 PM   global_step = 14
06/27 04:40:46 PM   loss = 2.192394733428955
06/27 04:40:47 PM ***** LOSS printing *****
06/27 04:40:47 PM loss
06/27 04:40:47 PM tensor(1.2689, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:47 PM ***** LOSS printing *****
06/27 04:40:47 PM loss
06/27 04:40:47 PM tensor(3.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:47 PM ***** LOSS printing *****
06/27 04:40:47 PM loss
06/27 04:40:47 PM tensor(2.2522, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:47 PM ***** LOSS printing *****
06/27 04:40:47 PM loss
06/27 04:40:47 PM tensor(1.5723, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:47 PM ***** LOSS printing *****
06/27 04:40:47 PM loss
06/27 04:40:47 PM tensor(2.7158, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:40:48 PM ***** Running evaluation MLM *****
06/27 04:40:48 PM   Epoch = 1 iter 19 step
06/27 04:40:48 PM   Num examples = 16
06/27 04:40:48 PM   Batch size = 32
06/27 04:40:48 PM ***** Eval results *****
06/27 04:40:48 PM   acc = 0.9375
06/27 04:40:48 PM   cls_loss = 2.273380398750305
06/27 04:40:48 PM   eval_loss = 1.2891271114349365
06/27 04:40:48 PM   global_step = 19
06/27 04:40:48 PM   loss = 2.273380398750305
06/27 04:40:48 PM ***** Save model *****
06/27 04:40:48 PM ***** Test Dataset Eval Result *****
06/27 04:41:52 PM ***** Eval results *****
06/27 04:41:52 PM   acc = 0.858
06/27 04:41:52 PM   cls_loss = 2.273380398750305
06/27 04:41:52 PM   eval_loss = 1.5918587086692688
06/27 04:41:52 PM   global_step = 19
06/27 04:41:52 PM   loss = 2.273380398750305
06/27 04:41:56 PM ***** LOSS printing *****
06/27 04:41:56 PM loss
06/27 04:41:56 PM tensor(2.1830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:56 PM ***** LOSS printing *****
06/27 04:41:56 PM loss
06/27 04:41:56 PM tensor(2.2332, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:57 PM ***** LOSS printing *****
06/27 04:41:57 PM loss
06/27 04:41:57 PM tensor(2.4085, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:57 PM ***** LOSS printing *****
06/27 04:41:57 PM loss
06/27 04:41:57 PM tensor(2.1818, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:57 PM ***** LOSS printing *****
06/27 04:41:57 PM loss
06/27 04:41:57 PM tensor(2.6431, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:57 PM ***** Running evaluation MLM *****
06/27 04:41:57 PM   Epoch = 1 iter 24 step
06/27 04:41:57 PM   Num examples = 16
06/27 04:41:57 PM   Batch size = 32
06/27 04:41:58 PM ***** Eval results *****
06/27 04:41:58 PM   acc = 0.875
06/27 04:41:58 PM   cls_loss = 2.2969469924767814
06/27 04:41:58 PM   eval_loss = 2.680778980255127
06/27 04:41:58 PM   global_step = 24
06/27 04:41:58 PM   loss = 2.2969469924767814
06/27 04:41:58 PM ***** LOSS printing *****
06/27 04:41:58 PM loss
06/27 04:41:58 PM tensor(1.6209, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:58 PM ***** LOSS printing *****
06/27 04:41:58 PM loss
06/27 04:41:58 PM tensor(2.8676, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:58 PM ***** LOSS printing *****
06/27 04:41:58 PM loss
06/27 04:41:58 PM tensor(1.3968, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:58 PM ***** LOSS printing *****
06/27 04:41:58 PM loss
06/27 04:41:58 PM tensor(1.2484, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:59 PM ***** LOSS printing *****
06/27 04:41:59 PM loss
06/27 04:41:59 PM tensor(1.8667, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:41:59 PM ***** Running evaluation MLM *****
06/27 04:41:59 PM   Epoch = 2 iter 29 step
06/27 04:41:59 PM   Num examples = 16
06/27 04:41:59 PM   Batch size = 32
06/27 04:41:59 PM ***** Eval results *****
06/27 04:41:59 PM   acc = 0.9375
06/27 04:41:59 PM   cls_loss = 1.8000722646713256
06/27 04:41:59 PM   eval_loss = 0.8992322683334351
06/27 04:41:59 PM   global_step = 29
06/27 04:41:59 PM   loss = 1.8000722646713256
06/27 04:41:59 PM ***** LOSS printing *****
06/27 04:41:59 PM loss
06/27 04:41:59 PM tensor(1.7826, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:42:00 PM ***** LOSS printing *****
06/27 04:42:00 PM loss
06/27 04:42:00 PM tensor(1.2428, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:42:00 PM ***** LOSS printing *****
06/27 04:42:00 PM loss
06/27 04:42:00 PM tensor(1.7409, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:42:00 PM ***** LOSS printing *****
06/27 04:42:00 PM loss
06/27 04:42:00 PM tensor(1.8554, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:42:00 PM ***** LOSS printing *****
06/27 04:42:00 PM loss
06/27 04:42:00 PM tensor(1.6759, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:42:00 PM ***** Running evaluation MLM *****
06/27 04:42:00 PM   Epoch = 2 iter 34 step
06/27 04:42:00 PM   Num examples = 16
06/27 04:42:00 PM   Batch size = 32
06/27 04:42:01 PM ***** Eval results *****
06/27 04:42:01 PM   acc = 1.0
06/27 04:42:01 PM   cls_loss = 1.729783284664154
06/27 04:42:01 PM   eval_loss = 1.198587417602539
06/27 04:42:01 PM   global_step = 34
06/27 04:42:01 PM   loss = 1.729783284664154
06/27 04:42:01 PM ***** Save model *****
06/27 04:42:01 PM ***** Test Dataset Eval Result *****
06/27 04:43:05 PM ***** Eval results *****
06/27 04:43:05 PM   acc = 0.8445
06/27 04:43:05 PM   cls_loss = 1.729783284664154
06/27 04:43:05 PM   eval_loss = 1.566732408508422
06/27 04:43:05 PM   global_step = 34
06/27 04:43:05 PM   loss = 1.729783284664154
06/27 04:43:09 PM ***** LOSS printing *****
06/27 04:43:09 PM loss
06/27 04:43:09 PM tensor(0.9811, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:09 PM ***** LOSS printing *****
06/27 04:43:09 PM loss
06/27 04:43:09 PM tensor(1.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:09 PM ***** LOSS printing *****
06/27 04:43:09 PM loss
06/27 04:43:09 PM tensor(2.2217, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:10 PM ***** LOSS printing *****
06/27 04:43:10 PM loss
06/27 04:43:10 PM tensor(2.4315, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:10 PM ***** LOSS printing *****
06/27 04:43:10 PM loss
06/27 04:43:10 PM tensor(0.8668, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:10 PM ***** Running evaluation MLM *****
06/27 04:43:10 PM   Epoch = 3 iter 39 step
06/27 04:43:10 PM   Num examples = 16
06/27 04:43:10 PM   Batch size = 32
06/27 04:43:10 PM ***** Eval results *****
06/27 04:43:10 PM   acc = 1.0
06/27 04:43:10 PM   cls_loss = 1.839995801448822
06/27 04:43:10 PM   eval_loss = 3.022758960723877
06/27 04:43:10 PM   global_step = 39
06/27 04:43:10 PM   loss = 1.839995801448822
06/27 04:43:11 PM ***** LOSS printing *****
06/27 04:43:11 PM loss
06/27 04:43:11 PM tensor(1.1342, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:11 PM ***** LOSS printing *****
06/27 04:43:11 PM loss
06/27 04:43:11 PM tensor(1.9226, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:11 PM ***** LOSS printing *****
06/27 04:43:11 PM loss
06/27 04:43:11 PM tensor(1.9548, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:11 PM ***** LOSS printing *****
06/27 04:43:11 PM loss
06/27 04:43:11 PM tensor(2.1121, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:11 PM ***** LOSS printing *****
06/27 04:43:11 PM loss
06/27 04:43:11 PM tensor(1.4740, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:12 PM ***** Running evaluation MLM *****
06/27 04:43:12 PM   Epoch = 3 iter 44 step
06/27 04:43:12 PM   Num examples = 16
06/27 04:43:12 PM   Batch size = 32
06/27 04:43:12 PM ***** Eval results *****
06/27 04:43:12 PM   acc = 0.9375
06/27 04:43:12 PM   cls_loss = 1.764707274734974
06/27 04:43:12 PM   eval_loss = 2.847621202468872
06/27 04:43:12 PM   global_step = 44
06/27 04:43:12 PM   loss = 1.764707274734974
06/27 04:43:12 PM ***** LOSS printing *****
06/27 04:43:12 PM loss
06/27 04:43:12 PM tensor(2.3701, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:12 PM ***** LOSS printing *****
06/27 04:43:12 PM loss
06/27 04:43:12 PM tensor(1.2128, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:13 PM ***** LOSS printing *****
06/27 04:43:13 PM loss
06/27 04:43:13 PM tensor(2.1620, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:13 PM ***** LOSS printing *****
06/27 04:43:13 PM loss
06/27 04:43:13 PM tensor(2.9336, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:13 PM ***** LOSS printing *****
06/27 04:43:13 PM loss
06/27 04:43:13 PM tensor(1.1789, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:13 PM ***** Running evaluation MLM *****
06/27 04:43:13 PM   Epoch = 4 iter 49 step
06/27 04:43:13 PM   Num examples = 16
06/27 04:43:13 PM   Batch size = 32
06/27 04:43:14 PM ***** Eval results *****
06/27 04:43:14 PM   acc = 0.9375
06/27 04:43:14 PM   cls_loss = 1.1789195537567139
06/27 04:43:14 PM   eval_loss = 1.7142486572265625
06/27 04:43:14 PM   global_step = 49
06/27 04:43:14 PM   loss = 1.1789195537567139
06/27 04:43:14 PM ***** LOSS printing *****
06/27 04:43:14 PM loss
06/27 04:43:14 PM tensor(1.5594, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:14 PM ***** LOSS printing *****
06/27 04:43:14 PM loss
06/27 04:43:14 PM tensor(1.4806, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:14 PM ***** LOSS printing *****
06/27 04:43:14 PM loss
06/27 04:43:14 PM tensor(1.1464, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:14 PM ***** LOSS printing *****
06/27 04:43:14 PM loss
06/27 04:43:14 PM tensor(1.0820, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:15 PM ***** LOSS printing *****
06/27 04:43:15 PM loss
06/27 04:43:15 PM tensor(1.4283, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:15 PM ***** Running evaluation MLM *****
06/27 04:43:15 PM   Epoch = 4 iter 54 step
06/27 04:43:15 PM   Num examples = 16
06/27 04:43:15 PM   Batch size = 32
06/27 04:43:15 PM ***** Eval results *****
06/27 04:43:15 PM   acc = 0.9375
06/27 04:43:15 PM   cls_loss = 1.3126002351442974
06/27 04:43:15 PM   eval_loss = 1.7287973165512085
06/27 04:43:15 PM   global_step = 54
06/27 04:43:15 PM   loss = 1.3126002351442974
06/27 04:43:15 PM ***** LOSS printing *****
06/27 04:43:15 PM loss
06/27 04:43:15 PM tensor(1.5696, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:16 PM ***** LOSS printing *****
06/27 04:43:16 PM loss
06/27 04:43:16 PM tensor(2.4371, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:16 PM ***** LOSS printing *****
06/27 04:43:16 PM loss
06/27 04:43:16 PM tensor(1.9691, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:16 PM ***** LOSS printing *****
06/27 04:43:16 PM loss
06/27 04:43:16 PM tensor(2.2092, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:16 PM ***** LOSS printing *****
06/27 04:43:16 PM loss
06/27 04:43:16 PM tensor(1.1745, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:16 PM ***** Running evaluation MLM *****
06/27 04:43:16 PM   Epoch = 4 iter 59 step
06/27 04:43:16 PM   Num examples = 16
06/27 04:43:16 PM   Batch size = 32
06/27 04:43:17 PM ***** Eval results *****
06/27 04:43:17 PM   acc = 0.9375
06/27 04:43:17 PM   cls_loss = 1.5668168393048374
06/27 04:43:17 PM   eval_loss = 0.8780028820037842
06/27 04:43:17 PM   global_step = 59
06/27 04:43:17 PM   loss = 1.5668168393048374
06/27 04:43:17 PM ***** LOSS printing *****
06/27 04:43:17 PM loss
06/27 04:43:17 PM tensor(1.2816, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:17 PM ***** LOSS printing *****
06/27 04:43:17 PM loss
06/27 04:43:17 PM tensor(1.2274, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:17 PM ***** LOSS printing *****
06/27 04:43:17 PM loss
06/27 04:43:17 PM tensor(1.4052, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:18 PM ***** LOSS printing *****
06/27 04:43:18 PM loss
06/27 04:43:18 PM tensor(1.2791, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:18 PM ***** LOSS printing *****
06/27 04:43:18 PM loss
06/27 04:43:18 PM tensor(2.2983, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:18 PM ***** Running evaluation MLM *****
06/27 04:43:18 PM   Epoch = 5 iter 64 step
06/27 04:43:18 PM   Num examples = 16
06/27 04:43:18 PM   Batch size = 32
06/27 04:43:19 PM ***** Eval results *****
06/27 04:43:19 PM   acc = 0.9375
06/27 04:43:19 PM   cls_loss = 1.5524951815605164
06/27 04:43:19 PM   eval_loss = 0.7794131636619568
06/27 04:43:19 PM   global_step = 64
06/27 04:43:19 PM   loss = 1.5524951815605164
06/27 04:43:19 PM ***** LOSS printing *****
06/27 04:43:19 PM loss
06/27 04:43:19 PM tensor(1.0715, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:19 PM ***** LOSS printing *****
06/27 04:43:19 PM loss
06/27 04:43:19 PM tensor(1.8082, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:19 PM ***** LOSS printing *****
06/27 04:43:19 PM loss
06/27 04:43:19 PM tensor(1.8553, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:19 PM ***** LOSS printing *****
06/27 04:43:19 PM loss
06/27 04:43:19 PM tensor(1.0297, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:19 PM ***** LOSS printing *****
06/27 04:43:19 PM loss
06/27 04:43:19 PM tensor(1.6009, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:20 PM ***** Running evaluation MLM *****
06/27 04:43:20 PM   Epoch = 5 iter 69 step
06/27 04:43:20 PM   Num examples = 16
06/27 04:43:20 PM   Batch size = 32
06/27 04:43:20 PM ***** Eval results *****
06/27 04:43:20 PM   acc = 0.9375
06/27 04:43:20 PM   cls_loss = 1.5083979500664606
06/27 04:43:20 PM   eval_loss = 1.2773737907409668
06/27 04:43:20 PM   global_step = 69
06/27 04:43:20 PM   loss = 1.5083979500664606
06/27 04:43:20 PM ***** LOSS printing *****
06/27 04:43:20 PM loss
06/27 04:43:20 PM tensor(1.9848, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:20 PM ***** LOSS printing *****
06/27 04:43:20 PM loss
06/27 04:43:20 PM tensor(1.1110, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:21 PM ***** LOSS printing *****
06/27 04:43:21 PM loss
06/27 04:43:21 PM tensor(1.5187, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:21 PM ***** LOSS printing *****
06/27 04:43:21 PM loss
06/27 04:43:21 PM tensor(1.2061, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:21 PM ***** LOSS printing *****
06/27 04:43:21 PM loss
06/27 04:43:21 PM tensor(1.2734, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:21 PM ***** Running evaluation MLM *****
06/27 04:43:21 PM   Epoch = 6 iter 74 step
06/27 04:43:21 PM   Num examples = 16
06/27 04:43:21 PM   Batch size = 32
06/27 04:43:22 PM ***** Eval results *****
06/27 04:43:22 PM   acc = 0.9375
06/27 04:43:22 PM   cls_loss = 1.2397680282592773
06/27 04:43:22 PM   eval_loss = 1.6245654821395874
06/27 04:43:22 PM   global_step = 74
06/27 04:43:22 PM   loss = 1.2397680282592773
06/27 04:43:22 PM ***** LOSS printing *****
06/27 04:43:22 PM loss
06/27 04:43:22 PM tensor(0.8919, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:22 PM ***** LOSS printing *****
06/27 04:43:22 PM loss
06/27 04:43:22 PM tensor(1.5294, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:22 PM ***** LOSS printing *****
06/27 04:43:22 PM loss
06/27 04:43:22 PM tensor(1.5132, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:22 PM ***** LOSS printing *****
06/27 04:43:22 PM loss
06/27 04:43:22 PM tensor(1.4851, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:23 PM ***** LOSS printing *****
06/27 04:43:23 PM loss
06/27 04:43:23 PM tensor(1.3594, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:23 PM ***** Running evaluation MLM *****
06/27 04:43:23 PM   Epoch = 6 iter 79 step
06/27 04:43:23 PM   Num examples = 16
06/27 04:43:23 PM   Batch size = 32
06/27 04:43:23 PM ***** Eval results *****
06/27 04:43:23 PM   acc = 1.0
06/27 04:43:23 PM   cls_loss = 1.3226518801280431
06/27 04:43:23 PM   eval_loss = 1.627551794052124
06/27 04:43:23 PM   global_step = 79
06/27 04:43:23 PM   loss = 1.3226518801280431
06/27 04:43:23 PM ***** LOSS printing *****
06/27 04:43:23 PM loss
06/27 04:43:23 PM tensor(0.9436, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:24 PM ***** LOSS printing *****
06/27 04:43:24 PM loss
06/27 04:43:24 PM tensor(1.6315, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:24 PM ***** LOSS printing *****
06/27 04:43:24 PM loss
06/27 04:43:24 PM tensor(1.4466, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:24 PM ***** LOSS printing *****
06/27 04:43:24 PM loss
06/27 04:43:24 PM tensor(1.3076, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:24 PM ***** LOSS printing *****
06/27 04:43:24 PM loss
06/27 04:43:24 PM tensor(1.3767, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:24 PM ***** Running evaluation MLM *****
06/27 04:43:25 PM   Epoch = 6 iter 84 step
06/27 04:43:25 PM   Num examples = 16
06/27 04:43:25 PM   Batch size = 32
06/27 04:43:25 PM ***** Eval results *****
06/27 04:43:25 PM   acc = 1.0
06/27 04:43:25 PM   cls_loss = 1.3303763469060261
06/27 04:43:25 PM   eval_loss = 1.1155329942703247
06/27 04:43:25 PM   global_step = 84
06/27 04:43:25 PM   loss = 1.3303763469060261
06/27 04:43:25 PM ***** LOSS printing *****
06/27 04:43:25 PM loss
06/27 04:43:25 PM tensor(1.0740, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:25 PM ***** LOSS printing *****
06/27 04:43:25 PM loss
06/27 04:43:25 PM tensor(1.2507, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:26 PM ***** LOSS printing *****
06/27 04:43:26 PM loss
06/27 04:43:26 PM tensor(1.3350, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:26 PM ***** LOSS printing *****
06/27 04:43:26 PM loss
06/27 04:43:26 PM tensor(1.2385, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:26 PM ***** LOSS printing *****
06/27 04:43:26 PM loss
06/27 04:43:26 PM tensor(1.2869, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:26 PM ***** Running evaluation MLM *****
06/27 04:43:26 PM   Epoch = 7 iter 89 step
06/27 04:43:26 PM   Num examples = 16
06/27 04:43:26 PM   Batch size = 32
06/27 04:43:27 PM ***** Eval results *****
06/27 04:43:27 PM   acc = 0.875
06/27 04:43:27 PM   cls_loss = 1.237029218673706
06/27 04:43:27 PM   eval_loss = 1.4080030918121338
06/27 04:43:27 PM   global_step = 89
06/27 04:43:27 PM   loss = 1.237029218673706
06/27 04:43:27 PM ***** LOSS printing *****
06/27 04:43:27 PM loss
06/27 04:43:27 PM tensor(1.8534, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:27 PM ***** LOSS printing *****
06/27 04:43:27 PM loss
06/27 04:43:27 PM tensor(1.4029, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:27 PM ***** LOSS printing *****
06/27 04:43:27 PM loss
06/27 04:43:27 PM tensor(1.8060, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:27 PM ***** LOSS printing *****
06/27 04:43:27 PM loss
06/27 04:43:27 PM tensor(0.9947, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:28 PM ***** LOSS printing *****
06/27 04:43:28 PM loss
06/27 04:43:28 PM tensor(1.5181, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:28 PM ***** Running evaluation MLM *****
06/27 04:43:28 PM   Epoch = 7 iter 94 step
06/27 04:43:28 PM   Num examples = 16
06/27 04:43:28 PM   Batch size = 32
06/27 04:43:28 PM ***** Eval results *****
06/27 04:43:28 PM   acc = 1.0
06/27 04:43:28 PM   cls_loss = 1.3760393857955933
06/27 04:43:28 PM   eval_loss = 0.9153909087181091
06/27 04:43:28 PM   global_step = 94
06/27 04:43:28 PM   loss = 1.3760393857955933
06/27 04:43:28 PM ***** LOSS printing *****
06/27 04:43:28 PM loss
06/27 04:43:28 PM tensor(1.2113, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:29 PM ***** LOSS printing *****
06/27 04:43:29 PM loss
06/27 04:43:29 PM tensor(1.7353, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:29 PM ***** LOSS printing *****
06/27 04:43:29 PM loss
06/27 04:43:29 PM tensor(1.4153, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:29 PM ***** LOSS printing *****
06/27 04:43:29 PM loss
06/27 04:43:29 PM tensor(1.1671, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:29 PM ***** LOSS printing *****
06/27 04:43:29 PM loss
06/27 04:43:29 PM tensor(1.5469, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:29 PM ***** Running evaluation MLM *****
06/27 04:43:29 PM   Epoch = 8 iter 99 step
06/27 04:43:29 PM   Num examples = 16
06/27 04:43:29 PM   Batch size = 32
06/27 04:43:30 PM ***** Eval results *****
06/27 04:43:30 PM   acc = 1.0
06/27 04:43:30 PM   cls_loss = 1.3764398495356243
06/27 04:43:30 PM   eval_loss = 0.992892861366272
06/27 04:43:30 PM   global_step = 99
06/27 04:43:30 PM   loss = 1.3764398495356243
06/27 04:43:30 PM ***** LOSS printing *****
06/27 04:43:30 PM loss
06/27 04:43:30 PM tensor(0.7700, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:30 PM ***** LOSS printing *****
06/27 04:43:30 PM loss
06/27 04:43:30 PM tensor(0.9177, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:30 PM ***** LOSS printing *****
06/27 04:43:30 PM loss
06/27 04:43:30 PM tensor(2.2662, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:31 PM ***** LOSS printing *****
06/27 04:43:31 PM loss
06/27 04:43:31 PM tensor(1.4163, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:31 PM ***** LOSS printing *****
06/27 04:43:31 PM loss
06/27 04:43:31 PM tensor(1.4117, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:31 PM ***** Running evaluation MLM *****
06/27 04:43:31 PM   Epoch = 8 iter 104 step
06/27 04:43:31 PM   Num examples = 16
06/27 04:43:31 PM   Batch size = 32
06/27 04:43:31 PM ***** Eval results *****
06/27 04:43:31 PM   acc = 1.0
06/27 04:43:31 PM   cls_loss = 1.3639098033308983
06/27 04:43:31 PM   eval_loss = 1.7416962385177612
06/27 04:43:31 PM   global_step = 104
06/27 04:43:31 PM   loss = 1.3639098033308983
06/27 04:43:32 PM ***** LOSS printing *****
06/27 04:43:32 PM loss
06/27 04:43:32 PM tensor(2.7755, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:32 PM ***** LOSS printing *****
06/27 04:43:32 PM loss
06/27 04:43:32 PM tensor(2.6778, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:32 PM ***** LOSS printing *****
06/27 04:43:32 PM loss
06/27 04:43:32 PM tensor(1.9723, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:32 PM ***** LOSS printing *****
06/27 04:43:32 PM loss
06/27 04:43:32 PM tensor(1.1499, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:32 PM ***** LOSS printing *****
06/27 04:43:32 PM loss
06/27 04:43:32 PM tensor(0.9945, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:33 PM ***** Running evaluation MLM *****
06/27 04:43:33 PM   Epoch = 9 iter 109 step
06/27 04:43:33 PM   Num examples = 16
06/27 04:43:33 PM   Batch size = 32
06/27 04:43:33 PM ***** Eval results *****
06/27 04:43:33 PM   acc = 1.0
06/27 04:43:33 PM   cls_loss = 0.9945170879364014
06/27 04:43:33 PM   eval_loss = 1.284035325050354
06/27 04:43:33 PM   global_step = 109
06/27 04:43:33 PM   loss = 0.9945170879364014
06/27 04:43:33 PM ***** LOSS printing *****
06/27 04:43:33 PM loss
06/27 04:43:33 PM tensor(0.9735, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:33 PM ***** LOSS printing *****
06/27 04:43:33 PM loss
06/27 04:43:33 PM tensor(1.2341, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:34 PM ***** LOSS printing *****
06/27 04:43:34 PM loss
06/27 04:43:34 PM tensor(1.5919, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:34 PM ***** LOSS printing *****
06/27 04:43:34 PM loss
06/27 04:43:34 PM tensor(1.4748, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:34 PM ***** LOSS printing *****
06/27 04:43:34 PM loss
06/27 04:43:34 PM tensor(1.4945, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:34 PM ***** Running evaluation MLM *****
06/27 04:43:34 PM   Epoch = 9 iter 114 step
06/27 04:43:34 PM   Num examples = 16
06/27 04:43:34 PM   Batch size = 32
06/27 04:43:35 PM ***** Eval results *****
06/27 04:43:35 PM   acc = 1.0
06/27 04:43:35 PM   cls_loss = 1.29387828707695
06/27 04:43:35 PM   eval_loss = 1.149654507637024
06/27 04:43:35 PM   global_step = 114
06/27 04:43:35 PM   loss = 1.29387828707695
06/27 04:43:35 PM ***** LOSS printing *****
06/27 04:43:35 PM loss
06/27 04:43:35 PM tensor(1.8253, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:35 PM ***** LOSS printing *****
06/27 04:43:35 PM loss
06/27 04:43:35 PM tensor(1.3618, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:35 PM ***** LOSS printing *****
06/27 04:43:35 PM loss
06/27 04:43:35 PM tensor(1.1174, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:35 PM ***** LOSS printing *****
06/27 04:43:35 PM loss
06/27 04:43:35 PM tensor(1.2241, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:36 PM ***** LOSS printing *****
06/27 04:43:36 PM loss
06/27 04:43:36 PM tensor(1.2774, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 04:43:36 PM ***** Running evaluation MLM *****
06/27 04:43:36 PM   Epoch = 9 iter 119 step
06/27 04:43:36 PM   Num examples = 16
06/27 04:43:36 PM   Batch size = 32
06/27 04:43:36 PM ***** Eval results *****
06/27 04:43:36 PM   acc = 0.9375
06/27 04:43:36 PM   cls_loss = 1.324486716227098
06/27 04:43:36 PM   eval_loss = 1.1300832033157349
06/27 04:43:36 PM   global_step = 119
06/27 04:43:36 PM   loss = 1.324486716227098
06/27 04:43:36 PM ***** LOSS printing *****
06/27 04:43:36 PM loss
06/27 04:43:36 PM tensor(1.3588, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 02:15:10 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/mr/8-100/', data_seed=100, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='mr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 02:15:10 PM device: cuda n_gpu: 1
06/27 02:15:10 PM Writing example 0 of 16
06/27 02:15:10 PM *** Example ***
06/27 02:15:10 PM guid: train-1
06/27 02:15:10 PM tokens: <s> supp osed ly Ġbased Ġupon Ġreal Ġ, Ġor Ġat Ġleast Ġsober ly Ġreported Ġincidents Ġ, Ġthe Ġfilm Ġends Ġwith Ġa Ġlarge Ġhuman Ġtragedy Ġ. Ġalas Ġ, Ġgetting Ġthere Ġis Ġnot Ġeven Ġhalf Ġthe Ġinterest Ġ. </s> ĠIt Ġis <mask>
06/27 02:15:10 PM input_ids: 0 16714 7878 352 716 2115 588 2156 50 23 513 17333 352 431 4495 2156 5 822 3587 19 10 739 1050 6906 479 40463 2156 562 89 16 45 190 457 5 773 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM label: ['Ġnegative']
06/27 02:15:10 PM Writing example 0 of 16
06/27 02:15:10 PM *** Example ***
06/27 02:15:10 PM guid: dev-1
06/27 02:15:10 PM tokens: <s> although Ġbased Ġon Ġa Ġreal - life Ġperson Ġ, Ġjohn Ġ, Ġin Ġthe Ġmovie Ġ, Ġis Ġa Ġrather Ġdull Ġperson Ġto Ġbe Ġstuck Ġwith Ġfor Ġtwo Ġhours Ġ. </s> ĠIt Ġis <mask>
06/27 02:15:10 PM input_ids: 0 24648 716 15 10 588 12 5367 621 2156 41906 2156 11 5 1569 2156 16 10 1195 22018 621 7 28 4889 19 13 80 722 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM label: ['Ġnegative']
06/27 02:15:10 PM Writing example 0 of 2000
06/27 02:15:10 PM *** Example ***
06/27 02:15:10 PM guid: dev-1
06/27 02:15:10 PM tokens: <s> sim pl istic Ġ, Ġsilly Ġand Ġtedious Ġ. </s> ĠIt Ġis <mask>
06/27 02:15:10 PM input_ids: 0 13092 2911 5580 2156 15470 8 35138 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:15:10 PM label: ['Ġnegative']
06/27 02:15:23 PM ***** Running training *****
06/27 02:15:23 PM   Num examples = 16
06/27 02:15:23 PM   Batch size = 4
06/27 02:15:23 PM   Num steps = 40
06/27 02:15:23 PM n: embeddings.word_embeddings.weight
06/27 02:15:23 PM n: embeddings.position_embeddings.weight
06/27 02:15:23 PM n: embeddings.token_type_embeddings.weight
06/27 02:15:23 PM n: embeddings.LayerNorm.weight
06/27 02:15:23 PM n: embeddings.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.0.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.0.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.0.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.0.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.0.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.0.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.0.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.0.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.0.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.0.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.0.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.0.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.1.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.1.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.1.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.1.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.1.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.1.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.1.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.1.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.1.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.1.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.1.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.1.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.2.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.2.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.2.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.2.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.2.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.2.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.2.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.2.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.2.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.2.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.2.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.2.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.3.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.3.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.3.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.3.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.3.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.3.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.3.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.3.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.3.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.3.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.3.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.3.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.4.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.4.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.4.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.4.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.4.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.4.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.4.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.4.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.4.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.4.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.4.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.4.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.5.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.5.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.5.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.5.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.5.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.5.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.5.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.5.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.5.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.5.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.5.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.5.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.6.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.6.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.6.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.6.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.6.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.6.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.6.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.6.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.6.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.6.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.6.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.6.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.7.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.7.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.7.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.7.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.7.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.7.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.7.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.7.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.7.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.7.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.7.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.7.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.8.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.8.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.8.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.8.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.8.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.8.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.8.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.8.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.8.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.8.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.8.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.8.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.9.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.9.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.9.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.9.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.9.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.9.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.9.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.9.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.9.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.9.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.9.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.9.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.10.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.10.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.10.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.10.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.10.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.10.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.10.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.10.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.10.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.10.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.10.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.10.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.11.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.11.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.11.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.11.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.11.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.11.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.11.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.11.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.11.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.11.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.11.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.11.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.12.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.12.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.12.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.12.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.12.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.12.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.12.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.12.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.12.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.12.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.12.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.12.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.13.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.13.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.13.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.13.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.13.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.13.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.13.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.13.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.13.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.13.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.13.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.13.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.14.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.14.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.14.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.14.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.14.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.14.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.14.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.14.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.14.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.14.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.14.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.14.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.15.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.15.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.15.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.15.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.15.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.15.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.15.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.15.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.15.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.15.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.15.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.15.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.16.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.16.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.16.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.16.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.16.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.16.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.16.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.16.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.16.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.16.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.16.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.16.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.17.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.17.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.17.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.17.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.17.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.17.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.17.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.17.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.17.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.17.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.17.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.17.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.18.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.18.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.18.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.18.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.18.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.18.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.18.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.18.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.18.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.18.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.18.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.18.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.19.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.19.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.19.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.19.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.19.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.19.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.19.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.19.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.19.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.19.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.19.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.19.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.20.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.20.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.20.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.20.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.20.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.20.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.20.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.20.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.20.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.20.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.20.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.20.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.21.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.21.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.21.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.21.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.21.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.21.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.21.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.21.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.21.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.21.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.21.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.21.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.22.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.22.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.22.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.22.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.22.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.22.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.22.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.22.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.22.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.22.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.22.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.22.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.23.attention.self.query.weight
06/27 02:15:23 PM n: encoder.layer.23.attention.self.query.bias
06/27 02:15:23 PM n: encoder.layer.23.attention.self.key.weight
06/27 02:15:23 PM n: encoder.layer.23.attention.self.key.bias
06/27 02:15:23 PM n: encoder.layer.23.attention.self.value.weight
06/27 02:15:23 PM n: encoder.layer.23.attention.self.value.bias
06/27 02:15:23 PM n: encoder.layer.23.attention.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.23.attention.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: encoder.layer.23.intermediate.dense.weight
06/27 02:15:23 PM n: encoder.layer.23.intermediate.dense.bias
06/27 02:15:23 PM n: encoder.layer.23.output.dense.weight
06/27 02:15:23 PM n: encoder.layer.23.output.dense.bias
06/27 02:15:23 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 02:15:23 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 02:15:23 PM n: pooler.dense.weight
06/27 02:15:23 PM n: pooler.dense.bias
06/27 02:15:23 PM n: roberta.embeddings.word_embeddings.weight
06/27 02:15:23 PM n: roberta.embeddings.position_embeddings.weight
06/27 02:15:23 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 02:15:23 PM n: roberta.embeddings.LayerNorm.weight
06/27 02:15:23 PM n: roberta.embeddings.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 02:15:23 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 02:15:23 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 02:15:23 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 02:15:23 PM n: roberta.pooler.dense.weight
06/27 02:15:23 PM n: roberta.pooler.dense.bias
06/27 02:15:23 PM n: lm_head.bias
06/27 02:15:23 PM n: lm_head.dense.weight
06/27 02:15:23 PM n: lm_head.dense.bias
06/27 02:15:23 PM n: lm_head.layer_norm.weight
06/27 02:15:23 PM n: lm_head.layer_norm.bias
06/27 02:15:23 PM n: lm_head.decoder.weight
06/27 02:15:23 PM Total parameters: 763292761
06/27 02:15:23 PM ***** LOSS printing *****
06/27 02:15:23 PM loss
06/27 02:15:23 PM tensor(20.3376, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:23 PM ***** LOSS printing *****
06/27 02:15:23 PM loss
06/27 02:15:23 PM tensor(14.5716, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:24 PM ***** LOSS printing *****
06/27 02:15:24 PM loss
06/27 02:15:24 PM tensor(6.1692, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:24 PM ***** LOSS printing *****
06/27 02:15:24 PM loss
06/27 02:15:24 PM tensor(2.5371, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:24 PM ***** Running evaluation MLM *****
06/27 02:15:24 PM   Epoch = 0 iter 4 step
06/27 02:15:24 PM   Num examples = 16
06/27 02:15:24 PM   Batch size = 32
06/27 02:15:25 PM ***** Eval results *****
06/27 02:15:25 PM   acc = 0.75
06/27 02:15:25 PM   cls_loss = 10.903878033161163
06/27 02:15:25 PM   eval_loss = 0.5974915027618408
06/27 02:15:25 PM   global_step = 4
06/27 02:15:25 PM   loss = 10.903878033161163
06/27 02:15:25 PM ***** Save model *****
06/27 02:15:25 PM ***** Test Dataset Eval Result *****
06/27 02:16:28 PM ***** Eval results *****
06/27 02:16:28 PM   acc = 0.691
06/27 02:16:28 PM   cls_loss = 10.903878033161163
06/27 02:16:28 PM   eval_loss = 0.6405805481804742
06/27 02:16:28 PM   global_step = 4
06/27 02:16:28 PM   loss = 10.903878033161163
06/27 02:16:32 PM ***** LOSS printing *****
06/27 02:16:32 PM loss
06/27 02:16:32 PM tensor(0.6726, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:33 PM ***** LOSS printing *****
06/27 02:16:33 PM loss
06/27 02:16:33 PM tensor(0.8853, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:33 PM ***** LOSS printing *****
06/27 02:16:33 PM loss
06/27 02:16:33 PM tensor(1.0558, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:33 PM ***** LOSS printing *****
06/27 02:16:33 PM loss
06/27 02:16:33 PM tensor(0.4324, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:33 PM ***** LOSS printing *****
06/27 02:16:33 PM loss
06/27 02:16:33 PM tensor(0.1417, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:33 PM ***** Running evaluation MLM *****
06/27 02:16:33 PM   Epoch = 2 iter 9 step
06/27 02:16:33 PM   Num examples = 16
06/27 02:16:33 PM   Batch size = 32
06/27 02:16:34 PM ***** Eval results *****
06/27 02:16:34 PM   acc = 0.5
06/27 02:16:34 PM   cls_loss = 0.1417498141527176
06/27 02:16:34 PM   eval_loss = 1.6720832586288452
06/27 02:16:34 PM   global_step = 9
06/27 02:16:34 PM   loss = 0.1417498141527176
06/27 02:16:34 PM ***** LOSS printing *****
06/27 02:16:34 PM loss
06/27 02:16:34 PM tensor(1.7318, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:34 PM ***** LOSS printing *****
06/27 02:16:34 PM loss
06/27 02:16:34 PM tensor(3.5883, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:34 PM ***** LOSS printing *****
06/27 02:16:34 PM loss
06/27 02:16:34 PM tensor(1.6629, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:35 PM ***** LOSS printing *****
06/27 02:16:35 PM loss
06/27 02:16:35 PM tensor(0.3765, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:35 PM ***** LOSS printing *****
06/27 02:16:35 PM loss
06/27 02:16:35 PM tensor(0.0993, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:16:35 PM ***** Running evaluation MLM *****
06/27 02:16:35 PM   Epoch = 3 iter 14 step
06/27 02:16:35 PM   Num examples = 16
06/27 02:16:35 PM   Batch size = 32
06/27 02:16:36 PM ***** Eval results *****
06/27 02:16:36 PM   acc = 0.8125
06/27 02:16:36 PM   cls_loss = 0.23791056126356125
06/27 02:16:36 PM   eval_loss = 0.4849662184715271
06/27 02:16:36 PM   global_step = 14
06/27 02:16:36 PM   loss = 0.23791056126356125
06/27 02:16:36 PM ***** Save model *****
06/27 02:16:36 PM ***** Test Dataset Eval Result *****
06/27 02:17:39 PM ***** Eval results *****
06/27 02:17:39 PM   acc = 0.689
06/27 02:17:39 PM   cls_loss = 0.23791056126356125
06/27 02:17:39 PM   eval_loss = 1.105610875464562
06/27 02:17:39 PM   global_step = 14
06/27 02:17:39 PM   loss = 0.23791056126356125
06/27 02:17:43 PM ***** LOSS printing *****
06/27 02:17:43 PM loss
06/27 02:17:43 PM tensor(0.9066, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:17:43 PM ***** LOSS printing *****
06/27 02:17:43 PM loss
06/27 02:17:43 PM tensor(0.8926, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:17:44 PM ***** LOSS printing *****
06/27 02:17:44 PM loss
06/27 02:17:44 PM tensor(0.3671, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:17:44 PM ***** LOSS printing *****
06/27 02:17:44 PM loss
06/27 02:17:44 PM tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:17:44 PM ***** LOSS printing *****
06/27 02:17:44 PM loss
06/27 02:17:44 PM tensor(0.0059, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:17:44 PM ***** Running evaluation MLM *****
06/27 02:17:44 PM   Epoch = 4 iter 19 step
06/27 02:17:44 PM   Num examples = 16
06/27 02:17:44 PM   Batch size = 32
06/27 02:17:45 PM ***** Eval results *****
06/27 02:17:45 PM   acc = 1.0
06/27 02:17:45 PM   cls_loss = 0.12566161310921112
06/27 02:17:45 PM   eval_loss = 0.04220551624894142
06/27 02:17:45 PM   global_step = 19
06/27 02:17:45 PM   loss = 0.12566161310921112
06/27 02:17:45 PM ***** Save model *****
06/27 02:17:45 PM ***** Test Dataset Eval Result *****
06/27 02:18:48 PM ***** Eval results *****
06/27 02:18:48 PM   acc = 0.816
06/27 02:18:48 PM   cls_loss = 0.12566161310921112
06/27 02:18:48 PM   eval_loss = 0.6646237195957274
06/27 02:18:48 PM   global_step = 19
06/27 02:18:48 PM   loss = 0.12566161310921112
06/27 02:18:52 PM ***** LOSS printing *****
06/27 02:18:52 PM loss
06/27 02:18:52 PM tensor(0.0043, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:52 PM ***** LOSS printing *****
06/27 02:18:52 PM loss
06/27 02:18:52 PM tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:53 PM ***** LOSS printing *****
06/27 02:18:53 PM loss
06/27 02:18:53 PM tensor(0.0041, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:53 PM ***** LOSS printing *****
06/27 02:18:53 PM loss
06/27 02:18:53 PM tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:53 PM ***** LOSS printing *****
06/27 02:18:53 PM loss
06/27 02:18:53 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:53 PM ***** Running evaluation MLM *****
06/27 02:18:53 PM   Epoch = 5 iter 24 step
06/27 02:18:53 PM   Num examples = 16
06/27 02:18:53 PM   Batch size = 32
06/27 02:18:54 PM ***** Eval results *****
06/27 02:18:54 PM   acc = 0.75
06/27 02:18:54 PM   cls_loss = 0.0039059377886587754
06/27 02:18:54 PM   eval_loss = 1.1953216791152954
06/27 02:18:54 PM   global_step = 24
06/27 02:18:54 PM   loss = 0.0039059377886587754
06/27 02:18:54 PM ***** LOSS printing *****
06/27 02:18:54 PM loss
06/27 02:18:54 PM tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:54 PM ***** LOSS printing *****
06/27 02:18:54 PM loss
06/27 02:18:54 PM tensor(1.1788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:54 PM ***** LOSS printing *****
06/27 02:18:54 PM loss
06/27 02:18:54 PM tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:54 PM ***** LOSS printing *****
06/27 02:18:54 PM loss
06/27 02:18:54 PM tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:55 PM ***** LOSS printing *****
06/27 02:18:55 PM loss
06/27 02:18:55 PM tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:55 PM ***** Running evaluation MLM *****
06/27 02:18:55 PM   Epoch = 7 iter 29 step
06/27 02:18:55 PM   Num examples = 16
06/27 02:18:55 PM   Batch size = 32
06/27 02:18:55 PM ***** Eval results *****
06/27 02:18:55 PM   acc = 1.0
06/27 02:18:55 PM   cls_loss = 0.0006776873487979174
06/27 02:18:55 PM   eval_loss = 0.04699360579252243
06/27 02:18:55 PM   global_step = 29
06/27 02:18:55 PM   loss = 0.0006776873487979174
06/27 02:18:55 PM ***** LOSS printing *****
06/27 02:18:55 PM loss
06/27 02:18:55 PM tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:56 PM ***** LOSS printing *****
06/27 02:18:56 PM loss
06/27 02:18:56 PM tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:56 PM ***** LOSS printing *****
06/27 02:18:56 PM loss
06/27 02:18:56 PM tensor(0.0019, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:56 PM ***** LOSS printing *****
06/27 02:18:56 PM loss
06/27 02:18:56 PM tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:56 PM ***** LOSS printing *****
06/27 02:18:56 PM loss
06/27 02:18:56 PM tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:56 PM ***** Running evaluation MLM *****
06/27 02:18:56 PM   Epoch = 8 iter 34 step
06/27 02:18:56 PM   Num examples = 16
06/27 02:18:56 PM   Batch size = 32
06/27 02:18:57 PM ***** Eval results *****
06/27 02:18:57 PM   acc = 0.75
06/27 02:18:57 PM   cls_loss = 0.0006328395829768851
06/27 02:18:57 PM   eval_loss = 0.9455797672271729
06/27 02:18:57 PM   global_step = 34
06/27 02:18:57 PM   loss = 0.0006328395829768851
06/27 02:18:57 PM ***** LOSS printing *****
06/27 02:18:57 PM loss
06/27 02:18:57 PM tensor(0.0055, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:57 PM ***** LOSS printing *****
06/27 02:18:57 PM loss
06/27 02:18:57 PM tensor(0.0069, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:58 PM ***** LOSS printing *****
06/27 02:18:58 PM loss
06/27 02:18:58 PM tensor(0.0022, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:58 PM ***** LOSS printing *****
06/27 02:18:58 PM loss
06/27 02:18:58 PM tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:58 PM ***** LOSS printing *****
06/27 02:18:58 PM loss
06/27 02:18:58 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:18:58 PM ***** Running evaluation MLM *****
06/27 02:18:58 PM   Epoch = 9 iter 39 step
06/27 02:18:58 PM   Num examples = 16
06/27 02:18:58 PM   Batch size = 32
06/27 02:18:59 PM ***** Eval results *****
06/27 02:18:59 PM   acc = 0.9375
06/27 02:18:59 PM   cls_loss = 0.0009957206493709236
06/27 02:18:59 PM   eval_loss = 0.3976723849773407
06/27 02:18:59 PM   global_step = 39
06/27 02:18:59 PM   loss = 0.0009957206493709236
06/27 02:18:59 PM ***** LOSS printing *****
06/27 02:18:59 PM loss
06/27 02:18:59 PM tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 01:34:42 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-87/', data_seed=87, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 01:34:42 PM device: cuda n_gpu: 1
06/27 01:34:42 PM Writing example 0 of 16
06/27 01:34:42 PM *** Example ***
06/27 01:34:42 PM guid: train-1
06/27 01:34:42 PM tokens: <s> the Ġpace Ġof Ġthe Ġfilm Ġis Ġvery Ġslow Ġ( Ġfor Ġobvious Ġreasons Ġ) Ġand Ġthat Ġtoo Ġbecomes Ġoff - put ting Ġ. </s> ĠIt Ġis <mask>
06/27 01:34:42 PM input_ids: 0 627 2877 9 5 822 16 182 2635 36 13 4678 2188 4839 8 14 350 3374 160 12 9179 2577 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:42 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:42 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:42 PM label: ['Ġindividual']
06/27 01:34:42 PM Writing example 0 of 16
06/27 01:34:42 PM *** Example ***
06/27 01:34:42 PM guid: dev-1
06/27 01:34:42 PM tokens: <s> what Ġcould Ġhave Ġbeen Ġa Ġpointed Ġlittle Ġch iller Ġabout Ġthe Ġfrightening Ġsed uct iveness Ġof Ġnew Ġtechnology Ġloses Ġfaith Ġin Ġits Ġown Ġviability Ġand Ġsucc umbs Ġto Ġjoy less Ġspecial - effects Ġexcess Ġ. </s> ĠIt Ġis <mask>
06/27 01:34:42 PM input_ids: 0 12196 115 33 57 10 3273 410 1855 8690 59 5 21111 10195 21491 12367 9 92 806 13585 3975 11 63 308 23990 8 25047 29123 7 5823 1672 780 12 38375 7400 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:42 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:42 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:42 PM label: ['Ġindividual']
06/27 01:34:43 PM Writing example 0 of 2000
06/27 01:34:43 PM *** Example ***
06/27 01:34:43 PM guid: dev-1
06/27 01:34:43 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 01:34:43 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:43 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:43 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:34:43 PM label: ['Ġindividual']
06/27 01:34:57 PM ***** Running training *****
06/27 01:34:57 PM   Num examples = 16
06/27 01:34:57 PM   Batch size = 4
06/27 01:34:57 PM   Num steps = 40
06/27 01:34:57 PM n: embeddings.word_embeddings.weight
06/27 01:34:57 PM n: embeddings.position_embeddings.weight
06/27 01:34:57 PM n: embeddings.token_type_embeddings.weight
06/27 01:34:57 PM n: embeddings.LayerNorm.weight
06/27 01:34:57 PM n: embeddings.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.0.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.0.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.0.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.0.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.0.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.0.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.0.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.0.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.0.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.0.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.0.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.0.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.1.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.1.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.1.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.1.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.1.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.1.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.1.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.1.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.1.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.1.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.1.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.1.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.2.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.2.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.2.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.2.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.2.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.2.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.2.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.2.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.2.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.2.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.2.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.2.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.3.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.3.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.3.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.3.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.3.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.3.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.3.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.3.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.3.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.3.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.3.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.3.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.4.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.4.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.4.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.4.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.4.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.4.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.4.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.4.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.4.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.4.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.4.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.4.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.5.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.5.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.5.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.5.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.5.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.5.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.5.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.5.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.5.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.5.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.5.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.5.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.6.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.6.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.6.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.6.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.6.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.6.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.6.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.6.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.6.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.6.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.6.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.6.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.7.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.7.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.7.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.7.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.7.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.7.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.7.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.7.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.7.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.7.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.7.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.7.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.8.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.8.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.8.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.8.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.8.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.8.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.8.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.8.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.8.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.8.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.8.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.8.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.9.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.9.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.9.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.9.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.9.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.9.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.9.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.9.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.9.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.9.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.9.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.9.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.10.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.10.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.10.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.10.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.10.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.10.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.10.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.10.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.10.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.10.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.10.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.10.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.11.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.11.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.11.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.11.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.11.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.11.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.11.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.11.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.11.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.11.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.11.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.11.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.12.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.12.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.12.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.12.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.12.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.12.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.12.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.12.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.12.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.12.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.12.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.12.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.13.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.13.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.13.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.13.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.13.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.13.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.13.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.13.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.13.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.13.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.13.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.13.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.14.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.14.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.14.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.14.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.14.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.14.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.14.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.14.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.14.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.14.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.14.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.14.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.15.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.15.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.15.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.15.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.15.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.15.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.15.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.15.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.15.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.15.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.15.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.15.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.16.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.16.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.16.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.16.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.16.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.16.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.16.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.16.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.16.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.16.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.16.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.16.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.17.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.17.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.17.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.17.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.17.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.17.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.17.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.17.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.17.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.17.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.17.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.17.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.18.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.18.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.18.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.18.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.18.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.18.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.18.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.18.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.18.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.18.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.18.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.18.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.19.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.19.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.19.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.19.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.19.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.19.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.19.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.19.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.19.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.19.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.19.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.19.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.20.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.20.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.20.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.20.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.20.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.20.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.20.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.20.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.20.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.20.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.20.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.20.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.21.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.21.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.21.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.21.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.21.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.21.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.21.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.21.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.21.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.21.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.21.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.21.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.22.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.22.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.22.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.22.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.22.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.22.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.22.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.22.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.22.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.22.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.22.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.22.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.23.attention.self.query.weight
06/27 01:34:57 PM n: encoder.layer.23.attention.self.query.bias
06/27 01:34:57 PM n: encoder.layer.23.attention.self.key.weight
06/27 01:34:57 PM n: encoder.layer.23.attention.self.key.bias
06/27 01:34:57 PM n: encoder.layer.23.attention.self.value.weight
06/27 01:34:57 PM n: encoder.layer.23.attention.self.value.bias
06/27 01:34:57 PM n: encoder.layer.23.attention.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.23.attention.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: encoder.layer.23.intermediate.dense.weight
06/27 01:34:57 PM n: encoder.layer.23.intermediate.dense.bias
06/27 01:34:57 PM n: encoder.layer.23.output.dense.weight
06/27 01:34:57 PM n: encoder.layer.23.output.dense.bias
06/27 01:34:57 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 01:34:57 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 01:34:57 PM n: pooler.dense.weight
06/27 01:34:57 PM n: pooler.dense.bias
06/27 01:34:57 PM n: roberta.embeddings.word_embeddings.weight
06/27 01:34:57 PM n: roberta.embeddings.position_embeddings.weight
06/27 01:34:57 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 01:34:57 PM n: roberta.embeddings.LayerNorm.weight
06/27 01:34:57 PM n: roberta.embeddings.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 01:34:57 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 01:34:57 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 01:34:57 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 01:34:57 PM n: roberta.pooler.dense.weight
06/27 01:34:57 PM n: roberta.pooler.dense.bias
06/27 01:34:57 PM n: lm_head.bias
06/27 01:34:57 PM n: lm_head.dense.weight
06/27 01:34:57 PM n: lm_head.dense.bias
06/27 01:34:57 PM n: lm_head.layer_norm.weight
06/27 01:34:57 PM n: lm_head.layer_norm.bias
06/27 01:34:57 PM n: lm_head.decoder.weight
06/27 01:34:57 PM Total parameters: 763292761
06/27 01:34:57 PM ***** LOSS printing *****
06/27 01:34:57 PM loss
06/27 01:34:57 PM tensor(17.9792, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:34:57 PM ***** LOSS printing *****
06/27 01:34:57 PM loss
06/27 01:34:57 PM tensor(17.2194, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:34:57 PM ***** LOSS printing *****
06/27 01:34:57 PM loss
06/27 01:34:57 PM tensor(13.0030, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:34:58 PM ***** LOSS printing *****
06/27 01:34:58 PM loss
06/27 01:34:58 PM tensor(7.1001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:34:58 PM ***** Running evaluation MLM *****
06/27 01:34:58 PM   Epoch = 0 iter 4 step
06/27 01:34:58 PM   Num examples = 16
06/27 01:34:58 PM   Batch size = 32
06/27 01:34:58 PM ***** Eval results *****
06/27 01:34:58 PM   acc = 0.5
06/27 01:34:58 PM   cls_loss = 13.82542896270752
06/27 01:34:58 PM   eval_loss = 4.056195259094238
06/27 01:34:58 PM   global_step = 4
06/27 01:34:58 PM   loss = 13.82542896270752
06/27 01:34:58 PM ***** Save model *****
06/27 01:34:58 PM ***** Test Dataset Eval Result *****
06/27 01:36:03 PM ***** Eval results *****
06/27 01:36:03 PM   acc = 0.5035
06/27 01:36:03 PM   cls_loss = 13.82542896270752
06/27 01:36:03 PM   eval_loss = 4.062705157295106
06/27 01:36:03 PM   global_step = 4
06/27 01:36:03 PM   loss = 13.82542896270752
06/27 01:36:07 PM ***** LOSS printing *****
06/27 01:36:07 PM loss
06/27 01:36:07 PM tensor(4.1760, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:07 PM ***** LOSS printing *****
06/27 01:36:07 PM loss
06/27 01:36:07 PM tensor(1.0079, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:07 PM ***** LOSS printing *****
06/27 01:36:07 PM loss
06/27 01:36:07 PM tensor(1.9464, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:08 PM ***** LOSS printing *****
06/27 01:36:08 PM loss
06/27 01:36:08 PM tensor(1.5387, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:08 PM ***** LOSS printing *****
06/27 01:36:08 PM loss
06/27 01:36:08 PM tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:08 PM ***** Running evaluation MLM *****
06/27 01:36:08 PM   Epoch = 2 iter 9 step
06/27 01:36:08 PM   Num examples = 16
06/27 01:36:08 PM   Batch size = 32
06/27 01:36:09 PM ***** Eval results *****
06/27 01:36:09 PM   acc = 0.5
06/27 01:36:09 PM   cls_loss = 0.12039640545845032
06/27 01:36:09 PM   eval_loss = 3.1347639560699463
06/27 01:36:09 PM   global_step = 9
06/27 01:36:09 PM   loss = 0.12039640545845032
06/27 01:36:09 PM ***** LOSS printing *****
06/27 01:36:09 PM loss
06/27 01:36:09 PM tensor(2.5254, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:09 PM ***** LOSS printing *****
06/27 01:36:09 PM loss
06/27 01:36:09 PM tensor(7.4106, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:09 PM ***** LOSS printing *****
06/27 01:36:09 PM loss
06/27 01:36:09 PM tensor(3.0484, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:09 PM ***** LOSS printing *****
06/27 01:36:09 PM loss
06/27 01:36:09 PM tensor(1.3351, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:09 PM ***** LOSS printing *****
06/27 01:36:09 PM loss
06/27 01:36:09 PM tensor(0.6542, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:10 PM ***** Running evaluation MLM *****
06/27 01:36:10 PM   Epoch = 3 iter 14 step
06/27 01:36:10 PM   Num examples = 16
06/27 01:36:10 PM   Batch size = 32
06/27 01:36:10 PM ***** Eval results *****
06/27 01:36:10 PM   acc = 0.5
06/27 01:36:10 PM   cls_loss = 0.9946580231189728
06/27 01:36:10 PM   eval_loss = 0.7554664611816406
06/27 01:36:10 PM   global_step = 14
06/27 01:36:10 PM   loss = 0.9946580231189728
06/27 01:36:10 PM ***** LOSS printing *****
06/27 01:36:10 PM loss
06/27 01:36:10 PM tensor(0.2128, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:10 PM ***** LOSS printing *****
06/27 01:36:10 PM loss
06/27 01:36:10 PM tensor(1.8566, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:11 PM ***** LOSS printing *****
06/27 01:36:11 PM loss
06/27 01:36:11 PM tensor(1.3582, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:11 PM ***** LOSS printing *****
06/27 01:36:11 PM loss
06/27 01:36:11 PM tensor(0.8149, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:11 PM ***** LOSS printing *****
06/27 01:36:11 PM loss
06/27 01:36:11 PM tensor(0.5990, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:36:11 PM ***** Running evaluation MLM *****
06/27 01:36:11 PM   Epoch = 4 iter 19 step
06/27 01:36:11 PM   Num examples = 16
06/27 01:36:11 PM   Batch size = 32
06/27 01:36:12 PM ***** Eval results *****
06/27 01:36:12 PM   acc = 0.875
06/27 01:36:12 PM   cls_loss = 0.9240495165189108
06/27 01:36:12 PM   eval_loss = 0.42464277148246765
06/27 01:36:12 PM   global_step = 19
06/27 01:36:12 PM   loss = 0.9240495165189108
06/27 01:36:12 PM ***** Save model *****
06/27 01:36:12 PM ***** Test Dataset Eval Result *****
06/27 01:37:16 PM ***** Eval results *****
06/27 01:37:16 PM   acc = 0.8015
06/27 01:37:16 PM   cls_loss = 0.9240495165189108
06/27 01:37:16 PM   eval_loss = 0.5040432895932879
06/27 01:37:16 PM   global_step = 19
06/27 01:37:16 PM   loss = 0.9240495165189108
06/27 01:37:20 PM ***** LOSS printing *****
06/27 01:37:20 PM loss
06/27 01:37:20 PM tensor(0.3522, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:37:21 PM ***** LOSS printing *****
06/27 01:37:21 PM loss
06/27 01:37:21 PM tensor(0.3593, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:37:21 PM ***** LOSS printing *****
06/27 01:37:21 PM loss
06/27 01:37:21 PM tensor(0.3255, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:37:21 PM ***** LOSS printing *****
06/27 01:37:21 PM loss
06/27 01:37:21 PM tensor(0.1670, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:37:21 PM ***** LOSS printing *****
06/27 01:37:21 PM loss
06/27 01:37:21 PM tensor(0.0741, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:37:22 PM ***** Running evaluation MLM *****
06/27 01:37:22 PM   Epoch = 5 iter 24 step
06/27 01:37:22 PM   Num examples = 16
06/27 01:37:22 PM   Batch size = 32
06/27 01:37:22 PM ***** Eval results *****
06/27 01:37:22 PM   acc = 0.9375
06/27 01:37:22 PM   cls_loss = 0.2315024845302105
06/27 01:37:22 PM   eval_loss = 0.24823181331157684
06/27 01:37:22 PM   global_step = 24
06/27 01:37:22 PM   loss = 0.2315024845302105
06/27 01:37:22 PM ***** Save model *****
06/27 01:37:22 PM ***** Test Dataset Eval Result *****
06/27 01:38:26 PM ***** Eval results *****
06/27 01:38:26 PM   acc = 0.784
06/27 01:38:26 PM   cls_loss = 0.2315024845302105
06/27 01:38:26 PM   eval_loss = 0.4608290966541048
06/27 01:38:26 PM   global_step = 24
06/27 01:38:26 PM   loss = 0.2315024845302105
06/27 01:38:30 PM ***** LOSS printing *****
06/27 01:38:30 PM loss
06/27 01:38:30 PM tensor(0.0290, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:30 PM ***** LOSS printing *****
06/27 01:38:30 PM loss
06/27 01:38:30 PM tensor(0.1011, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:30 PM ***** LOSS printing *****
06/27 01:38:30 PM loss
06/27 01:38:30 PM tensor(0.1010, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:30 PM ***** LOSS printing *****
06/27 01:38:30 PM loss
06/27 01:38:30 PM tensor(0.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:31 PM ***** LOSS printing *****
06/27 01:38:31 PM loss
06/27 01:38:31 PM tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:31 PM ***** Running evaluation MLM *****
06/27 01:38:31 PM   Epoch = 7 iter 29 step
06/27 01:38:31 PM   Num examples = 16
06/27 01:38:31 PM   Batch size = 32
06/27 01:38:31 PM ***** Eval results *****
06/27 01:38:31 PM   acc = 0.8125
06/27 01:38:31 PM   cls_loss = 0.0024585993960499763
06/27 01:38:31 PM   eval_loss = 0.3933962881565094
06/27 01:38:31 PM   global_step = 29
06/27 01:38:31 PM   loss = 0.0024585993960499763
06/27 01:38:31 PM ***** LOSS printing *****
06/27 01:38:31 PM loss
06/27 01:38:31 PM tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:31 PM ***** LOSS printing *****
06/27 01:38:31 PM loss
06/27 01:38:31 PM tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:32 PM ***** LOSS printing *****
06/27 01:38:32 PM loss
06/27 01:38:32 PM tensor(0.0798, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:32 PM ***** LOSS printing *****
06/27 01:38:32 PM loss
06/27 01:38:32 PM tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:32 PM ***** LOSS printing *****
06/27 01:38:32 PM loss
06/27 01:38:32 PM tensor(0.0086, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:32 PM ***** Running evaluation MLM *****
06/27 01:38:32 PM   Epoch = 8 iter 34 step
06/27 01:38:32 PM   Num examples = 16
06/27 01:38:32 PM   Batch size = 32
06/27 01:38:33 PM ***** Eval results *****
06/27 01:38:33 PM   acc = 0.75
06/27 01:38:33 PM   cls_loss = 0.004960531950928271
06/27 01:38:33 PM   eval_loss = 1.6356747150421143
06/27 01:38:33 PM   global_step = 34
06/27 01:38:33 PM   loss = 0.004960531950928271
06/27 01:38:33 PM ***** LOSS printing *****
06/27 01:38:33 PM loss
06/27 01:38:33 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:33 PM ***** LOSS printing *****
06/27 01:38:33 PM loss
06/27 01:38:33 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:33 PM ***** LOSS printing *****
06/27 01:38:33 PM loss
06/27 01:38:33 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:34 PM ***** LOSS printing *****
06/27 01:38:34 PM loss
06/27 01:38:34 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:34 PM ***** LOSS printing *****
06/27 01:38:34 PM loss
06/27 01:38:34 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:38:34 PM ***** Running evaluation MLM *****
06/27 01:38:34 PM   Epoch = 9 iter 39 step
06/27 01:38:34 PM   Num examples = 16
06/27 01:38:34 PM   Batch size = 32
06/27 01:38:34 PM ***** Eval results *****
06/27 01:38:34 PM   acc = 0.75
06/27 01:38:34 PM   cls_loss = 0.0002813540049828589
06/27 01:38:34 PM   eval_loss = 1.8800313472747803
06/27 01:38:34 PM   global_step = 39
06/27 01:38:34 PM   loss = 0.0002813540049828589
06/27 01:38:34 PM ***** LOSS printing *****
06/27 01:38:34 PM loss
06/27 01:38:34 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)

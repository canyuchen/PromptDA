06/27 01:47:26 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-13/', data_seed=13, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 01:47:26 PM device: cuda n_gpu: 1
06/27 01:47:26 PM Writing example 0 of 16
06/27 01:47:26 PM *** Example ***
06/27 01:47:26 PM guid: train-1
06/27 01:47:26 PM tokens: <s> this Ġis Ġthe Ġstory Ġof Ġnat Ġbanks Ġ, Ġan Ġ8 th Ġgeneration Ġvirgin ian Ġgentleman Ġfarmer Ġliving Ġin Ġthe Ġpast Ġ, Ġwho Ġloses Ġhis Ġfamily Ġfarm Ġ, Ġgreen wood Ġ, Ġto Ġa Ġpair Ġof Ġland Ġspec ulators Ġfrom Ġwashing ton Ġ, Ġd Ġ. Ġc Ġ. </s> ĠIt Ġis <mask>
06/27 01:47:26 PM input_ids: 0 9226 16 5 527 9 23577 1520 2156 41 290 212 2706 33799 811 22164 10305 1207 11 5 375 2156 54 13585 39 284 3380 2156 2272 1845 2156 7 10 1763 9 1212 12002 17810 31 14784 1054 2156 385 479 740 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:26 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:26 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:26 PM label: ['Ġactual']
06/27 01:47:26 PM Writing example 0 of 16
06/27 01:47:26 PM *** Example ***
06/27 01:47:26 PM guid: dev-1
06/27 01:47:26 PM tokens: <s> he Ġis Ġstill Ġfamous Ġ, Ġalthough Ġstill Ġdisliked Ġby Ġsn ape Ġ, Ġmalf oy Ġ, Ġand Ġthe Ġrest Ġof Ġthe Ġsly ther ins Ġ. </s> ĠIt Ġis <mask>
06/27 01:47:26 PM input_ids: 0 700 16 202 3395 2156 1712 202 40891 30 4543 5776 2156 36432 2160 2156 8 5 1079 9 5 40568 12968 1344 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:26 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:26 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:26 PM label: ['Ġactual']
06/27 01:47:27 PM Writing example 0 of 2000
06/27 01:47:27 PM *** Example ***
06/27 01:47:27 PM guid: dev-1
06/27 01:47:27 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 01:47:27 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:27 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:27 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:47:27 PM label: ['Ġindividual']
06/27 01:47:40 PM ***** Running training *****
06/27 01:47:40 PM   Num examples = 16
06/27 01:47:40 PM   Batch size = 4
06/27 01:47:40 PM   Num steps = 40
06/27 01:47:40 PM n: embeddings.word_embeddings.weight
06/27 01:47:40 PM n: embeddings.position_embeddings.weight
06/27 01:47:40 PM n: embeddings.token_type_embeddings.weight
06/27 01:47:40 PM n: embeddings.LayerNorm.weight
06/27 01:47:40 PM n: embeddings.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.0.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.0.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.0.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.0.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.0.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.0.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.0.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.0.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.0.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.0.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.0.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.0.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.1.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.1.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.1.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.1.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.1.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.1.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.1.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.1.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.1.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.1.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.1.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.1.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.2.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.2.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.2.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.2.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.2.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.2.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.2.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.2.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.2.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.2.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.2.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.2.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.3.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.3.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.3.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.3.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.3.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.3.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.3.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.3.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.3.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.3.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.3.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.3.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.4.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.4.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.4.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.4.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.4.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.4.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.4.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.4.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.4.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.4.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.4.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.4.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.5.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.5.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.5.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.5.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.5.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.5.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.5.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.5.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.5.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.5.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.5.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.5.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.6.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.6.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.6.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.6.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.6.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.6.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.6.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.6.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.6.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.6.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.6.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.6.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.7.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.7.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.7.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.7.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.7.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.7.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.7.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.7.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.7.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.7.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.7.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.7.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.8.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.8.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.8.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.8.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.8.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.8.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.8.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.8.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.8.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.8.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.8.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.8.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.9.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.9.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.9.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.9.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.9.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.9.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.9.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.9.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.9.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.9.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.9.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.9.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.10.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.10.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.10.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.10.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.10.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.10.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.10.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.10.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.10.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.10.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.10.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.10.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.11.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.11.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.11.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.11.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.11.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.11.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.11.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.11.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.11.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.11.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.11.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.11.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.12.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.12.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.12.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.12.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.12.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.12.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.12.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.12.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.12.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.12.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.12.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.12.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.13.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.13.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.13.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.13.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.13.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.13.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.13.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.13.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.13.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.13.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.13.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.13.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.14.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.14.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.14.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.14.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.14.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.14.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.14.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.14.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.14.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.14.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.14.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.14.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.15.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.15.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.15.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.15.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.15.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.15.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.15.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.15.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.15.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.15.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.15.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.15.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.16.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.16.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.16.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.16.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.16.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.16.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.16.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.16.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.16.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.16.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.16.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.16.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.17.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.17.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.17.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.17.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.17.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.17.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.17.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.17.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.17.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.17.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.17.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.17.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.18.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.18.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.18.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.18.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.18.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.18.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.18.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.18.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.18.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.18.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.18.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.18.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.19.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.19.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.19.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.19.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.19.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.19.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.19.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.19.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.19.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.19.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.19.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.19.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.20.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.20.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.20.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.20.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.20.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.20.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.20.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.20.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.20.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.20.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.20.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.20.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.21.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.21.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.21.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.21.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.21.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.21.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.21.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.21.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.21.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.21.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.21.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.21.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.22.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.22.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.22.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.22.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.22.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.22.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.22.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.22.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.22.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.22.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.22.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.22.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.23.attention.self.query.weight
06/27 01:47:40 PM n: encoder.layer.23.attention.self.query.bias
06/27 01:47:40 PM n: encoder.layer.23.attention.self.key.weight
06/27 01:47:40 PM n: encoder.layer.23.attention.self.key.bias
06/27 01:47:40 PM n: encoder.layer.23.attention.self.value.weight
06/27 01:47:40 PM n: encoder.layer.23.attention.self.value.bias
06/27 01:47:40 PM n: encoder.layer.23.attention.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.23.attention.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: encoder.layer.23.intermediate.dense.weight
06/27 01:47:40 PM n: encoder.layer.23.intermediate.dense.bias
06/27 01:47:40 PM n: encoder.layer.23.output.dense.weight
06/27 01:47:40 PM n: encoder.layer.23.output.dense.bias
06/27 01:47:40 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 01:47:40 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 01:47:40 PM n: pooler.dense.weight
06/27 01:47:40 PM n: pooler.dense.bias
06/27 01:47:40 PM n: roberta.embeddings.word_embeddings.weight
06/27 01:47:40 PM n: roberta.embeddings.position_embeddings.weight
06/27 01:47:40 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 01:47:40 PM n: roberta.embeddings.LayerNorm.weight
06/27 01:47:40 PM n: roberta.embeddings.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 01:47:40 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 01:47:40 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 01:47:40 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 01:47:40 PM n: roberta.pooler.dense.weight
06/27 01:47:40 PM n: roberta.pooler.dense.bias
06/27 01:47:40 PM n: lm_head.bias
06/27 01:47:40 PM n: lm_head.dense.weight
06/27 01:47:40 PM n: lm_head.dense.bias
06/27 01:47:40 PM n: lm_head.layer_norm.weight
06/27 01:47:40 PM n: lm_head.layer_norm.bias
06/27 01:47:40 PM n: lm_head.decoder.weight
06/27 01:47:40 PM Total parameters: 763292761
06/27 01:47:40 PM ***** LOSS printing *****
06/27 01:47:40 PM loss
06/27 01:47:40 PM tensor(21.1922, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:47:40 PM ***** LOSS printing *****
06/27 01:47:40 PM loss
06/27 01:47:40 PM tensor(14.7148, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:47:40 PM ***** LOSS printing *****
06/27 01:47:40 PM loss
06/27 01:47:40 PM tensor(9.8539, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:47:40 PM ***** LOSS printing *****
06/27 01:47:40 PM loss
06/27 01:47:40 PM tensor(6.7007, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:47:41 PM ***** Running evaluation MLM *****
06/27 01:47:41 PM   Epoch = 0 iter 4 step
06/27 01:47:41 PM   Num examples = 16
06/27 01:47:41 PM   Batch size = 32
06/27 01:47:41 PM ***** Eval results *****
06/27 01:47:41 PM   acc = 0.5
06/27 01:47:41 PM   cls_loss = 13.115395426750183
06/27 01:47:41 PM   eval_loss = 3.3699913024902344
06/27 01:47:41 PM   global_step = 4
06/27 01:47:41 PM   loss = 13.115395426750183
06/27 01:47:41 PM ***** Save model *****
06/27 01:47:41 PM ***** Test Dataset Eval Result *****
06/27 01:48:45 PM ***** Eval results *****
06/27 01:48:45 PM   acc = 0.4945
06/27 01:48:45 PM   cls_loss = 13.115395426750183
06/27 01:48:45 PM   eval_loss = 3.5463844935099282
06/27 01:48:45 PM   global_step = 4
06/27 01:48:45 PM   loss = 13.115395426750183
06/27 01:48:49 PM ***** LOSS printing *****
06/27 01:48:49 PM loss
06/27 01:48:49 PM tensor(4.1454, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:49 PM ***** LOSS printing *****
06/27 01:48:49 PM loss
06/27 01:48:49 PM tensor(1.6134, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:49 PM ***** LOSS printing *****
06/27 01:48:49 PM loss
06/27 01:48:49 PM tensor(1.5740, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:49 PM ***** LOSS printing *****
06/27 01:48:49 PM loss
06/27 01:48:49 PM tensor(0.6922, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:50 PM ***** LOSS printing *****
06/27 01:48:50 PM loss
06/27 01:48:50 PM tensor(0.1225, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:50 PM ***** Running evaluation MLM *****
06/27 01:48:50 PM   Epoch = 2 iter 9 step
06/27 01:48:50 PM   Num examples = 16
06/27 01:48:50 PM   Batch size = 32
06/27 01:48:50 PM ***** Eval results *****
06/27 01:48:50 PM   acc = 0.5
06/27 01:48:50 PM   cls_loss = 0.12251575291156769
06/27 01:48:50 PM   eval_loss = 3.4929118156433105
06/27 01:48:50 PM   global_step = 9
06/27 01:48:50 PM   loss = 0.12251575291156769
06/27 01:48:50 PM ***** LOSS printing *****
06/27 01:48:50 PM loss
06/27 01:48:50 PM tensor(3.0426, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:51 PM ***** LOSS printing *****
06/27 01:48:51 PM loss
06/27 01:48:51 PM tensor(6.5103, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:51 PM ***** LOSS printing *****
06/27 01:48:51 PM loss
06/27 01:48:51 PM tensor(2.8596, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:51 PM ***** LOSS printing *****
06/27 01:48:51 PM loss
06/27 01:48:51 PM tensor(0.7294, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:51 PM ***** LOSS printing *****
06/27 01:48:51 PM loss
06/27 01:48:51 PM tensor(0.1623, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:51 PM ***** Running evaluation MLM *****
06/27 01:48:51 PM   Epoch = 3 iter 14 step
06/27 01:48:51 PM   Num examples = 16
06/27 01:48:51 PM   Batch size = 32
06/27 01:48:52 PM ***** Eval results *****
06/27 01:48:52 PM   acc = 0.5
06/27 01:48:52 PM   cls_loss = 0.445859856903553
06/27 01:48:52 PM   eval_loss = 1.1556289196014404
06/27 01:48:52 PM   global_step = 14
06/27 01:48:52 PM   loss = 0.445859856903553
06/27 01:48:52 PM ***** LOSS printing *****
06/27 01:48:52 PM loss
06/27 01:48:52 PM tensor(0.7597, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:52 PM ***** LOSS printing *****
06/27 01:48:52 PM loss
06/27 01:48:52 PM tensor(2.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:52 PM ***** LOSS printing *****
06/27 01:48:52 PM loss
06/27 01:48:52 PM tensor(1.3802, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:53 PM ***** LOSS printing *****
06/27 01:48:53 PM loss
06/27 01:48:53 PM tensor(1.4998, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:53 PM ***** LOSS printing *****
06/27 01:48:53 PM loss
06/27 01:48:53 PM tensor(0.1041, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:48:53 PM ***** Running evaluation MLM *****
06/27 01:48:53 PM   Epoch = 4 iter 19 step
06/27 01:48:53 PM   Num examples = 16
06/27 01:48:53 PM   Batch size = 32
06/27 01:48:54 PM ***** Eval results *****
06/27 01:48:54 PM   acc = 0.6875
06/27 01:48:54 PM   cls_loss = 0.9947096953789393
06/27 01:48:54 PM   eval_loss = 0.5205839276313782
06/27 01:48:54 PM   global_step = 19
06/27 01:48:54 PM   loss = 0.9947096953789393
06/27 01:48:54 PM ***** Save model *****
06/27 01:48:54 PM ***** Test Dataset Eval Result *****
06/27 01:49:57 PM ***** Eval results *****
06/27 01:49:57 PM   acc = 0.7335
06/27 01:49:57 PM   cls_loss = 0.9947096953789393
06/27 01:49:57 PM   eval_loss = 0.5752598475369196
06/27 01:49:57 PM   global_step = 19
06/27 01:49:57 PM   loss = 0.9947096953789393
06/27 01:50:01 PM ***** LOSS printing *****
06/27 01:50:01 PM loss
06/27 01:50:01 PM tensor(0.3287, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:50:01 PM ***** LOSS printing *****
06/27 01:50:01 PM loss
06/27 01:50:01 PM tensor(0.3100, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:50:02 PM ***** LOSS printing *****
06/27 01:50:02 PM loss
06/27 01:50:02 PM tensor(0.6465, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:50:02 PM ***** LOSS printing *****
06/27 01:50:02 PM loss
06/27 01:50:02 PM tensor(0.0758, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:50:02 PM ***** LOSS printing *****
06/27 01:50:02 PM loss
06/27 01:50:02 PM tensor(0.1553, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:50:02 PM ***** Running evaluation MLM *****
06/27 01:50:02 PM   Epoch = 5 iter 24 step
06/27 01:50:02 PM   Num examples = 16
06/27 01:50:02 PM   Batch size = 32
06/27 01:50:03 PM ***** Eval results *****
06/27 01:50:03 PM   acc = 0.875
06/27 01:50:03 PM   cls_loss = 0.2969014625996351
06/27 01:50:03 PM   eval_loss = 0.25310879945755005
06/27 01:50:03 PM   global_step = 24
06/27 01:50:03 PM   loss = 0.2969014625996351
06/27 01:50:03 PM ***** Save model *****
06/27 01:50:03 PM ***** Test Dataset Eval Result *****
06/27 01:51:06 PM ***** Eval results *****
06/27 01:51:06 PM   acc = 0.7615
06/27 01:51:06 PM   cls_loss = 0.2969014625996351
06/27 01:51:06 PM   eval_loss = 0.5004499782882039
06/27 01:51:06 PM   global_step = 24
06/27 01:51:06 PM   loss = 0.2969014625996351
06/27 01:51:10 PM ***** LOSS printing *****
06/27 01:51:10 PM loss
06/27 01:51:10 PM tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:10 PM ***** LOSS printing *****
06/27 01:51:10 PM loss
06/27 01:51:10 PM tensor(0.1576, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:11 PM ***** LOSS printing *****
06/27 01:51:11 PM loss
06/27 01:51:11 PM tensor(0.0175, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:11 PM ***** LOSS printing *****
06/27 01:51:11 PM loss
06/27 01:51:11 PM tensor(1.2322, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:11 PM ***** LOSS printing *****
06/27 01:51:11 PM loss
06/27 01:51:11 PM tensor(0.5913, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:11 PM ***** Running evaluation MLM *****
06/27 01:51:11 PM   Epoch = 7 iter 29 step
06/27 01:51:11 PM   Num examples = 16
06/27 01:51:11 PM   Batch size = 32
06/27 01:51:12 PM ***** Eval results *****
06/27 01:51:12 PM   acc = 0.875
06/27 01:51:12 PM   cls_loss = 0.5912976264953613
06/27 01:51:12 PM   eval_loss = 0.29517269134521484
06/27 01:51:12 PM   global_step = 29
06/27 01:51:12 PM   loss = 0.5912976264953613
06/27 01:51:12 PM ***** LOSS printing *****
06/27 01:51:12 PM loss
06/27 01:51:12 PM tensor(0.0013, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:12 PM ***** LOSS printing *****
06/27 01:51:12 PM loss
06/27 01:51:12 PM tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:12 PM ***** LOSS printing *****
06/27 01:51:12 PM loss
06/27 01:51:12 PM tensor(0.0035, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:13 PM ***** LOSS printing *****
06/27 01:51:13 PM loss
06/27 01:51:13 PM tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:13 PM ***** LOSS printing *****
06/27 01:51:13 PM loss
06/27 01:51:13 PM tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:13 PM ***** Running evaluation MLM *****
06/27 01:51:13 PM   Epoch = 8 iter 34 step
06/27 01:51:13 PM   Num examples = 16
06/27 01:51:13 PM   Batch size = 32
06/27 01:51:13 PM ***** Eval results *****
06/27 01:51:13 PM   acc = 0.6875
06/27 01:51:13 PM   cls_loss = 0.0005388630088418722
06/27 01:51:13 PM   eval_loss = 0.794792652130127
06/27 01:51:13 PM   global_step = 34
06/27 01:51:13 PM   loss = 0.0005388630088418722
06/27 01:51:13 PM ***** LOSS printing *****
06/27 01:51:13 PM loss
06/27 01:51:13 PM tensor(0.0074, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:14 PM ***** LOSS printing *****
06/27 01:51:14 PM loss
06/27 01:51:14 PM tensor(0.0011, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:14 PM ***** LOSS printing *****
06/27 01:51:14 PM loss
06/27 01:51:14 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:14 PM ***** LOSS printing *****
06/27 01:51:14 PM loss
06/27 01:51:14 PM tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:14 PM ***** LOSS printing *****
06/27 01:51:14 PM loss
06/27 01:51:14 PM tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:51:15 PM ***** Running evaluation MLM *****
06/27 01:51:15 PM   Epoch = 9 iter 39 step
06/27 01:51:15 PM   Num examples = 16
06/27 01:51:15 PM   Batch size = 32
06/27 01:51:15 PM ***** Eval results *****
06/27 01:51:15 PM   acc = 0.6875
06/27 01:51:15 PM   cls_loss = 0.0011810361465904862
06/27 01:51:15 PM   eval_loss = 2.148800849914551
06/27 01:51:15 PM   global_step = 39
06/27 01:51:15 PM   loss = 0.0011810361465904862
06/27 01:51:15 PM ***** LOSS printing *****
06/27 01:51:15 PM loss
06/27 01:51:15 PM tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)

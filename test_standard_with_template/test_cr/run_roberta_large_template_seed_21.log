06/27 02:09:43 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-21/', data_seed=21, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 02:09:43 PM device: cuda n_gpu: 1
06/27 02:09:43 PM Writing example 0 of 16
06/27 02:09:43 PM *** Example ***
06/27 02:09:43 PM guid: train-1
06/27 02:09:43 PM tokens: <s> the Ġn okia Ġ66 00 Ġis Ġa Ġdecent Ġextension Ġof Ġthe Ġsmart Ġphone Ġline Ġ. </s> ĠIt Ġis <mask>
06/27 02:09:43 PM input_ids: 0 627 295 43946 5138 612 16 10 7297 5064 9 5 2793 1028 516 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM label: ['Ġpositive']
06/27 02:09:43 PM Writing example 0 of 16
06/27 02:09:43 PM *** Example ***
06/27 02:09:43 PM guid: dev-1
06/27 02:09:43 PM tokens: <s> the Ġclarity Ġis Ġunbelievable Ġ, Ġwhich Ġmeans Ġyou Ġcan Ġcram Ġalmost Ġthe Ġfull Ġ2 Ġ, Ġ5 Ġ! Ġ. </s> ĠIt Ġis <mask>
06/27 02:09:43 PM input_ids: 0 627 10498 16 14011 2156 61 839 47 64 39011 818 5 455 132 2156 195 27785 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM label: ['Ġpositive']
06/27 02:09:43 PM Writing example 0 of 2000
06/27 02:09:43 PM *** Example ***
06/27 02:09:43 PM guid: dev-1
06/27 02:09:43 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 02:09:43 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:09:43 PM label: ['Ġnegative']
06/27 02:09:56 PM ***** Running training *****
06/27 02:09:56 PM   Num examples = 16
06/27 02:09:56 PM   Batch size = 4
06/27 02:09:56 PM   Num steps = 40
06/27 02:09:56 PM n: embeddings.word_embeddings.weight
06/27 02:09:56 PM n: embeddings.position_embeddings.weight
06/27 02:09:56 PM n: embeddings.token_type_embeddings.weight
06/27 02:09:56 PM n: embeddings.LayerNorm.weight
06/27 02:09:56 PM n: embeddings.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.0.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.0.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.0.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.0.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.0.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.0.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.0.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.0.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.0.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.0.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.0.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.0.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.1.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.1.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.1.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.1.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.1.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.1.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.1.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.1.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.1.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.1.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.1.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.1.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.2.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.2.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.2.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.2.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.2.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.2.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.2.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.2.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.2.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.2.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.2.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.2.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.3.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.3.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.3.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.3.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.3.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.3.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.3.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.3.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.3.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.3.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.3.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.3.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.4.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.4.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.4.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.4.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.4.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.4.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.4.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.4.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.4.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.4.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.4.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.4.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.5.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.5.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.5.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.5.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.5.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.5.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.5.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.5.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.5.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.5.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.5.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.5.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.6.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.6.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.6.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.6.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.6.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.6.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.6.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.6.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.6.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.6.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.6.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.6.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.7.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.7.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.7.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.7.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.7.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.7.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.7.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.7.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.7.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.7.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.7.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.7.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.8.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.8.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.8.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.8.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.8.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.8.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.8.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.8.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.8.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.8.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.8.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.8.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.9.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.9.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.9.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.9.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.9.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.9.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.9.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.9.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.9.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.9.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.9.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.9.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.10.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.10.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.10.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.10.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.10.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.10.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.10.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.10.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.10.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.10.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.10.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.10.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.11.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.11.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.11.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.11.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.11.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.11.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.11.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.11.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.11.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.11.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.11.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.11.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.12.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.12.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.12.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.12.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.12.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.12.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.12.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.12.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.12.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.12.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.12.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.12.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.13.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.13.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.13.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.13.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.13.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.13.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.13.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.13.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.13.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.13.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.13.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.13.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.14.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.14.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.14.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.14.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.14.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.14.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.14.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.14.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.14.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.14.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.14.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.14.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.15.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.15.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.15.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.15.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.15.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.15.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.15.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.15.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.15.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.15.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.15.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.15.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.16.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.16.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.16.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.16.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.16.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.16.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.16.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.16.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.16.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.16.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.16.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.16.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.17.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.17.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.17.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.17.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.17.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.17.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.17.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.17.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.17.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.17.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.17.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.17.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.18.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.18.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.18.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.18.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.18.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.18.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.18.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.18.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.18.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.18.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.18.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.18.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.19.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.19.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.19.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.19.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.19.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.19.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.19.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.19.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.19.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.19.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.19.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.19.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.20.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.20.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.20.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.20.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.20.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.20.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.20.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.20.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.20.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.20.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.20.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.20.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.21.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.21.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.21.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.21.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.21.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.21.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.21.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.21.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.21.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.21.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.21.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.21.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.22.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.22.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.22.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.22.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.22.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.22.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.22.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.22.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.22.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.22.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.22.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.22.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.23.attention.self.query.weight
06/27 02:09:56 PM n: encoder.layer.23.attention.self.query.bias
06/27 02:09:56 PM n: encoder.layer.23.attention.self.key.weight
06/27 02:09:56 PM n: encoder.layer.23.attention.self.key.bias
06/27 02:09:56 PM n: encoder.layer.23.attention.self.value.weight
06/27 02:09:56 PM n: encoder.layer.23.attention.self.value.bias
06/27 02:09:56 PM n: encoder.layer.23.attention.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.23.attention.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: encoder.layer.23.intermediate.dense.weight
06/27 02:09:56 PM n: encoder.layer.23.intermediate.dense.bias
06/27 02:09:56 PM n: encoder.layer.23.output.dense.weight
06/27 02:09:56 PM n: encoder.layer.23.output.dense.bias
06/27 02:09:56 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 02:09:56 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 02:09:56 PM n: pooler.dense.weight
06/27 02:09:56 PM n: pooler.dense.bias
06/27 02:09:56 PM n: roberta.embeddings.word_embeddings.weight
06/27 02:09:56 PM n: roberta.embeddings.position_embeddings.weight
06/27 02:09:56 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 02:09:56 PM n: roberta.embeddings.LayerNorm.weight
06/27 02:09:56 PM n: roberta.embeddings.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 02:09:56 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 02:09:56 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 02:09:56 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 02:09:56 PM n: roberta.pooler.dense.weight
06/27 02:09:56 PM n: roberta.pooler.dense.bias
06/27 02:09:56 PM n: lm_head.bias
06/27 02:09:56 PM n: lm_head.dense.weight
06/27 02:09:56 PM n: lm_head.dense.bias
06/27 02:09:56 PM n: lm_head.layer_norm.weight
06/27 02:09:56 PM n: lm_head.layer_norm.bias
06/27 02:09:56 PM n: lm_head.decoder.weight
06/27 02:09:56 PM Total parameters: 763292761
06/27 02:09:56 PM ***** LOSS printing *****
06/27 02:09:56 PM loss
06/27 02:09:56 PM tensor(21.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:09:56 PM ***** LOSS printing *****
06/27 02:09:56 PM loss
06/27 02:09:56 PM tensor(14.6786, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:09:56 PM ***** LOSS printing *****
06/27 02:09:56 PM loss
06/27 02:09:56 PM tensor(7.0075, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:09:57 PM ***** LOSS printing *****
06/27 02:09:57 PM loss
06/27 02:09:57 PM tensor(2.4220, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:09:57 PM ***** Running evaluation MLM *****
06/27 02:09:57 PM   Epoch = 0 iter 4 step
06/27 02:09:57 PM   Num examples = 16
06/27 02:09:57 PM   Batch size = 32
06/27 02:09:57 PM ***** Eval results *****
06/27 02:09:57 PM   acc = 0.75
06/27 02:09:57 PM   cls_loss = 11.303292512893677
06/27 02:09:57 PM   eval_loss = 0.6889033317565918
06/27 02:09:57 PM   global_step = 4
06/27 02:09:57 PM   loss = 11.303292512893677
06/27 02:09:57 PM ***** Save model *****
06/27 02:09:57 PM ***** Test Dataset Eval Result *****
06/27 02:11:00 PM ***** Eval results *****
06/27 02:11:00 PM   acc = 0.709
06/27 02:11:00 PM   cls_loss = 11.303292512893677
06/27 02:11:00 PM   eval_loss = 0.6452501998061225
06/27 02:11:00 PM   global_step = 4
06/27 02:11:00 PM   loss = 11.303292512893677
06/27 02:11:04 PM ***** LOSS printing *****
06/27 02:11:04 PM loss
06/27 02:11:04 PM tensor(0.7380, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:05 PM ***** LOSS printing *****
06/27 02:11:05 PM loss
06/27 02:11:05 PM tensor(0.8070, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:05 PM ***** LOSS printing *****
06/27 02:11:05 PM loss
06/27 02:11:05 PM tensor(0.6346, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:05 PM ***** LOSS printing *****
06/27 02:11:05 PM loss
06/27 02:11:05 PM tensor(0.8178, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:05 PM ***** LOSS printing *****
06/27 02:11:05 PM loss
06/27 02:11:05 PM tensor(0.0756, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:05 PM ***** Running evaluation MLM *****
06/27 02:11:05 PM   Epoch = 2 iter 9 step
06/27 02:11:05 PM   Num examples = 16
06/27 02:11:05 PM   Batch size = 32
06/27 02:11:06 PM ***** Eval results *****
06/27 02:11:06 PM   acc = 0.5
06/27 02:11:06 PM   cls_loss = 0.07562442868947983
06/27 02:11:06 PM   eval_loss = 1.613615870475769
06/27 02:11:06 PM   global_step = 9
06/27 02:11:06 PM   loss = 0.07562442868947983
06/27 02:11:06 PM ***** LOSS printing *****
06/27 02:11:06 PM loss
06/27 02:11:06 PM tensor(1.1609, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:06 PM ***** LOSS printing *****
06/27 02:11:06 PM loss
06/27 02:11:06 PM tensor(1.7487, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:06 PM ***** LOSS printing *****
06/27 02:11:06 PM loss
06/27 02:11:06 PM tensor(1.2856, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:07 PM ***** LOSS printing *****
06/27 02:11:07 PM loss
06/27 02:11:07 PM tensor(0.0357, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:07 PM ***** LOSS printing *****
06/27 02:11:07 PM loss
06/27 02:11:07 PM tensor(0.0058, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:11:07 PM ***** Running evaluation MLM *****
06/27 02:11:07 PM   Epoch = 3 iter 14 step
06/27 02:11:07 PM   Num examples = 16
06/27 02:11:07 PM   Batch size = 32
06/27 02:11:08 PM ***** Eval results *****
06/27 02:11:08 PM   acc = 0.9375
06/27 02:11:08 PM   cls_loss = 0.02072440553456545
06/27 02:11:08 PM   eval_loss = 0.4803365468978882
06/27 02:11:08 PM   global_step = 14
06/27 02:11:08 PM   loss = 0.02072440553456545
06/27 02:11:08 PM ***** Save model *****
06/27 02:11:08 PM ***** Test Dataset Eval Result *****
06/27 02:12:11 PM ***** Eval results *****
06/27 02:12:11 PM   acc = 0.8565
06/27 02:12:11 PM   cls_loss = 0.02072440553456545
06/27 02:12:11 PM   eval_loss = 0.8357165727370773
06/27 02:12:11 PM   global_step = 14
06/27 02:12:11 PM   loss = 0.02072440553456545
06/27 02:12:15 PM ***** LOSS printing *****
06/27 02:12:15 PM loss
06/27 02:12:15 PM tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:15 PM ***** LOSS printing *****
06/27 02:12:15 PM loss
06/27 02:12:15 PM tensor(0.1913, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:15 PM ***** LOSS printing *****
06/27 02:12:15 PM loss
06/27 02:12:15 PM tensor(2.1657, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:16 PM ***** LOSS printing *****
06/27 02:12:16 PM loss
06/27 02:12:16 PM tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:16 PM ***** LOSS printing *****
06/27 02:12:16 PM loss
06/27 02:12:16 PM tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:16 PM ***** Running evaluation MLM *****
06/27 02:12:16 PM   Epoch = 4 iter 19 step
06/27 02:12:16 PM   Num examples = 16
06/27 02:12:16 PM   Batch size = 32
06/27 02:12:17 PM ***** Eval results *****
06/27 02:12:17 PM   acc = 0.8125
06/27 02:12:17 PM   cls_loss = 0.7241914718567083
06/27 02:12:17 PM   eval_loss = 0.857184886932373
06/27 02:12:17 PM   global_step = 19
06/27 02:12:17 PM   loss = 0.7241914718567083
06/27 02:12:17 PM ***** LOSS printing *****
06/27 02:12:17 PM loss
06/27 02:12:17 PM tensor(1.5205, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:17 PM ***** LOSS printing *****
06/27 02:12:17 PM loss
06/27 02:12:17 PM tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:17 PM ***** LOSS printing *****
06/27 02:12:17 PM loss
06/27 02:12:17 PM tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:17 PM ***** LOSS printing *****
06/27 02:12:17 PM loss
06/27 02:12:17 PM tensor(0.0114, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:18 PM ***** LOSS printing *****
06/27 02:12:18 PM loss
06/27 02:12:18 PM tensor(0.0044, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:18 PM ***** Running evaluation MLM *****
06/27 02:12:18 PM   Epoch = 5 iter 24 step
06/27 02:12:18 PM   Num examples = 16
06/27 02:12:18 PM   Batch size = 32
06/27 02:12:18 PM ***** Eval results *****
06/27 02:12:18 PM   acc = 0.75
06/27 02:12:18 PM   cls_loss = 0.004784962671692483
06/27 02:12:18 PM   eval_loss = 1.6784675121307373
06/27 02:12:18 PM   global_step = 24
06/27 02:12:18 PM   loss = 0.004784962671692483
06/27 02:12:18 PM ***** LOSS printing *****
06/27 02:12:18 PM loss
06/27 02:12:18 PM tensor(0.0183, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:19 PM ***** LOSS printing *****
06/27 02:12:19 PM loss
06/27 02:12:19 PM tensor(0.8673, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:19 PM ***** LOSS printing *****
06/27 02:12:19 PM loss
06/27 02:12:19 PM tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:19 PM ***** LOSS printing *****
06/27 02:12:19 PM loss
06/27 02:12:19 PM tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:19 PM ***** LOSS printing *****
06/27 02:12:19 PM loss
06/27 02:12:19 PM tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:19 PM ***** Running evaluation MLM *****
06/27 02:12:19 PM   Epoch = 7 iter 29 step
06/27 02:12:19 PM   Num examples = 16
06/27 02:12:19 PM   Batch size = 32
06/27 02:12:20 PM ***** Eval results *****
06/27 02:12:20 PM   acc = 0.75
06/27 02:12:20 PM   cls_loss = 0.0016100539360195398
06/27 02:12:20 PM   eval_loss = 1.2700475454330444
06/27 02:12:20 PM   global_step = 29
06/27 02:12:20 PM   loss = 0.0016100539360195398
06/27 02:12:20 PM ***** LOSS printing *****
06/27 02:12:20 PM loss
06/27 02:12:20 PM tensor(0.0012, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:20 PM ***** LOSS printing *****
06/27 02:12:20 PM loss
06/27 02:12:20 PM tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:20 PM ***** LOSS printing *****
06/27 02:12:20 PM loss
06/27 02:12:20 PM tensor(0.8441, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:21 PM ***** LOSS printing *****
06/27 02:12:21 PM loss
06/27 02:12:21 PM tensor(0.0145, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:21 PM ***** LOSS printing *****
06/27 02:12:21 PM loss
06/27 02:12:21 PM tensor(0.0102, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:21 PM ***** Running evaluation MLM *****
06/27 02:12:21 PM   Epoch = 8 iter 34 step
06/27 02:12:21 PM   Num examples = 16
06/27 02:12:21 PM   Batch size = 32
06/27 02:12:21 PM ***** Eval results *****
06/27 02:12:21 PM   acc = 0.6875
06/27 02:12:21 PM   cls_loss = 0.012364471796900034
06/27 02:12:21 PM   eval_loss = 0.6515219807624817
06/27 02:12:21 PM   global_step = 34
06/27 02:12:21 PM   loss = 0.012364471796900034
06/27 02:12:22 PM ***** LOSS printing *****
06/27 02:12:22 PM loss
06/27 02:12:22 PM tensor(0.0026, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:22 PM ***** LOSS printing *****
06/27 02:12:22 PM loss
06/27 02:12:22 PM tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:22 PM ***** LOSS printing *****
06/27 02:12:22 PM loss
06/27 02:12:22 PM tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:22 PM ***** LOSS printing *****
06/27 02:12:22 PM loss
06/27 02:12:22 PM tensor(0.1119, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:22 PM ***** LOSS printing *****
06/27 02:12:22 PM loss
06/27 02:12:22 PM tensor(0.0180, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:23 PM ***** Running evaluation MLM *****
06/27 02:12:23 PM   Epoch = 9 iter 39 step
06/27 02:12:23 PM   Num examples = 16
06/27 02:12:23 PM   Batch size = 32
06/27 02:12:23 PM ***** Eval results *****
06/27 02:12:23 PM   acc = 0.75
06/27 02:12:23 PM   cls_loss = 0.043895017549706004
06/27 02:12:23 PM   eval_loss = 0.8302576541900635
06/27 02:12:23 PM   global_step = 39
06/27 02:12:23 PM   loss = 0.043895017549706004
06/27 02:12:23 PM ***** LOSS printing *****
06/27 02:12:23 PM loss
06/27 02:12:23 PM tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)

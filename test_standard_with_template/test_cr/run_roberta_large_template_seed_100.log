06/27 02:00:27 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-100/', data_seed=100, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 02:00:27 PM device: cuda n_gpu: 1
06/27 02:00:27 PM Writing example 0 of 16
06/27 02:00:27 PM *** Example ***
06/27 02:00:27 PM guid: train-1
06/27 02:00:27 PM tokens: <s> love Ġthe Ġspeaker Ġphone Ġ. </s> ĠIt Ġis <mask>
06/27 02:00:27 PM input_ids: 0 17693 5 5385 1028 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM label: ['Ġpositive']
06/27 02:00:27 PM Writing example 0 of 16
06/27 02:00:27 PM *** Example ***
06/27 02:00:27 PM guid: dev-1
06/27 02:00:27 PM tokens: <s> i Ġhave Ġhad Ġthe Ġip od Ġmini Ġand Ġit Ġperforms Ġon Ġthe Ġlevel Ġwith Ġmy Ġsound Ġcard Ġbut Ġthe Ġmicro Ġbeats Ġit Ġoutright Ġ. </s> ĠIt Ġis <mask>
06/27 02:00:27 PM input_ids: 0 118 33 56 5 36180 1630 7983 8 24 14023 15 5 672 19 127 2369 1886 53 5 5177 13410 24 12162 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM label: ['Ġpositive']
06/27 02:00:27 PM Writing example 0 of 2000
06/27 02:00:27 PM *** Example ***
06/27 02:00:27 PM guid: dev-1
06/27 02:00:27 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 02:00:27 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:00:27 PM label: ['Ġnegative']
06/27 02:00:40 PM ***** Running training *****
06/27 02:00:40 PM   Num examples = 16
06/27 02:00:40 PM   Batch size = 4
06/27 02:00:40 PM   Num steps = 40
06/27 02:00:40 PM n: embeddings.word_embeddings.weight
06/27 02:00:40 PM n: embeddings.position_embeddings.weight
06/27 02:00:40 PM n: embeddings.token_type_embeddings.weight
06/27 02:00:40 PM n: embeddings.LayerNorm.weight
06/27 02:00:40 PM n: embeddings.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.0.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.0.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.0.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.0.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.0.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.0.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.0.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.0.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.0.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.0.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.0.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.0.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.1.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.1.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.1.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.1.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.1.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.1.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.1.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.1.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.1.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.1.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.1.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.1.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.2.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.2.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.2.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.2.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.2.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.2.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.2.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.2.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.2.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.2.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.2.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.2.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.3.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.3.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.3.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.3.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.3.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.3.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.3.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.3.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.3.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.3.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.3.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.3.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.4.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.4.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.4.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.4.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.4.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.4.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.4.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.4.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.4.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.4.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.4.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.4.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.5.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.5.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.5.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.5.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.5.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.5.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.5.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.5.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.5.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.5.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.5.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.5.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.6.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.6.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.6.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.6.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.6.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.6.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.6.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.6.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.6.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.6.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.6.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.6.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.7.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.7.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.7.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.7.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.7.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.7.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.7.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.7.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.7.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.7.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.7.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.7.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.8.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.8.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.8.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.8.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.8.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.8.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.8.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.8.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.8.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.8.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.8.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.8.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.9.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.9.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.9.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.9.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.9.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.9.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.9.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.9.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.9.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.9.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.9.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.9.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.10.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.10.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.10.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.10.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.10.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.10.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.10.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.10.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.10.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.10.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.10.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.10.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.11.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.11.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.11.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.11.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.11.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.11.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.11.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.11.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.11.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.11.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.11.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.11.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.12.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.12.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.12.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.12.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.12.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.12.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.12.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.12.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.12.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.12.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.12.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.12.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.13.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.13.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.13.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.13.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.13.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.13.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.13.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.13.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.13.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.13.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.13.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.13.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.14.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.14.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.14.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.14.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.14.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.14.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.14.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.14.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.14.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.14.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.14.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.14.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.15.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.15.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.15.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.15.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.15.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.15.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.15.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.15.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.15.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.15.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.15.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.15.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.16.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.16.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.16.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.16.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.16.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.16.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.16.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.16.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.16.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.16.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.16.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.16.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.17.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.17.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.17.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.17.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.17.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.17.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.17.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.17.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.17.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.17.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.17.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.17.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.18.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.18.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.18.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.18.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.18.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.18.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.18.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.18.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.18.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.18.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.18.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.18.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.19.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.19.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.19.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.19.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.19.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.19.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.19.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.19.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.19.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.19.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.19.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.19.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.20.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.20.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.20.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.20.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.20.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.20.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.20.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.20.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.20.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.20.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.20.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.20.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.21.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.21.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.21.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.21.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.21.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.21.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.21.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.21.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.21.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.21.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.21.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.21.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.22.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.22.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.22.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.22.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.22.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.22.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.22.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.22.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.22.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.22.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.22.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.22.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.23.attention.self.query.weight
06/27 02:00:40 PM n: encoder.layer.23.attention.self.query.bias
06/27 02:00:40 PM n: encoder.layer.23.attention.self.key.weight
06/27 02:00:40 PM n: encoder.layer.23.attention.self.key.bias
06/27 02:00:40 PM n: encoder.layer.23.attention.self.value.weight
06/27 02:00:40 PM n: encoder.layer.23.attention.self.value.bias
06/27 02:00:40 PM n: encoder.layer.23.attention.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.23.attention.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: encoder.layer.23.intermediate.dense.weight
06/27 02:00:40 PM n: encoder.layer.23.intermediate.dense.bias
06/27 02:00:40 PM n: encoder.layer.23.output.dense.weight
06/27 02:00:40 PM n: encoder.layer.23.output.dense.bias
06/27 02:00:40 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 02:00:40 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 02:00:40 PM n: pooler.dense.weight
06/27 02:00:40 PM n: pooler.dense.bias
06/27 02:00:40 PM n: roberta.embeddings.word_embeddings.weight
06/27 02:00:40 PM n: roberta.embeddings.position_embeddings.weight
06/27 02:00:40 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 02:00:40 PM n: roberta.embeddings.LayerNorm.weight
06/27 02:00:40 PM n: roberta.embeddings.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 02:00:40 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 02:00:40 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 02:00:40 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 02:00:40 PM n: roberta.pooler.dense.weight
06/27 02:00:40 PM n: roberta.pooler.dense.bias
06/27 02:00:40 PM n: lm_head.bias
06/27 02:00:40 PM n: lm_head.dense.weight
06/27 02:00:40 PM n: lm_head.dense.bias
06/27 02:00:40 PM n: lm_head.layer_norm.weight
06/27 02:00:40 PM n: lm_head.layer_norm.bias
06/27 02:00:40 PM n: lm_head.decoder.weight
06/27 02:00:40 PM Total parameters: 763292761
06/27 02:00:40 PM ***** LOSS printing *****
06/27 02:00:40 PM loss
06/27 02:00:40 PM tensor(22.9365, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:00:41 PM ***** LOSS printing *****
06/27 02:00:41 PM loss
06/27 02:00:41 PM tensor(13.0709, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:00:41 PM ***** LOSS printing *****
06/27 02:00:41 PM loss
06/27 02:00:41 PM tensor(8.5153, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:00:41 PM ***** LOSS printing *****
06/27 02:00:41 PM loss
06/27 02:00:41 PM tensor(4.8366, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:00:41 PM ***** Running evaluation MLM *****
06/27 02:00:41 PM   Epoch = 0 iter 4 step
06/27 02:00:41 PM   Num examples = 16
06/27 02:00:41 PM   Batch size = 32
06/27 02:00:42 PM ***** Eval results *****
06/27 02:00:42 PM   acc = 0.8125
06/27 02:00:42 PM   cls_loss = 12.33983302116394
06/27 02:00:42 PM   eval_loss = 1.4555459022521973
06/27 02:00:42 PM   global_step = 4
06/27 02:00:42 PM   loss = 12.33983302116394
06/27 02:00:42 PM ***** Save model *****
06/27 02:00:42 PM ***** Test Dataset Eval Result *****
06/27 02:01:45 PM ***** Eval results *****
06/27 02:01:45 PM   acc = 0.726
06/27 02:01:45 PM   cls_loss = 12.33983302116394
06/27 02:01:45 PM   eval_loss = 1.549410725396777
06/27 02:01:45 PM   global_step = 4
06/27 02:01:45 PM   loss = 12.33983302116394
06/27 02:01:49 PM ***** LOSS printing *****
06/27 02:01:49 PM loss
06/27 02:01:49 PM tensor(0.9586, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:49 PM ***** LOSS printing *****
06/27 02:01:49 PM loss
06/27 02:01:49 PM tensor(0.5309, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:49 PM ***** LOSS printing *****
06/27 02:01:49 PM loss
06/27 02:01:49 PM tensor(0.7756, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:50 PM ***** LOSS printing *****
06/27 02:01:50 PM loss
06/27 02:01:50 PM tensor(0.5578, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:50 PM ***** LOSS printing *****
06/27 02:01:50 PM loss
06/27 02:01:50 PM tensor(0.0985, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:50 PM ***** Running evaluation MLM *****
06/27 02:01:50 PM   Epoch = 2 iter 9 step
06/27 02:01:50 PM   Num examples = 16
06/27 02:01:50 PM   Batch size = 32
06/27 02:01:51 PM ***** Eval results *****
06/27 02:01:51 PM   acc = 0.5
06/27 02:01:51 PM   cls_loss = 0.09853427857160568
06/27 02:01:51 PM   eval_loss = 1.559274673461914
06/27 02:01:51 PM   global_step = 9
06/27 02:01:51 PM   loss = 0.09853427857160568
06/27 02:01:51 PM ***** LOSS printing *****
06/27 02:01:51 PM loss
06/27 02:01:51 PM tensor(1.5001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:51 PM ***** LOSS printing *****
06/27 02:01:51 PM loss
06/27 02:01:51 PM tensor(2.8918, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:51 PM ***** LOSS printing *****
06/27 02:01:51 PM loss
06/27 02:01:51 PM tensor(0.4459, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:51 PM ***** LOSS printing *****
06/27 02:01:51 PM loss
06/27 02:01:51 PM tensor(0.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:52 PM ***** LOSS printing *****
06/27 02:01:52 PM loss
06/27 02:01:52 PM tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:52 PM ***** Running evaluation MLM *****
06/27 02:01:52 PM   Epoch = 3 iter 14 step
06/27 02:01:52 PM   Num examples = 16
06/27 02:01:52 PM   Batch size = 32
06/27 02:01:52 PM ***** Eval results *****
06/27 02:01:52 PM   acc = 0.8125
06/27 02:01:52 PM   cls_loss = 0.07219329080544412
06/27 02:01:52 PM   eval_loss = 1.1166789531707764
06/27 02:01:52 PM   global_step = 14
06/27 02:01:52 PM   loss = 0.07219329080544412
06/27 02:01:52 PM ***** LOSS printing *****
06/27 02:01:52 PM loss
06/27 02:01:52 PM tensor(0.0008, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:52 PM ***** LOSS printing *****
06/27 02:01:52 PM loss
06/27 02:01:52 PM tensor(0.0018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:53 PM ***** LOSS printing *****
06/27 02:01:53 PM loss
06/27 02:01:53 PM tensor(0.0015, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:53 PM ***** LOSS printing *****
06/27 02:01:53 PM loss
06/27 02:01:53 PM tensor(0.0089, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:53 PM ***** LOSS printing *****
06/27 02:01:53 PM loss
06/27 02:01:53 PM tensor(0.0038, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:53 PM ***** Running evaluation MLM *****
06/27 02:01:53 PM   Epoch = 4 iter 19 step
06/27 02:01:53 PM   Num examples = 16
06/27 02:01:53 PM   Batch size = 32
06/27 02:01:54 PM ***** Eval results *****
06/27 02:01:54 PM   acc = 0.8125
06/27 02:01:54 PM   cls_loss = 0.004736077467290063
06/27 02:01:54 PM   eval_loss = 2.9616265296936035
06/27 02:01:54 PM   global_step = 19
06/27 02:01:54 PM   loss = 0.004736077467290063
06/27 02:01:54 PM ***** LOSS printing *****
06/27 02:01:54 PM loss
06/27 02:01:54 PM tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:54 PM ***** LOSS printing *****
06/27 02:01:54 PM loss
06/27 02:01:54 PM tensor(0.0009, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:54 PM ***** LOSS printing *****
06/27 02:01:54 PM loss
06/27 02:01:54 PM tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:55 PM ***** LOSS printing *****
06/27 02:01:55 PM loss
06/27 02:01:55 PM tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:55 PM ***** LOSS printing *****
06/27 02:01:55 PM loss
06/27 02:01:55 PM tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:55 PM ***** Running evaluation MLM *****
06/27 02:01:55 PM   Epoch = 5 iter 24 step
06/27 02:01:55 PM   Num examples = 16
06/27 02:01:55 PM   Batch size = 32
06/27 02:01:55 PM ***** Eval results *****
06/27 02:01:55 PM   acc = 0.625
06/27 02:01:55 PM   cls_loss = 0.0006927736685611308
06/27 02:01:55 PM   eval_loss = 7.045936584472656
06/27 02:01:55 PM   global_step = 24
06/27 02:01:55 PM   loss = 0.0006927736685611308
06/27 02:01:55 PM ***** LOSS printing *****
06/27 02:01:55 PM loss
06/27 02:01:55 PM tensor(7.8883e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:56 PM ***** LOSS printing *****
06/27 02:01:56 PM loss
06/27 02:01:56 PM tensor(1.2159e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:56 PM ***** LOSS printing *****
06/27 02:01:56 PM loss
06/27 02:01:56 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:56 PM ***** LOSS printing *****
06/27 02:01:56 PM loss
06/27 02:01:56 PM tensor(8.6719e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:56 PM ***** LOSS printing *****
06/27 02:01:56 PM loss
06/27 02:01:56 PM tensor(4.6102e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:57 PM ***** Running evaluation MLM *****
06/27 02:01:57 PM   Epoch = 7 iter 29 step
06/27 02:01:57 PM   Num examples = 16
06/27 02:01:57 PM   Batch size = 32
06/27 02:01:57 PM ***** Eval results *****
06/27 02:01:57 PM   acc = 0.625
06/27 02:01:57 PM   cls_loss = 4.61024246760644e-05
06/27 02:01:57 PM   eval_loss = 7.783500671386719
06/27 02:01:57 PM   global_step = 29
06/27 02:01:57 PM   loss = 4.61024246760644e-05
06/27 02:01:57 PM ***** LOSS printing *****
06/27 02:01:57 PM loss
06/27 02:01:57 PM tensor(0.0064, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:57 PM ***** LOSS printing *****
06/27 02:01:57 PM loss
06/27 02:01:57 PM tensor(4.1245e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:58 PM ***** LOSS printing *****
06/27 02:01:58 PM loss
06/27 02:01:58 PM tensor(4.9142e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:58 PM ***** LOSS printing *****
06/27 02:01:58 PM loss
06/27 02:01:58 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:58 PM ***** LOSS printing *****
06/27 02:01:58 PM loss
06/27 02:01:58 PM tensor(2.6047e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:58 PM ***** Running evaluation MLM *****
06/27 02:01:58 PM   Epoch = 8 iter 34 step
06/27 02:01:58 PM   Num examples = 16
06/27 02:01:58 PM   Batch size = 32
06/27 02:01:59 PM ***** Eval results *****
06/27 02:01:59 PM   acc = 0.6875
06/27 02:01:59 PM   cls_loss = 0.0001212573743032408
06/27 02:01:59 PM   eval_loss = 5.463688373565674
06/27 02:01:59 PM   global_step = 34
06/27 02:01:59 PM   loss = 0.0001212573743032408
06/27 02:01:59 PM ***** LOSS printing *****
06/27 02:01:59 PM loss
06/27 02:01:59 PM tensor(2.1398e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:59 PM ***** LOSS printing *****
06/27 02:01:59 PM loss
06/27 02:01:59 PM tensor(3.3467e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:59 PM ***** LOSS printing *****
06/27 02:01:59 PM loss
06/27 02:01:59 PM tensor(4.1334e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:01:59 PM ***** LOSS printing *****
06/27 02:01:59 PM loss
06/27 02:01:59 PM tensor(1.2219e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:02:00 PM ***** LOSS printing *****
06/27 02:02:00 PM loss
06/27 02:02:00 PM tensor(1.0252e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:02:00 PM ***** Running evaluation MLM *****
06/27 02:02:00 PM   Epoch = 9 iter 39 step
06/27 02:02:00 PM   Num examples = 16
06/27 02:02:00 PM   Batch size = 32
06/27 02:02:00 PM ***** Eval results *****
06/27 02:02:00 PM   acc = 0.75
06/27 02:02:00 PM   cls_loss = 2.1268334724785138e-05
06/27 02:02:00 PM   eval_loss = 4.18977165222168
06/27 02:02:00 PM   global_step = 39
06/27 02:02:00 PM   loss = 2.1268334724785138e-05
06/27 02:02:00 PM ***** LOSS printing *****
06/27 02:02:00 PM loss
06/27 02:02:00 PM tensor(2.2858e-05, device='cuda:0', grad_fn=<NllLossBackward0>)

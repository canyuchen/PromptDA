06/27 02:12:25 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-13/', data_seed=13, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 02:12:25 PM device: cuda n_gpu: 1
06/27 02:12:25 PM Writing example 0 of 16
06/27 02:12:25 PM *** Example ***
06/27 02:12:25 PM guid: train-1
06/27 02:12:25 PM tokens: <s> the Ġtouch Ġbuttons Ġare Ġtext ured Ġso Ġthat Ġyour Ġfinger Ġs don Ġ' t Ġslip Ġwhich Ġwas Ġvery Ġthoughtful Ġ. </s> ĠIt Ġis <mask>
06/27 02:12:25 PM input_ids: 0 627 2842 14893 32 2788 4075 98 14 110 8411 579 7254 128 90 9215 61 21 182 16801 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM label: ['Ġpositive']
06/27 02:12:25 PM Writing example 0 of 16
06/27 02:12:25 PM *** Example ***
06/27 02:12:25 PM guid: dev-1
06/27 02:12:25 PM tokens: <s> " all Ġthe Ġ"" Ġcool Ġ"" Ġfeatures Ġof Ġthe Ġmini Ġ, Ġbut Ġwith Ġ20 gb Ġinstead Ġof Ġ5 Ġ." </s> ĠIt Ġis <mask>
06/27 02:12:25 PM input_ids: 0 113 1250 5 41039 3035 41039 1575 9 5 7983 2156 53 19 291 19562 1386 9 195 39058 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM label: ['Ġpositive']
06/27 02:12:25 PM Writing example 0 of 2000
06/27 02:12:25 PM *** Example ***
06/27 02:12:25 PM guid: dev-1
06/27 02:12:25 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 02:12:25 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 02:12:25 PM label: ['Ġnegative']
06/27 02:12:38 PM ***** Running training *****
06/27 02:12:38 PM   Num examples = 16
06/27 02:12:38 PM   Batch size = 4
06/27 02:12:38 PM   Num steps = 40
06/27 02:12:38 PM n: embeddings.word_embeddings.weight
06/27 02:12:38 PM n: embeddings.position_embeddings.weight
06/27 02:12:38 PM n: embeddings.token_type_embeddings.weight
06/27 02:12:38 PM n: embeddings.LayerNorm.weight
06/27 02:12:38 PM n: embeddings.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.0.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.0.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.0.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.0.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.0.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.0.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.0.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.0.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.0.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.0.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.0.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.0.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.1.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.1.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.1.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.1.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.1.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.1.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.1.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.1.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.1.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.1.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.1.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.1.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.2.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.2.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.2.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.2.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.2.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.2.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.2.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.2.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.2.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.2.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.2.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.2.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.3.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.3.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.3.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.3.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.3.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.3.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.3.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.3.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.3.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.3.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.3.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.3.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.4.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.4.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.4.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.4.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.4.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.4.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.4.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.4.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.4.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.4.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.4.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.4.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.5.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.5.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.5.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.5.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.5.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.5.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.5.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.5.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.5.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.5.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.5.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.5.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.6.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.6.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.6.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.6.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.6.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.6.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.6.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.6.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.6.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.6.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.6.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.6.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.7.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.7.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.7.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.7.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.7.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.7.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.7.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.7.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.7.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.7.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.7.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.7.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.8.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.8.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.8.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.8.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.8.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.8.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.8.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.8.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.8.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.8.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.8.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.8.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.9.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.9.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.9.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.9.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.9.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.9.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.9.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.9.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.9.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.9.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.9.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.9.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.10.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.10.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.10.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.10.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.10.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.10.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.10.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.10.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.10.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.10.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.10.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.10.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.11.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.11.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.11.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.11.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.11.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.11.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.11.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.11.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.11.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.11.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.11.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.11.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.12.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.12.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.12.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.12.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.12.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.12.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.12.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.12.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.12.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.12.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.12.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.12.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.13.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.13.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.13.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.13.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.13.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.13.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.13.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.13.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.13.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.13.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.13.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.13.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.14.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.14.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.14.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.14.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.14.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.14.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.14.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.14.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.14.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.14.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.14.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.14.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.15.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.15.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.15.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.15.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.15.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.15.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.15.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.15.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.15.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.15.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.15.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.15.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.16.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.16.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.16.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.16.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.16.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.16.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.16.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.16.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.16.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.16.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.16.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.16.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.17.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.17.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.17.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.17.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.17.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.17.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.17.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.17.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.17.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.17.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.17.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.17.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.18.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.18.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.18.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.18.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.18.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.18.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.18.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.18.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.18.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.18.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.18.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.18.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.19.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.19.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.19.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.19.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.19.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.19.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.19.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.19.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.19.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.19.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.19.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.19.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.20.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.20.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.20.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.20.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.20.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.20.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.20.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.20.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.20.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.20.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.20.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.20.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.21.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.21.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.21.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.21.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.21.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.21.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.21.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.21.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.21.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.21.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.21.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.21.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.22.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.22.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.22.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.22.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.22.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.22.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.22.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.22.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.22.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.22.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.22.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.22.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.23.attention.self.query.weight
06/27 02:12:38 PM n: encoder.layer.23.attention.self.query.bias
06/27 02:12:38 PM n: encoder.layer.23.attention.self.key.weight
06/27 02:12:38 PM n: encoder.layer.23.attention.self.key.bias
06/27 02:12:38 PM n: encoder.layer.23.attention.self.value.weight
06/27 02:12:38 PM n: encoder.layer.23.attention.self.value.bias
06/27 02:12:38 PM n: encoder.layer.23.attention.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.23.attention.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: encoder.layer.23.intermediate.dense.weight
06/27 02:12:38 PM n: encoder.layer.23.intermediate.dense.bias
06/27 02:12:38 PM n: encoder.layer.23.output.dense.weight
06/27 02:12:38 PM n: encoder.layer.23.output.dense.bias
06/27 02:12:38 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 02:12:38 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 02:12:38 PM n: pooler.dense.weight
06/27 02:12:38 PM n: pooler.dense.bias
06/27 02:12:38 PM n: roberta.embeddings.word_embeddings.weight
06/27 02:12:38 PM n: roberta.embeddings.position_embeddings.weight
06/27 02:12:38 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 02:12:38 PM n: roberta.embeddings.LayerNorm.weight
06/27 02:12:38 PM n: roberta.embeddings.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 02:12:38 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 02:12:38 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 02:12:38 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 02:12:38 PM n: roberta.pooler.dense.weight
06/27 02:12:38 PM n: roberta.pooler.dense.bias
06/27 02:12:38 PM n: lm_head.bias
06/27 02:12:38 PM n: lm_head.dense.weight
06/27 02:12:38 PM n: lm_head.dense.bias
06/27 02:12:38 PM n: lm_head.layer_norm.weight
06/27 02:12:38 PM n: lm_head.layer_norm.bias
06/27 02:12:38 PM n: lm_head.decoder.weight
06/27 02:12:38 PM Total parameters: 763292761
06/27 02:12:38 PM ***** LOSS printing *****
06/27 02:12:38 PM loss
06/27 02:12:38 PM tensor(19.0127, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:39 PM ***** LOSS printing *****
06/27 02:12:39 PM loss
06/27 02:12:39 PM tensor(13.8873, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:39 PM ***** LOSS printing *****
06/27 02:12:39 PM loss
06/27 02:12:39 PM tensor(7.1397, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:39 PM ***** LOSS printing *****
06/27 02:12:39 PM loss
06/27 02:12:39 PM tensor(2.7147, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:12:39 PM ***** Running evaluation MLM *****
06/27 02:12:39 PM   Epoch = 0 iter 4 step
06/27 02:12:39 PM   Num examples = 16
06/27 02:12:39 PM   Batch size = 32
06/27 02:12:40 PM ***** Eval results *****
06/27 02:12:40 PM   acc = 0.625
06/27 02:12:40 PM   cls_loss = 10.688608050346375
06/27 02:12:40 PM   eval_loss = 0.7114540934562683
06/27 02:12:40 PM   global_step = 4
06/27 02:12:40 PM   loss = 10.688608050346375
06/27 02:12:40 PM ***** Save model *****
06/27 02:12:40 PM ***** Test Dataset Eval Result *****
06/27 02:13:44 PM ***** Eval results *****
06/27 02:13:44 PM   acc = 0.553
06/27 02:13:44 PM   cls_loss = 10.688608050346375
06/27 02:13:44 PM   eval_loss = 0.7389147376257276
06/27 02:13:44 PM   global_step = 4
06/27 02:13:44 PM   loss = 10.688608050346375
06/27 02:13:48 PM ***** LOSS printing *****
06/27 02:13:48 PM loss
06/27 02:13:48 PM tensor(0.8492, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:49 PM ***** LOSS printing *****
06/27 02:13:49 PM loss
06/27 02:13:49 PM tensor(0.3986, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:49 PM ***** LOSS printing *****
06/27 02:13:49 PM loss
06/27 02:13:49 PM tensor(0.5611, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:49 PM ***** LOSS printing *****
06/27 02:13:49 PM loss
06/27 02:13:49 PM tensor(0.6406, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:49 PM ***** LOSS printing *****
06/27 02:13:49 PM loss
06/27 02:13:49 PM tensor(0.0722, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:49 PM ***** Running evaluation MLM *****
06/27 02:13:49 PM   Epoch = 2 iter 9 step
06/27 02:13:49 PM   Num examples = 16
06/27 02:13:49 PM   Batch size = 32
06/27 02:13:50 PM ***** Eval results *****
06/27 02:13:50 PM   acc = 0.5
06/27 02:13:50 PM   cls_loss = 0.07215738296508789
06/27 02:13:50 PM   eval_loss = 2.545055627822876
06/27 02:13:50 PM   global_step = 9
06/27 02:13:50 PM   loss = 0.07215738296508789
06/27 02:13:50 PM ***** LOSS printing *****
06/27 02:13:50 PM loss
06/27 02:13:50 PM tensor(2.0384, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:50 PM ***** LOSS printing *****
06/27 02:13:50 PM loss
06/27 02:13:50 PM tensor(4.7171, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:50 PM ***** LOSS printing *****
06/27 02:13:50 PM loss
06/27 02:13:50 PM tensor(2.2777, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:51 PM ***** LOSS printing *****
06/27 02:13:51 PM loss
06/27 02:13:51 PM tensor(0.3720, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:51 PM ***** LOSS printing *****
06/27 02:13:51 PM loss
06/27 02:13:51 PM tensor(0.0640, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:13:51 PM ***** Running evaluation MLM *****
06/27 02:13:51 PM   Epoch = 3 iter 14 step
06/27 02:13:51 PM   Num examples = 16
06/27 02:13:51 PM   Batch size = 32
06/27 02:13:52 PM ***** Eval results *****
06/27 02:13:52 PM   acc = 1.0
06/27 02:13:52 PM   cls_loss = 0.21799439191818237
06/27 02:13:52 PM   eval_loss = 0.09142135083675385
06/27 02:13:52 PM   global_step = 14
06/27 02:13:52 PM   loss = 0.21799439191818237
06/27 02:13:52 PM ***** Save model *****
06/27 02:13:52 PM ***** Test Dataset Eval Result *****
06/27 02:14:56 PM ***** Eval results *****
06/27 02:14:56 PM   acc = 0.907
06/27 02:14:56 PM   cls_loss = 0.21799439191818237
06/27 02:14:56 PM   eval_loss = 0.2715087157215864
06/27 02:14:56 PM   global_step = 14
06/27 02:14:56 PM   loss = 0.21799439191818237
06/27 02:15:00 PM ***** LOSS printing *****
06/27 02:15:00 PM loss
06/27 02:15:00 PM tensor(0.0182, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:00 PM ***** LOSS printing *****
06/27 02:15:00 PM loss
06/27 02:15:00 PM tensor(0.4014, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:00 PM ***** LOSS printing *****
06/27 02:15:00 PM loss
06/27 02:15:00 PM tensor(0.5139, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:00 PM ***** LOSS printing *****
06/27 02:15:00 PM loss
06/27 02:15:00 PM tensor(0.0642, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:01 PM ***** LOSS printing *****
06/27 02:15:01 PM loss
06/27 02:15:01 PM tensor(0.0121, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:01 PM ***** Running evaluation MLM *****
06/27 02:15:01 PM   Epoch = 4 iter 19 step
06/27 02:15:01 PM   Num examples = 16
06/27 02:15:01 PM   Batch size = 32
06/27 02:15:01 PM ***** Eval results *****
06/27 02:15:01 PM   acc = 1.0
06/27 02:15:01 PM   cls_loss = 0.19670419860631227
06/27 02:15:01 PM   eval_loss = 0.013740013353526592
06/27 02:15:01 PM   global_step = 19
06/27 02:15:01 PM   loss = 0.19670419860631227
06/27 02:15:01 PM ***** LOSS printing *****
06/27 02:15:01 PM loss
06/27 02:15:01 PM tensor(0.0021, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:02 PM ***** LOSS printing *****
06/27 02:15:02 PM loss
06/27 02:15:02 PM tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:02 PM ***** LOSS printing *****
06/27 02:15:02 PM loss
06/27 02:15:02 PM tensor(0.0024, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:02 PM ***** LOSS printing *****
06/27 02:15:02 PM loss
06/27 02:15:02 PM tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:02 PM ***** LOSS printing *****
06/27 02:15:02 PM loss
06/27 02:15:02 PM tensor(0.0010, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:02 PM ***** Running evaluation MLM *****
06/27 02:15:02 PM   Epoch = 5 iter 24 step
06/27 02:15:02 PM   Num examples = 16
06/27 02:15:02 PM   Batch size = 32
06/27 02:15:03 PM ***** Eval results *****
06/27 02:15:03 PM   acc = 1.0
06/27 02:15:03 PM   cls_loss = 0.0014104830188443884
06/27 02:15:03 PM   eval_loss = 0.006924029905349016
06/27 02:15:03 PM   global_step = 24
06/27 02:15:03 PM   loss = 0.0014104830188443884
06/27 02:15:03 PM ***** LOSS printing *****
06/27 02:15:03 PM loss
06/27 02:15:03 PM tensor(0.0007, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:03 PM ***** LOSS printing *****
06/27 02:15:03 PM loss
06/27 02:15:03 PM tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:03 PM ***** LOSS printing *****
06/27 02:15:03 PM loss
06/27 02:15:03 PM tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:04 PM ***** LOSS printing *****
06/27 02:15:04 PM loss
06/27 02:15:04 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:04 PM ***** LOSS printing *****
06/27 02:15:04 PM loss
06/27 02:15:04 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:04 PM ***** Running evaluation MLM *****
06/27 02:15:04 PM   Epoch = 7 iter 29 step
06/27 02:15:04 PM   Num examples = 16
06/27 02:15:04 PM   Batch size = 32
06/27 02:15:05 PM ***** Eval results *****
06/27 02:15:05 PM   acc = 0.875
06/27 02:15:05 PM   cls_loss = 0.00039846598519943655
06/27 02:15:05 PM   eval_loss = 0.6377606391906738
06/27 02:15:05 PM   global_step = 29
06/27 02:15:05 PM   loss = 0.00039846598519943655
06/27 02:15:05 PM ***** LOSS printing *****
06/27 02:15:05 PM loss
06/27 02:15:05 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:05 PM ***** LOSS printing *****
06/27 02:15:05 PM loss
06/27 02:15:05 PM tensor(2.2718, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:05 PM ***** LOSS printing *****
06/27 02:15:05 PM loss
06/27 02:15:05 PM tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:05 PM ***** LOSS printing *****
06/27 02:15:05 PM loss
06/27 02:15:05 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:06 PM ***** LOSS printing *****
06/27 02:15:06 PM loss
06/27 02:15:06 PM tensor(0.0006, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:06 PM ***** Running evaluation MLM *****
06/27 02:15:06 PM   Epoch = 8 iter 34 step
06/27 02:15:06 PM   Num examples = 16
06/27 02:15:06 PM   Batch size = 32
06/27 02:15:06 PM ***** Eval results *****
06/27 02:15:06 PM   acc = 1.0
06/27 02:15:06 PM   cls_loss = 0.0003846748877549544
06/27 02:15:06 PM   eval_loss = 0.000734752684365958
06/27 02:15:06 PM   global_step = 34
06/27 02:15:06 PM   loss = 0.0003846748877549544
06/27 02:15:06 PM ***** LOSS printing *****
06/27 02:15:06 PM loss
06/27 02:15:06 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:06 PM ***** LOSS printing *****
06/27 02:15:06 PM loss
06/27 02:15:06 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:07 PM ***** LOSS printing *****
06/27 02:15:07 PM loss
06/27 02:15:07 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:07 PM ***** LOSS printing *****
06/27 02:15:07 PM loss
06/27 02:15:07 PM tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:07 PM ***** LOSS printing *****
06/27 02:15:07 PM loss
06/27 02:15:07 PM tensor(0.0003, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 02:15:07 PM ***** Running evaluation MLM *****
06/27 02:15:07 PM   Epoch = 9 iter 39 step
06/27 02:15:07 PM   Num examples = 16
06/27 02:15:07 PM   Batch size = 32
06/27 02:15:08 PM ***** Eval results *****
06/27 02:15:08 PM   acc = 1.0
06/27 02:15:08 PM   cls_loss = 0.00036701552259425324
06/27 02:15:08 PM   eval_loss = 0.00032791070407256484
06/27 02:15:08 PM   global_step = 39
06/27 02:15:08 PM   loss = 0.00036701552259425324
06/27 02:15:08 PM ***** LOSS printing *****
06/27 02:15:08 PM loss
06/27 02:15:08 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)

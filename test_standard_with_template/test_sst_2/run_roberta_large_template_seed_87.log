06/27 01:52:48 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/SST-2/8-87/', data_seed=87, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='sst-2', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 01:52:48 PM device: cuda n_gpu: 1
06/27 01:52:48 PM Writing example 0 of 16
06/27 01:52:48 PM *** Example ***
06/27 01:52:48 PM guid: train-1
06/27 01:52:48 PM tokens: <s> the Ġsatire Ġis Ġunfocused Ġ, Ġwhile Ġthe Ġstory Ġgoes Ġnowhere Ġ. </s> ĠIt Ġis <mask>
06/27 01:52:48 PM input_ids: 0 627 31368 16 47306 2156 150 5 527 1411 9261 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM label: ['Ġnegative']
06/27 01:52:48 PM Writing example 0 of 16
06/27 01:52:48 PM *** Example ***
06/27 01:52:48 PM guid: dev-1
06/27 01:52:48 PM tokens: <s> norm ally Ġ, Ġro h mer Ġ' s Ġtalk y Ġfilms Ġfasc inate Ġme Ġ, Ġbut Ġwhen Ġhe Ġmoves Ġhis Ġsetting Ġto Ġthe Ġpast Ġ, Ġand Ġrelies Ġon Ġa Ġhistorical Ġtext Ġ, Ġhe Ġloses Ġthe Ġrichness Ġof Ġcharacterization Ġthat Ġmakes Ġhis Ġfilms Ġso Ġmemorable Ġ. </s> ĠIt Ġis <mask>
06/27 01:52:48 PM input_ids: 0 42258 2368 2156 4533 298 2089 128 29 1067 219 3541 35439 13014 162 2156 53 77 37 3136 39 2749 7 5 375 2156 8 12438 15 10 4566 2788 2156 37 13585 5 38857 9 34934 14 817 39 3541 98 10132 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM label: ['Ġnegative']
06/27 01:52:48 PM Writing example 0 of 872
06/27 01:52:48 PM *** Example ***
06/27 01:52:48 PM guid: dev-1
06/27 01:52:48 PM tokens: <s> one Ġlong Ġstring Ġof Ġcl ic hes Ġ. </s> ĠIt Ġis <mask>
06/27 01:52:48 PM input_ids: 0 1264 251 6755 9 3741 636 5065 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 01:52:48 PM label: ['Ġnegative']
06/27 01:53:01 PM ***** Running training *****
06/27 01:53:01 PM   Num examples = 16
06/27 01:53:01 PM   Batch size = 4
06/27 01:53:01 PM   Num steps = 40
06/27 01:53:01 PM n: embeddings.word_embeddings.weight
06/27 01:53:01 PM n: embeddings.position_embeddings.weight
06/27 01:53:01 PM n: embeddings.token_type_embeddings.weight
06/27 01:53:01 PM n: embeddings.LayerNorm.weight
06/27 01:53:01 PM n: embeddings.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.0.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.0.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.0.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.0.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.0.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.0.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.0.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.0.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.0.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.0.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.0.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.0.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.1.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.1.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.1.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.1.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.1.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.1.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.1.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.1.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.1.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.1.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.1.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.1.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.2.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.2.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.2.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.2.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.2.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.2.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.2.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.2.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.2.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.2.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.2.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.2.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.3.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.3.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.3.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.3.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.3.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.3.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.3.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.3.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.3.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.3.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.3.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.3.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.4.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.4.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.4.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.4.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.4.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.4.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.4.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.4.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.4.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.4.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.4.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.4.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.5.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.5.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.5.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.5.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.5.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.5.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.5.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.5.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.5.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.5.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.5.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.5.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.6.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.6.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.6.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.6.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.6.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.6.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.6.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.6.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.6.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.6.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.6.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.6.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.7.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.7.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.7.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.7.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.7.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.7.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.7.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.7.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.7.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.7.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.7.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.7.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.8.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.8.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.8.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.8.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.8.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.8.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.8.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.8.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.8.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.8.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.8.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.8.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.9.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.9.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.9.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.9.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.9.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.9.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.9.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.9.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.9.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.9.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.9.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.9.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.10.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.10.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.10.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.10.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.10.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.10.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.10.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.10.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.10.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.10.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.10.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.10.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.11.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.11.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.11.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.11.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.11.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.11.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.11.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.11.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.11.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.11.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.11.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.11.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.12.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.12.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.12.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.12.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.12.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.12.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.12.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.12.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.12.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.12.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.12.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.12.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.13.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.13.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.13.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.13.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.13.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.13.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.13.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.13.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.13.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.13.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.13.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.13.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.14.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.14.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.14.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.14.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.14.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.14.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.14.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.14.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.14.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.14.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.14.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.14.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.15.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.15.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.15.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.15.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.15.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.15.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.15.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.15.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.15.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.15.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.15.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.15.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.16.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.16.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.16.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.16.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.16.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.16.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.16.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.16.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.16.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.16.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.16.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.16.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.17.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.17.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.17.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.17.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.17.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.17.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.17.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.17.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.17.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.17.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.17.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.17.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.18.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.18.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.18.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.18.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.18.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.18.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.18.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.18.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.18.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.18.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.18.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.18.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.19.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.19.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.19.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.19.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.19.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.19.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.19.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.19.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.19.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.19.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.19.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.19.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.20.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.20.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.20.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.20.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.20.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.20.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.20.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.20.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.20.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.20.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.20.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.20.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.21.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.21.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.21.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.21.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.21.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.21.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.21.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.21.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.21.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.21.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.21.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.21.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.22.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.22.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.22.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.22.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.22.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.22.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.22.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.22.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.22.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.22.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.22.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.22.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.23.attention.self.query.weight
06/27 01:53:01 PM n: encoder.layer.23.attention.self.query.bias
06/27 01:53:01 PM n: encoder.layer.23.attention.self.key.weight
06/27 01:53:01 PM n: encoder.layer.23.attention.self.key.bias
06/27 01:53:01 PM n: encoder.layer.23.attention.self.value.weight
06/27 01:53:01 PM n: encoder.layer.23.attention.self.value.bias
06/27 01:53:01 PM n: encoder.layer.23.attention.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.23.attention.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: encoder.layer.23.intermediate.dense.weight
06/27 01:53:01 PM n: encoder.layer.23.intermediate.dense.bias
06/27 01:53:01 PM n: encoder.layer.23.output.dense.weight
06/27 01:53:01 PM n: encoder.layer.23.output.dense.bias
06/27 01:53:01 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 01:53:01 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 01:53:01 PM n: pooler.dense.weight
06/27 01:53:01 PM n: pooler.dense.bias
06/27 01:53:01 PM n: roberta.embeddings.word_embeddings.weight
06/27 01:53:01 PM n: roberta.embeddings.position_embeddings.weight
06/27 01:53:01 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 01:53:01 PM n: roberta.embeddings.LayerNorm.weight
06/27 01:53:01 PM n: roberta.embeddings.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 01:53:01 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 01:53:01 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 01:53:01 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 01:53:01 PM n: roberta.pooler.dense.weight
06/27 01:53:01 PM n: roberta.pooler.dense.bias
06/27 01:53:01 PM n: lm_head.bias
06/27 01:53:01 PM n: lm_head.dense.weight
06/27 01:53:01 PM n: lm_head.dense.bias
06/27 01:53:01 PM n: lm_head.layer_norm.weight
06/27 01:53:01 PM n: lm_head.layer_norm.bias
06/27 01:53:01 PM n: lm_head.decoder.weight
06/27 01:53:01 PM Total parameters: 763292761
06/27 01:53:01 PM ***** LOSS printing *****
06/27 01:53:01 PM loss
06/27 01:53:01 PM tensor(22.7871, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:01 PM ***** LOSS printing *****
06/27 01:53:01 PM loss
06/27 01:53:01 PM tensor(12.7808, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:02 PM ***** LOSS printing *****
06/27 01:53:02 PM loss
06/27 01:53:02 PM tensor(6.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:02 PM ***** LOSS printing *****
06/27 01:53:02 PM loss
06/27 01:53:02 PM tensor(2.0044, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:02 PM ***** Running evaluation MLM *****
06/27 01:53:02 PM   Epoch = 0 iter 4 step
06/27 01:53:02 PM   Num examples = 16
06/27 01:53:02 PM   Batch size = 32
06/27 01:53:03 PM ***** Eval results *****
06/27 01:53:03 PM   acc = 0.5
06/27 01:53:03 PM   cls_loss = 11.038055419921875
06/27 01:53:03 PM   eval_loss = 0.6719210147857666
06/27 01:53:03 PM   global_step = 4
06/27 01:53:03 PM   loss = 11.038055419921875
06/27 01:53:03 PM ***** Save model *****
06/27 01:53:03 PM ***** Test Dataset Eval Result *****
06/27 01:53:30 PM ***** Eval results *****
06/27 01:53:30 PM   acc = 0.7327981651376146
06/27 01:53:30 PM   cls_loss = 11.038055419921875
06/27 01:53:30 PM   eval_loss = 0.6221669103418078
06/27 01:53:30 PM   global_step = 4
06/27 01:53:30 PM   loss = 11.038055419921875
06/27 01:53:34 PM ***** LOSS printing *****
06/27 01:53:34 PM loss
06/27 01:53:34 PM tensor(0.4034, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:35 PM ***** LOSS printing *****
06/27 01:53:35 PM loss
06/27 01:53:35 PM tensor(0.6127, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:35 PM ***** LOSS printing *****
06/27 01:53:35 PM loss
06/27 01:53:35 PM tensor(0.6673, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:35 PM ***** LOSS printing *****
06/27 01:53:35 PM loss
06/27 01:53:35 PM tensor(0.5957, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:35 PM ***** LOSS printing *****
06/27 01:53:35 PM loss
06/27 01:53:35 PM tensor(0.2459, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:36 PM ***** Running evaluation MLM *****
06/27 01:53:36 PM   Epoch = 2 iter 9 step
06/27 01:53:36 PM   Num examples = 16
06/27 01:53:36 PM   Batch size = 32
06/27 01:53:36 PM ***** Eval results *****
06/27 01:53:36 PM   acc = 0.5
06/27 01:53:36 PM   cls_loss = 0.24592380225658417
06/27 01:53:36 PM   eval_loss = 2.056103229522705
06/27 01:53:36 PM   global_step = 9
06/27 01:53:36 PM   loss = 0.24592380225658417
06/27 01:53:36 PM ***** LOSS printing *****
06/27 01:53:36 PM loss
06/27 01:53:36 PM tensor(1.0363, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:36 PM ***** LOSS printing *****
06/27 01:53:36 PM loss
06/27 01:53:36 PM tensor(2.5811, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:37 PM ***** LOSS printing *****
06/27 01:53:37 PM loss
06/27 01:53:37 PM tensor(0.6585, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:37 PM ***** LOSS printing *****
06/27 01:53:37 PM loss
06/27 01:53:37 PM tensor(0.0169, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:37 PM ***** LOSS printing *****
06/27 01:53:37 PM loss
06/27 01:53:37 PM tensor(0.0372, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:53:37 PM ***** Running evaluation MLM *****
06/27 01:53:37 PM   Epoch = 3 iter 14 step
06/27 01:53:37 PM   Num examples = 16
06/27 01:53:37 PM   Batch size = 32
06/27 01:53:38 PM ***** Eval results *****
06/27 01:53:38 PM   acc = 0.9375
06/27 01:53:38 PM   cls_loss = 0.027039199136197567
06/27 01:53:38 PM   eval_loss = 0.2570230960845947
06/27 01:53:38 PM   global_step = 14
06/27 01:53:38 PM   loss = 0.027039199136197567
06/27 01:53:38 PM ***** Save model *****
06/27 01:53:38 PM ***** Test Dataset Eval Result *****
06/27 01:54:05 PM ***** Eval results *****
06/27 01:54:05 PM   acc = 0.9139908256880734
06/27 01:54:05 PM   cls_loss = 0.027039199136197567
06/27 01:54:05 PM   eval_loss = 0.47461520943657626
06/27 01:54:05 PM   global_step = 14
06/27 01:54:05 PM   loss = 0.027039199136197567
06/27 01:54:09 PM ***** LOSS printing *****
06/27 01:54:09 PM loss
06/27 01:54:09 PM tensor(0.0603, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:10 PM ***** LOSS printing *****
06/27 01:54:10 PM loss
06/27 01:54:10 PM tensor(0.0101, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:10 PM ***** LOSS printing *****
06/27 01:54:10 PM loss
06/27 01:54:10 PM tensor(0.0092, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:10 PM ***** LOSS printing *****
06/27 01:54:10 PM loss
06/27 01:54:10 PM tensor(0.0025, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:10 PM ***** LOSS printing *****
06/27 01:54:10 PM loss
06/27 01:54:10 PM tensor(0.0023, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:11 PM ***** Running evaluation MLM *****
06/27 01:54:11 PM   Epoch = 4 iter 19 step
06/27 01:54:11 PM   Num examples = 16
06/27 01:54:11 PM   Batch size = 32
06/27 01:54:11 PM ***** Eval results *****
06/27 01:54:11 PM   acc = 0.9375
06/27 01:54:11 PM   cls_loss = 0.0046723576573034125
06/27 01:54:11 PM   eval_loss = 0.525397539138794
06/27 01:54:11 PM   global_step = 19
06/27 01:54:11 PM   loss = 0.0046723576573034125
06/27 01:54:11 PM ***** LOSS printing *****
06/27 01:54:11 PM loss
06/27 01:54:11 PM tensor(0.0005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:11 PM ***** LOSS printing *****
06/27 01:54:11 PM loss
06/27 01:54:11 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:12 PM ***** LOSS printing *****
06/27 01:54:12 PM loss
06/27 01:54:12 PM tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:12 PM ***** LOSS printing *****
06/27 01:54:12 PM loss
06/27 01:54:12 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:12 PM ***** LOSS printing *****
06/27 01:54:12 PM loss
06/27 01:54:12 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:12 PM ***** Running evaluation MLM *****
06/27 01:54:12 PM   Epoch = 5 iter 24 step
06/27 01:54:12 PM   Num examples = 16
06/27 01:54:12 PM   Batch size = 32
06/27 01:54:13 PM ***** Eval results *****
06/27 01:54:13 PM   acc = 0.9375
06/27 01:54:13 PM   cls_loss = 0.0002897484373534098
06/27 01:54:13 PM   eval_loss = 0.5719443559646606
06/27 01:54:13 PM   global_step = 24
06/27 01:54:13 PM   loss = 0.0002897484373534098
06/27 01:54:13 PM ***** LOSS printing *****
06/27 01:54:13 PM loss
06/27 01:54:13 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:13 PM ***** LOSS printing *****
06/27 01:54:13 PM loss
06/27 01:54:13 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:13 PM ***** LOSS printing *****
06/27 01:54:13 PM loss
06/27 01:54:13 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:13 PM ***** LOSS printing *****
06/27 01:54:13 PM loss
06/27 01:54:13 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:14 PM ***** LOSS printing *****
06/27 01:54:14 PM loss
06/27 01:54:14 PM tensor(8.5171e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:14 PM ***** Running evaluation MLM *****
06/27 01:54:14 PM   Epoch = 7 iter 29 step
06/27 01:54:14 PM   Num examples = 16
06/27 01:54:14 PM   Batch size = 32
06/27 01:54:14 PM ***** Eval results *****
06/27 01:54:14 PM   acc = 0.9375
06/27 01:54:14 PM   cls_loss = 8.517089736415073e-05
06/27 01:54:14 PM   eval_loss = 0.8188021779060364
06/27 01:54:14 PM   global_step = 29
06/27 01:54:14 PM   loss = 8.517089736415073e-05
06/27 01:54:14 PM ***** LOSS printing *****
06/27 01:54:14 PM loss
06/27 01:54:14 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:15 PM ***** LOSS printing *****
06/27 01:54:15 PM loss
06/27 01:54:15 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:15 PM ***** LOSS printing *****
06/27 01:54:15 PM loss
06/27 01:54:15 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:15 PM ***** LOSS printing *****
06/27 01:54:15 PM loss
06/27 01:54:15 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:15 PM ***** LOSS printing *****
06/27 01:54:15 PM loss
06/27 01:54:15 PM tensor(9.7805e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:15 PM ***** Running evaluation MLM *****
06/27 01:54:15 PM   Epoch = 8 iter 34 step
06/27 01:54:15 PM   Num examples = 16
06/27 01:54:15 PM   Batch size = 32
06/27 01:54:16 PM ***** Eval results *****
06/27 01:54:16 PM   acc = 0.9375
06/27 01:54:16 PM   cls_loss = 0.0001266494691662956
06/27 01:54:16 PM   eval_loss = 0.9161033034324646
06/27 01:54:16 PM   global_step = 34
06/27 01:54:16 PM   loss = 0.0001266494691662956
06/27 01:54:16 PM ***** LOSS printing *****
06/27 01:54:16 PM loss
06/27 01:54:16 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:16 PM ***** LOSS printing *****
06/27 01:54:16 PM loss
06/27 01:54:16 PM tensor(7.7750e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:16 PM ***** LOSS printing *****
06/27 01:54:16 PM loss
06/27 01:54:16 PM tensor(7.9062e-05, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:17 PM ***** LOSS printing *****
06/27 01:54:17 PM loss
06/27 01:54:17 PM tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:17 PM ***** LOSS printing *****
06/27 01:54:17 PM loss
06/27 01:54:17 PM tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 01:54:17 PM ***** Running evaluation MLM *****
06/27 01:54:17 PM   Epoch = 9 iter 39 step
06/27 01:54:17 PM   Num examples = 16
06/27 01:54:17 PM   Batch size = 32
06/27 01:54:17 PM ***** Eval results *****
06/27 01:54:17 PM   acc = 0.9375
06/27 01:54:17 PM   cls_loss = 0.00012977715232409537
06/27 01:54:17 PM   eval_loss = 0.9627607464790344
06/27 01:54:17 PM   global_step = 39
06/27 01:54:17 PM   loss = 0.00012977715232409537
06/27 01:54:18 PM ***** LOSS printing *****
06/27 01:54:18 PM loss
06/27 01:54:18 PM tensor(6.8126e-05, device='cuda:0', grad_fn=<NllLossBackward0>)

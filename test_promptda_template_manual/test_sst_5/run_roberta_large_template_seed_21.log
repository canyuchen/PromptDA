06/27 09:05:23 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/sst-5/8-21/', data_seed=21, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='sst-5', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 09:05:23 PM device: cuda n_gpu: 1
06/27 09:05:23 PM Writing example 0 of 120
06/27 09:05:23 PM *** Example ***
06/27 09:05:23 PM guid: train-1
06/27 09:05:23 PM tokens: <s> it Ġhas Ġthat Ġrare Ġquality Ġof Ġbeing Ġable Ġto Ġcreep Ġthe Ġliving Ġhell Ġout Ġof Ġyou Ġ... </s> ĠIt Ġis <mask>
06/27 09:05:23 PM input_ids: 0 405 34 14 3159 1318 9 145 441 7 26637 5 1207 7105 66 9 47 1666 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM label: ['Ġgreat']
06/27 09:05:23 PM Writing example 0 of 40
06/27 09:05:23 PM *** Example ***
06/27 09:05:23 PM guid: dev-1
06/27 09:05:23 PM tokens: <s> fine ly Ġcrafted Ġ, Ġfinely Ġwritten Ġ, Ġexqu is itely Ġperformed </s> ĠIt Ġis <mask>
06/27 09:05:23 PM input_ids: 0 35093 352 17626 2156 29535 1982 2156 44477 354 41776 3744 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM label: ['Ġgreat']
06/27 09:05:23 PM Writing example 0 of 2210
06/27 09:05:23 PM *** Example ***
06/27 09:05:23 PM guid: dev-1
06/27 09:05:23 PM tokens: <s> no Ġmovement Ġ, Ġno Ġy u ks Ġ, Ġnot Ġmuch Ġof Ġanything Ġ. </s> ĠIt Ġis <mask>
06/27 09:05:23 PM input_ids: 0 2362 2079 2156 117 1423 257 2258 2156 45 203 9 932 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 09:05:23 PM label: ['Ġbad']
06/27 09:05:36 PM ***** Running training *****
06/27 09:05:36 PM   Num examples = 120
06/27 09:05:36 PM   Batch size = 4
06/27 09:05:36 PM   Num steps = 300
06/27 09:05:36 PM n: embeddings.word_embeddings.weight
06/27 09:05:36 PM n: embeddings.position_embeddings.weight
06/27 09:05:36 PM n: embeddings.token_type_embeddings.weight
06/27 09:05:36 PM n: embeddings.LayerNorm.weight
06/27 09:05:36 PM n: embeddings.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.0.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.0.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.0.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.0.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.0.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.0.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.0.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.0.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.0.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.0.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.0.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.0.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.1.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.1.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.1.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.1.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.1.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.1.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.1.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.1.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.1.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.1.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.1.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.1.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.2.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.2.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.2.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.2.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.2.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.2.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.2.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.2.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.2.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.2.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.2.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.2.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.3.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.3.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.3.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.3.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.3.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.3.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.3.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.3.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.3.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.3.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.3.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.3.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.4.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.4.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.4.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.4.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.4.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.4.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.4.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.4.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.4.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.4.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.4.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.4.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.5.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.5.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.5.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.5.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.5.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.5.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.5.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.5.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.5.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.5.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.5.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.5.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.6.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.6.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.6.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.6.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.6.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.6.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.6.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.6.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.6.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.6.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.6.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.6.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.7.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.7.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.7.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.7.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.7.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.7.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.7.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.7.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.7.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.7.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.7.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.7.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.8.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.8.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.8.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.8.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.8.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.8.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.8.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.8.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.8.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.8.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.8.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.8.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.9.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.9.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.9.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.9.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.9.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.9.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.9.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.9.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.9.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.9.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.9.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.9.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.10.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.10.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.10.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.10.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.10.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.10.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.10.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.10.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.10.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.10.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.10.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.10.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.11.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.11.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.11.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.11.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.11.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.11.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.11.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.11.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.11.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.11.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.11.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.11.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.12.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.12.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.12.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.12.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.12.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.12.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.12.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.12.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.12.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.12.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.12.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.12.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.13.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.13.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.13.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.13.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.13.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.13.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.13.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.13.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.13.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.13.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.13.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.13.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.14.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.14.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.14.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.14.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.14.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.14.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.14.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.14.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.14.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.14.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.14.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.14.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.15.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.15.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.15.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.15.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.15.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.15.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.15.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.15.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.15.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.15.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.15.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.15.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.16.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.16.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.16.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.16.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.16.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.16.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.16.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.16.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.16.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.16.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.16.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.16.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.17.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.17.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.17.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.17.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.17.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.17.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.17.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.17.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.17.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.17.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.17.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.17.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.18.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.18.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.18.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.18.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.18.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.18.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.18.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.18.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.18.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.18.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.18.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.18.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.19.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.19.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.19.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.19.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.19.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.19.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.19.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.19.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.19.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.19.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.19.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.19.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.20.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.20.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.20.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.20.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.20.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.20.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.20.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.20.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.20.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.20.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.20.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.20.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.21.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.21.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.21.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.21.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.21.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.21.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.21.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.21.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.21.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.21.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.21.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.21.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.22.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.22.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.22.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.22.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.22.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.22.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.22.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.22.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.22.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.22.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.22.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.22.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.23.attention.self.query.weight
06/27 09:05:36 PM n: encoder.layer.23.attention.self.query.bias
06/27 09:05:36 PM n: encoder.layer.23.attention.self.key.weight
06/27 09:05:36 PM n: encoder.layer.23.attention.self.key.bias
06/27 09:05:36 PM n: encoder.layer.23.attention.self.value.weight
06/27 09:05:36 PM n: encoder.layer.23.attention.self.value.bias
06/27 09:05:36 PM n: encoder.layer.23.attention.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.23.attention.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: encoder.layer.23.intermediate.dense.weight
06/27 09:05:36 PM n: encoder.layer.23.intermediate.dense.bias
06/27 09:05:36 PM n: encoder.layer.23.output.dense.weight
06/27 09:05:36 PM n: encoder.layer.23.output.dense.bias
06/27 09:05:36 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 09:05:36 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 09:05:36 PM n: pooler.dense.weight
06/27 09:05:36 PM n: pooler.dense.bias
06/27 09:05:36 PM n: roberta.embeddings.word_embeddings.weight
06/27 09:05:36 PM n: roberta.embeddings.position_embeddings.weight
06/27 09:05:36 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 09:05:36 PM n: roberta.embeddings.LayerNorm.weight
06/27 09:05:36 PM n: roberta.embeddings.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 09:05:36 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 09:05:36 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 09:05:36 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 09:05:36 PM n: roberta.pooler.dense.weight
06/27 09:05:36 PM n: roberta.pooler.dense.bias
06/27 09:05:36 PM n: lm_head.bias
06/27 09:05:36 PM n: lm_head.dense.weight
06/27 09:05:36 PM n: lm_head.dense.bias
06/27 09:05:36 PM n: lm_head.layer_norm.weight
06/27 09:05:36 PM n: lm_head.layer_norm.bias
06/27 09:05:36 PM n: lm_head.decoder.weight
06/27 09:05:36 PM Total parameters: 763292761
06/27 09:05:36 PM ***** LOSS printing *****
06/27 09:05:36 PM loss
06/27 09:05:36 PM tensor(19.0133, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:37 PM ***** LOSS printing *****
06/27 09:05:37 PM loss
06/27 09:05:37 PM tensor(15.5636, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:37 PM ***** LOSS printing *****
06/27 09:05:37 PM loss
06/27 09:05:37 PM tensor(9.6866, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:37 PM ***** LOSS printing *****
06/27 09:05:37 PM loss
06/27 09:05:37 PM tensor(9.1587, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:37 PM ***** Running evaluation MLM *****
06/27 09:05:37 PM   Epoch = 0 iter 4 step
06/27 09:05:37 PM   Num examples = 40
06/27 09:05:37 PM   Batch size = 32
06/27 09:05:39 PM ***** Eval results *****
06/27 09:05:39 PM   acc = 0.25
06/27 09:05:39 PM   cls_loss = 13.355529546737671
06/27 09:05:39 PM   eval_loss = 4.050859689712524
06/27 09:05:39 PM   global_step = 4
06/27 09:05:39 PM   loss = 13.355529546737671
06/27 09:05:39 PM ***** Save model *****
06/27 09:05:39 PM ***** Test Dataset Eval Result *****
06/27 09:06:48 PM ***** Eval results *****
06/27 09:06:48 PM   acc = 0.2647058823529412
06/27 09:06:48 PM   cls_loss = 13.355529546737671
06/27 09:06:48 PM   eval_loss = 4.408660442488534
06/27 09:06:48 PM   global_step = 4
06/27 09:06:48 PM   loss = 13.355529546737671
06/27 09:06:52 PM ***** LOSS printing *****
06/27 09:06:52 PM loss
06/27 09:06:52 PM tensor(6.1067, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:06:52 PM ***** LOSS printing *****
06/27 09:06:52 PM loss
06/27 09:06:52 PM tensor(3.6204, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:06:52 PM ***** LOSS printing *****
06/27 09:06:52 PM loss
06/27 09:06:52 PM tensor(5.4550, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:06:52 PM ***** LOSS printing *****
06/27 09:06:52 PM loss
06/27 09:06:52 PM tensor(4.7914, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:06:53 PM ***** LOSS printing *****
06/27 09:06:53 PM loss
06/27 09:06:53 PM tensor(6.0992, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:06:53 PM ***** Running evaluation MLM *****
06/27 09:06:53 PM   Epoch = 0 iter 9 step
06/27 09:06:53 PM   Num examples = 40
06/27 09:06:53 PM   Batch size = 32
06/27 09:06:54 PM ***** Eval results *****
06/27 09:06:54 PM   acc = 0.375
06/27 09:06:54 PM   cls_loss = 8.83275975121392
06/27 09:06:54 PM   eval_loss = 2.8314484357833862
06/27 09:06:54 PM   global_step = 9
06/27 09:06:54 PM   loss = 8.83275975121392
06/27 09:06:54 PM ***** Save model *****
06/27 09:06:54 PM ***** Test Dataset Eval Result *****
06/27 09:08:03 PM ***** Eval results *****
06/27 09:08:03 PM   acc = 0.33167420814479637
06/27 09:08:03 PM   cls_loss = 8.83275975121392
06/27 09:08:03 PM   eval_loss = 3.376728449548994
06/27 09:08:03 PM   global_step = 9
06/27 09:08:03 PM   loss = 8.83275975121392
06/27 09:08:07 PM ***** LOSS printing *****
06/27 09:08:07 PM loss
06/27 09:08:07 PM tensor(4.2715, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:08 PM ***** LOSS printing *****
06/27 09:08:08 PM loss
06/27 09:08:08 PM tensor(4.7718, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:08 PM ***** LOSS printing *****
06/27 09:08:08 PM loss
06/27 09:08:08 PM tensor(3.8145, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:08 PM ***** LOSS printing *****
06/27 09:08:08 PM loss
06/27 09:08:08 PM tensor(5.3176, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:08 PM ***** LOSS printing *****
06/27 09:08:08 PM loss
06/27 09:08:08 PM tensor(2.9658, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:08 PM ***** Running evaluation MLM *****
06/27 09:08:08 PM   Epoch = 0 iter 14 step
06/27 09:08:08 PM   Num examples = 40
06/27 09:08:08 PM   Batch size = 32
06/27 09:08:10 PM ***** Eval results *****
06/27 09:08:10 PM   acc = 0.3
06/27 09:08:10 PM   cls_loss = 7.188290340559823
06/27 09:08:10 PM   eval_loss = 3.8527709245681763
06/27 09:08:10 PM   global_step = 14
06/27 09:08:10 PM   loss = 7.188290340559823
06/27 09:08:10 PM ***** LOSS printing *****
06/27 09:08:10 PM loss
06/27 09:08:10 PM tensor(4.1860, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:10 PM ***** LOSS printing *****
06/27 09:08:10 PM loss
06/27 09:08:10 PM tensor(3.4724, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:10 PM ***** LOSS printing *****
06/27 09:08:10 PM loss
06/27 09:08:10 PM tensor(3.7066, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:10 PM ***** LOSS printing *****
06/27 09:08:10 PM loss
06/27 09:08:10 PM tensor(3.2641, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:11 PM ***** LOSS printing *****
06/27 09:08:11 PM loss
06/27 09:08:11 PM tensor(2.8773, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:11 PM ***** Running evaluation MLM *****
06/27 09:08:11 PM   Epoch = 0 iter 19 step
06/27 09:08:11 PM   Num examples = 40
06/27 09:08:11 PM   Batch size = 32
06/27 09:08:12 PM ***** Eval results *****
06/27 09:08:12 PM   acc = 0.35
06/27 09:08:12 PM   cls_loss = 6.218022584915161
06/27 09:08:12 PM   eval_loss = 2.734487771987915
06/27 09:08:12 PM   global_step = 19
06/27 09:08:12 PM   loss = 6.218022584915161
06/27 09:08:12 PM ***** LOSS printing *****
06/27 09:08:12 PM loss
06/27 09:08:12 PM tensor(3.4607, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:12 PM ***** LOSS printing *****
06/27 09:08:12 PM loss
06/27 09:08:12 PM tensor(4.0540, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:13 PM ***** LOSS printing *****
06/27 09:08:13 PM loss
06/27 09:08:13 PM tensor(3.4666, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:13 PM ***** LOSS printing *****
06/27 09:08:13 PM loss
06/27 09:08:13 PM tensor(2.8442, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:13 PM ***** LOSS printing *****
06/27 09:08:13 PM loss
06/27 09:08:13 PM tensor(3.3651, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:08:13 PM ***** Running evaluation MLM *****
06/27 09:08:13 PM   Epoch = 0 iter 24 step
06/27 09:08:13 PM   Num examples = 40
06/27 09:08:13 PM   Batch size = 32
06/27 09:08:14 PM ***** Eval results *****
06/27 09:08:14 PM   acc = 0.4
06/27 09:08:14 PM   cls_loss = 5.63887036840121
06/27 09:08:14 PM   eval_loss = 3.0235438346862793
06/27 09:08:14 PM   global_step = 24
06/27 09:08:14 PM   loss = 5.63887036840121
06/27 09:08:14 PM ***** Save model *****
06/27 09:08:14 PM ***** Test Dataset Eval Result *****
06/27 09:09:24 PM ***** Eval results *****
06/27 09:09:24 PM   acc = 0.43846153846153846
06/27 09:09:24 PM   cls_loss = 5.63887036840121
06/27 09:09:24 PM   eval_loss = 3.2501830271312167
06/27 09:09:24 PM   global_step = 24
06/27 09:09:24 PM   loss = 5.63887036840121
06/27 09:09:28 PM ***** LOSS printing *****
06/27 09:09:28 PM loss
06/27 09:09:28 PM tensor(2.9496, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:09:28 PM ***** LOSS printing *****
06/27 09:09:28 PM loss
06/27 09:09:28 PM tensor(4.6742, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:09:28 PM ***** LOSS printing *****
06/27 09:09:28 PM loss
06/27 09:09:28 PM tensor(4.1508, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:09:28 PM ***** LOSS printing *****
06/27 09:09:28 PM loss
06/27 09:09:28 PM tensor(3.6885, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:09:29 PM ***** LOSS printing *****
06/27 09:09:29 PM loss
06/27 09:09:29 PM tensor(3.3789, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:09:29 PM ***** Running evaluation MLM *****
06/27 09:09:29 PM   Epoch = 0 iter 29 step
06/27 09:09:29 PM   Num examples = 40
06/27 09:09:29 PM   Batch size = 32
06/27 09:09:30 PM ***** Eval results *****
06/27 09:09:30 PM   acc = 0.55
06/27 09:09:30 PM   cls_loss = 5.3163753049126985
06/27 09:09:30 PM   eval_loss = 2.091812551021576
06/27 09:09:30 PM   global_step = 29
06/27 09:09:30 PM   loss = 5.3163753049126985
06/27 09:09:30 PM ***** Save model *****
06/27 09:09:30 PM ***** Test Dataset Eval Result *****
06/27 09:10:39 PM ***** Eval results *****
06/27 09:10:39 PM   acc = 0.4656108597285068
06/27 09:10:39 PM   cls_loss = 5.3163753049126985
06/27 09:10:39 PM   eval_loss = 2.49186943258558
06/27 09:10:39 PM   global_step = 29
06/27 09:10:39 PM   loss = 5.3163753049126985
06/27 09:10:43 PM ***** LOSS printing *****
06/27 09:10:43 PM loss
06/27 09:10:43 PM tensor(3.7508, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:44 PM ***** LOSS printing *****
06/27 09:10:44 PM loss
06/27 09:10:44 PM tensor(3.0823, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:44 PM ***** LOSS printing *****
06/27 09:10:44 PM loss
06/27 09:10:44 PM tensor(2.8869, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:44 PM ***** LOSS printing *****
06/27 09:10:44 PM loss
06/27 09:10:44 PM tensor(2.2598, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:44 PM ***** LOSS printing *****
06/27 09:10:44 PM loss
06/27 09:10:44 PM tensor(2.3850, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:45 PM ***** Running evaluation MLM *****
06/27 09:10:45 PM   Epoch = 1 iter 34 step
06/27 09:10:45 PM   Num examples = 40
06/27 09:10:45 PM   Batch size = 32
06/27 09:10:46 PM ***** Eval results *****
06/27 09:10:46 PM   acc = 0.25
06/27 09:10:46 PM   cls_loss = 2.653512418270111
06/27 09:10:46 PM   eval_loss = 2.786940336227417
06/27 09:10:46 PM   global_step = 34
06/27 09:10:46 PM   loss = 2.653512418270111
06/27 09:10:46 PM ***** LOSS printing *****
06/27 09:10:46 PM loss
06/27 09:10:46 PM tensor(3.1735, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:46 PM ***** LOSS printing *****
06/27 09:10:46 PM loss
06/27 09:10:46 PM tensor(2.0905, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:46 PM ***** LOSS printing *****
06/27 09:10:46 PM loss
06/27 09:10:46 PM tensor(2.1880, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:46 PM ***** LOSS printing *****
06/27 09:10:46 PM loss
06/27 09:10:46 PM tensor(1.8310, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:47 PM ***** LOSS printing *****
06/27 09:10:47 PM loss
06/27 09:10:47 PM tensor(2.3095, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:47 PM ***** Running evaluation MLM *****
06/27 09:10:47 PM   Epoch = 1 iter 39 step
06/27 09:10:47 PM   Num examples = 40
06/27 09:10:47 PM   Batch size = 32
06/27 09:10:48 PM ***** Eval results *****
06/27 09:10:48 PM   acc = 0.375
06/27 09:10:48 PM   cls_loss = 2.4673936631944446
06/27 09:10:48 PM   eval_loss = 2.509572982788086
06/27 09:10:48 PM   global_step = 39
06/27 09:10:48 PM   loss = 2.4673936631944446
06/27 09:10:48 PM ***** LOSS printing *****
06/27 09:10:48 PM loss
06/27 09:10:48 PM tensor(2.2852, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:48 PM ***** LOSS printing *****
06/27 09:10:48 PM loss
06/27 09:10:48 PM tensor(2.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:49 PM ***** LOSS printing *****
06/27 09:10:49 PM loss
06/27 09:10:49 PM tensor(3.7208, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:49 PM ***** LOSS printing *****
06/27 09:10:49 PM loss
06/27 09:10:49 PM tensor(3.0324, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:49 PM ***** LOSS printing *****
06/27 09:10:49 PM loss
06/27 09:10:49 PM tensor(2.9674, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:49 PM ***** Running evaluation MLM *****
06/27 09:10:49 PM   Epoch = 1 iter 44 step
06/27 09:10:49 PM   Num examples = 40
06/27 09:10:49 PM   Batch size = 32
06/27 09:10:51 PM ***** Eval results *****
06/27 09:10:51 PM   acc = 0.4
06/27 09:10:51 PM   cls_loss = 2.5922147376196727
06/27 09:10:51 PM   eval_loss = 2.892514228820801
06/27 09:10:51 PM   global_step = 44
06/27 09:10:51 PM   loss = 2.5922147376196727
06/27 09:10:51 PM ***** LOSS printing *****
06/27 09:10:51 PM loss
06/27 09:10:51 PM tensor(2.4019, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:51 PM ***** LOSS printing *****
06/27 09:10:51 PM loss
06/27 09:10:51 PM tensor(2.7942, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:51 PM ***** LOSS printing *****
06/27 09:10:51 PM loss
06/27 09:10:51 PM tensor(3.4144, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:51 PM ***** LOSS printing *****
06/27 09:10:51 PM loss
06/27 09:10:51 PM tensor(2.5681, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:51 PM ***** LOSS printing *****
06/27 09:10:51 PM loss
06/27 09:10:51 PM tensor(2.5081, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:52 PM ***** Running evaluation MLM *****
06/27 09:10:52 PM   Epoch = 1 iter 49 step
06/27 09:10:52 PM   Num examples = 40
06/27 09:10:52 PM   Batch size = 32
06/27 09:10:53 PM ***** Eval results *****
06/27 09:10:53 PM   acc = 0.5
06/27 09:10:53 PM   cls_loss = 2.630402138358668
06/27 09:10:53 PM   eval_loss = 2.0345041751861572
06/27 09:10:53 PM   global_step = 49
06/27 09:10:53 PM   loss = 2.630402138358668
06/27 09:10:53 PM ***** LOSS printing *****
06/27 09:10:53 PM loss
06/27 09:10:53 PM tensor(3.6507, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:53 PM ***** LOSS printing *****
06/27 09:10:53 PM loss
06/27 09:10:53 PM tensor(2.7673, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:53 PM ***** LOSS printing *****
06/27 09:10:53 PM loss
06/27 09:10:53 PM tensor(2.2076, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:54 PM ***** LOSS printing *****
06/27 09:10:54 PM loss
06/27 09:10:54 PM tensor(2.5132, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:54 PM ***** LOSS printing *****
06/27 09:10:54 PM loss
06/27 09:10:54 PM tensor(2.5647, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:54 PM ***** Running evaluation MLM *****
06/27 09:10:54 PM   Epoch = 1 iter 54 step
06/27 09:10:54 PM   Num examples = 40
06/27 09:10:54 PM   Batch size = 32
06/27 09:10:55 PM ***** Eval results *****
06/27 09:10:55 PM   acc = 0.5
06/27 09:10:55 PM   cls_loss = 2.653380195299784
06/27 09:10:55 PM   eval_loss = 2.1110728979110718
06/27 09:10:55 PM   global_step = 54
06/27 09:10:55 PM   loss = 2.653380195299784
06/27 09:10:55 PM ***** LOSS printing *****
06/27 09:10:55 PM loss
06/27 09:10:55 PM tensor(2.4107, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:55 PM ***** LOSS printing *****
06/27 09:10:55 PM loss
06/27 09:10:55 PM tensor(2.8086, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:56 PM ***** LOSS printing *****
06/27 09:10:56 PM loss
06/27 09:10:56 PM tensor(2.9641, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:56 PM ***** LOSS printing *****
06/27 09:10:56 PM loss
06/27 09:10:56 PM tensor(1.7822, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:56 PM ***** LOSS printing *****
06/27 09:10:56 PM loss
06/27 09:10:56 PM tensor(1.7201, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:56 PM ***** Running evaluation MLM *****
06/27 09:10:56 PM   Epoch = 1 iter 59 step
06/27 09:10:56 PM   Num examples = 40
06/27 09:10:56 PM   Batch size = 32
06/27 09:10:58 PM ***** Eval results *****
06/27 09:10:58 PM   acc = 0.475
06/27 09:10:58 PM   cls_loss = 2.5988516848662804
06/27 09:10:58 PM   eval_loss = 2.3297170400619507
06/27 09:10:58 PM   global_step = 59
06/27 09:10:58 PM   loss = 2.5988516848662804
06/27 09:10:58 PM ***** LOSS printing *****
06/27 09:10:58 PM loss
06/27 09:10:58 PM tensor(2.1342, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:58 PM ***** LOSS printing *****
06/27 09:10:58 PM loss
06/27 09:10:58 PM tensor(2.9112, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:58 PM ***** LOSS printing *****
06/27 09:10:58 PM loss
06/27 09:10:58 PM tensor(3.3164, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:58 PM ***** LOSS printing *****
06/27 09:10:58 PM loss
06/27 09:10:58 PM tensor(2.5869, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:59 PM ***** LOSS printing *****
06/27 09:10:59 PM loss
06/27 09:10:59 PM tensor(2.3915, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:10:59 PM ***** Running evaluation MLM *****
06/27 09:10:59 PM   Epoch = 2 iter 64 step
06/27 09:10:59 PM   Num examples = 40
06/27 09:10:59 PM   Batch size = 32
06/27 09:11:00 PM ***** Eval results *****
06/27 09:11:00 PM   acc = 0.4
06/27 09:11:00 PM   cls_loss = 2.8014920949935913
06/27 09:11:00 PM   eval_loss = 2.7000473737716675
06/27 09:11:00 PM   global_step = 64
06/27 09:11:00 PM   loss = 2.8014920949935913
06/27 09:11:00 PM ***** LOSS printing *****
06/27 09:11:00 PM loss
06/27 09:11:00 PM tensor(1.6279, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:00 PM ***** LOSS printing *****
06/27 09:11:00 PM loss
06/27 09:11:00 PM tensor(2.6116, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:00 PM ***** LOSS printing *****
06/27 09:11:00 PM loss
06/27 09:11:00 PM tensor(3.0830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:01 PM ***** LOSS printing *****
06/27 09:11:01 PM loss
06/27 09:11:01 PM tensor(2.6216, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:01 PM ***** LOSS printing *****
06/27 09:11:01 PM loss
06/27 09:11:01 PM tensor(1.6629, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:01 PM ***** Running evaluation MLM *****
06/27 09:11:01 PM   Epoch = 2 iter 69 step
06/27 09:11:01 PM   Num examples = 40
06/27 09:11:01 PM   Batch size = 32
06/27 09:11:02 PM ***** Eval results *****
06/27 09:11:02 PM   acc = 0.425
06/27 09:11:02 PM   cls_loss = 2.5347701576020985
06/27 09:11:02 PM   eval_loss = 2.4435619115829468
06/27 09:11:02 PM   global_step = 69
06/27 09:11:02 PM   loss = 2.5347701576020985
06/27 09:11:02 PM ***** LOSS printing *****
06/27 09:11:02 PM loss
06/27 09:11:02 PM tensor(2.3331, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:03 PM ***** LOSS printing *****
06/27 09:11:03 PM loss
06/27 09:11:03 PM tensor(2.8434, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:03 PM ***** LOSS printing *****
06/27 09:11:03 PM loss
06/27 09:11:03 PM tensor(2.5851, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:03 PM ***** LOSS printing *****
06/27 09:11:03 PM loss
06/27 09:11:03 PM tensor(2.1849, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:03 PM ***** LOSS printing *****
06/27 09:11:03 PM loss
06/27 09:11:03 PM tensor(2.5929, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:03 PM ***** Running evaluation MLM *****
06/27 09:11:03 PM   Epoch = 2 iter 74 step
06/27 09:11:03 PM   Num examples = 40
06/27 09:11:03 PM   Batch size = 32
06/27 09:11:05 PM ***** Eval results *****
06/27 09:11:05 PM   acc = 0.425
06/27 09:11:05 PM   cls_loss = 2.525162024157388
06/27 09:11:05 PM   eval_loss = 2.247091293334961
06/27 09:11:05 PM   global_step = 74
06/27 09:11:05 PM   loss = 2.525162024157388
06/27 09:11:05 PM ***** LOSS printing *****
06/27 09:11:05 PM loss
06/27 09:11:05 PM tensor(2.0271, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:05 PM ***** LOSS printing *****
06/27 09:11:05 PM loss
06/27 09:11:05 PM tensor(2.0043, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:05 PM ***** LOSS printing *****
06/27 09:11:05 PM loss
06/27 09:11:05 PM tensor(1.8805, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:05 PM ***** LOSS printing *****
06/27 09:11:05 PM loss
06/27 09:11:05 PM tensor(3.3167, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:06 PM ***** LOSS printing *****
06/27 09:11:06 PM loss
06/27 09:11:06 PM tensor(2.6493, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:06 PM ***** Running evaluation MLM *****
06/27 09:11:06 PM   Epoch = 2 iter 79 step
06/27 09:11:06 PM   Num examples = 40
06/27 09:11:06 PM   Batch size = 32
06/27 09:11:07 PM ***** Eval results *****
06/27 09:11:07 PM   acc = 0.4
06/27 09:11:07 PM   cls_loss = 2.485799795702884
06/27 09:11:07 PM   eval_loss = 2.3165178298950195
06/27 09:11:07 PM   global_step = 79
06/27 09:11:07 PM   loss = 2.485799795702884
06/27 09:11:07 PM ***** LOSS printing *****
06/27 09:11:07 PM loss
06/27 09:11:07 PM tensor(2.3215, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:07 PM ***** LOSS printing *****
06/27 09:11:07 PM loss
06/27 09:11:07 PM tensor(2.2228, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:08 PM ***** LOSS printing *****
06/27 09:11:08 PM loss
06/27 09:11:08 PM tensor(1.7463, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:08 PM ***** LOSS printing *****
06/27 09:11:08 PM loss
06/27 09:11:08 PM tensor(2.3301, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:08 PM ***** LOSS printing *****
06/27 09:11:08 PM loss
06/27 09:11:08 PM tensor(1.8446, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:08 PM ***** Running evaluation MLM *****
06/27 09:11:08 PM   Epoch = 2 iter 84 step
06/27 09:11:08 PM   Num examples = 40
06/27 09:11:08 PM   Batch size = 32
06/27 09:11:09 PM ***** Eval results *****
06/27 09:11:09 PM   acc = 0.45
06/27 09:11:09 PM   cls_loss = 2.4039780348539352
06/27 09:11:09 PM   eval_loss = 2.365526556968689
06/27 09:11:09 PM   global_step = 84
06/27 09:11:09 PM   loss = 2.4039780348539352
06/27 09:11:09 PM ***** LOSS printing *****
06/27 09:11:09 PM loss
06/27 09:11:09 PM tensor(1.4902, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:10 PM ***** LOSS printing *****
06/27 09:11:10 PM loss
06/27 09:11:10 PM tensor(2.1966, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:10 PM ***** LOSS printing *****
06/27 09:11:10 PM loss
06/27 09:11:10 PM tensor(2.2392, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:10 PM ***** LOSS printing *****
06/27 09:11:10 PM loss
06/27 09:11:10 PM tensor(2.4792, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:10 PM ***** LOSS printing *****
06/27 09:11:10 PM loss
06/27 09:11:10 PM tensor(2.7072, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:11:11 PM ***** Running evaluation MLM *****
06/27 09:11:11 PM   Epoch = 2 iter 89 step
06/27 09:11:11 PM   Num examples = 40
06/27 09:11:11 PM   Batch size = 32
06/27 09:11:12 PM ***** Eval results *****
06/27 09:11:12 PM   acc = 0.575
06/27 09:11:12 PM   cls_loss = 2.372687076700145
06/27 09:11:12 PM   eval_loss = 2.3053744435310364
06/27 09:11:12 PM   global_step = 89
06/27 09:11:12 PM   loss = 2.372687076700145
06/27 09:11:12 PM ***** Save model *****
06/27 09:11:12 PM ***** Test Dataset Eval Result *****
06/27 09:12:21 PM ***** Eval results *****
06/27 09:12:21 PM   acc = 0.45656108597285067
06/27 09:12:21 PM   cls_loss = 2.372687076700145
06/27 09:12:21 PM   eval_loss = 2.9827986342566355
06/27 09:12:21 PM   global_step = 89
06/27 09:12:21 PM   loss = 2.372687076700145
06/27 09:12:25 PM ***** LOSS printing *****
06/27 09:12:25 PM loss
06/27 09:12:25 PM tensor(1.9514, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:25 PM ***** LOSS printing *****
06/27 09:12:25 PM loss
06/27 09:12:25 PM tensor(1.1386, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:26 PM ***** LOSS printing *****
06/27 09:12:26 PM loss
06/27 09:12:26 PM tensor(1.9881, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:26 PM ***** LOSS printing *****
06/27 09:12:26 PM loss
06/27 09:12:26 PM tensor(1.9519, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:26 PM ***** LOSS printing *****
06/27 09:12:26 PM loss
06/27 09:12:26 PM tensor(3.0371, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:26 PM ***** Running evaluation MLM *****
06/27 09:12:26 PM   Epoch = 3 iter 94 step
06/27 09:12:26 PM   Num examples = 40
06/27 09:12:26 PM   Batch size = 32
06/27 09:12:28 PM ***** Eval results *****
06/27 09:12:28 PM   acc = 0.525
06/27 09:12:28 PM   cls_loss = 2.0289307236671448
06/27 09:12:28 PM   eval_loss = 2.209477961063385
06/27 09:12:28 PM   global_step = 94
06/27 09:12:28 PM   loss = 2.0289307236671448
06/27 09:12:28 PM ***** LOSS printing *****
06/27 09:12:28 PM loss
06/27 09:12:28 PM tensor(1.4095, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:28 PM ***** LOSS printing *****
06/27 09:12:28 PM loss
06/27 09:12:28 PM tensor(1.2951, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:28 PM ***** LOSS printing *****
06/27 09:12:28 PM loss
06/27 09:12:28 PM tensor(2.2267, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:28 PM ***** LOSS printing *****
06/27 09:12:28 PM loss
06/27 09:12:28 PM tensor(2.2772, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:28 PM ***** LOSS printing *****
06/27 09:12:28 PM loss
06/27 09:12:28 PM tensor(1.3563, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:29 PM ***** Running evaluation MLM *****
06/27 09:12:29 PM   Epoch = 3 iter 99 step
06/27 09:12:29 PM   Num examples = 40
06/27 09:12:29 PM   Batch size = 32
06/27 09:12:30 PM ***** Eval results *****
06/27 09:12:30 PM   acc = 0.575
06/27 09:12:30 PM   cls_loss = 1.8533897929721408
06/27 09:12:30 PM   eval_loss = 2.2286837100982666
06/27 09:12:30 PM   global_step = 99
06/27 09:12:30 PM   loss = 1.8533897929721408
06/27 09:12:30 PM ***** LOSS printing *****
06/27 09:12:30 PM loss
06/27 09:12:30 PM tensor(1.5293, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:30 PM ***** LOSS printing *****
06/27 09:12:30 PM loss
06/27 09:12:30 PM tensor(1.6940, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:30 PM ***** LOSS printing *****
06/27 09:12:30 PM loss
06/27 09:12:30 PM tensor(2.4222, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:31 PM ***** LOSS printing *****
06/27 09:12:31 PM loss
06/27 09:12:31 PM tensor(2.0042, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:31 PM ***** LOSS printing *****
06/27 09:12:31 PM loss
06/27 09:12:31 PM tensor(2.1841, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:31 PM ***** Running evaluation MLM *****
06/27 09:12:31 PM   Epoch = 3 iter 104 step
06/27 09:12:31 PM   Num examples = 40
06/27 09:12:31 PM   Batch size = 32
06/27 09:12:32 PM ***** Eval results *****
06/27 09:12:32 PM   acc = 0.55
06/27 09:12:32 PM   cls_loss = 1.8938695703233992
06/27 09:12:32 PM   eval_loss = 2.1888872385025024
06/27 09:12:32 PM   global_step = 104
06/27 09:12:32 PM   loss = 1.8938695703233992
06/27 09:12:32 PM ***** LOSS printing *****
06/27 09:12:32 PM loss
06/27 09:12:32 PM tensor(1.6966, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:32 PM ***** LOSS printing *****
06/27 09:12:32 PM loss
06/27 09:12:32 PM tensor(2.1997, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:33 PM ***** LOSS printing *****
06/27 09:12:33 PM loss
06/27 09:12:33 PM tensor(3.1712, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:33 PM ***** LOSS printing *****
06/27 09:12:33 PM loss
06/27 09:12:33 PM tensor(1.6110, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:33 PM ***** LOSS printing *****
06/27 09:12:33 PM loss
06/27 09:12:33 PM tensor(1.5201, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:33 PM ***** Running evaluation MLM *****
06/27 09:12:33 PM   Epoch = 3 iter 109 step
06/27 09:12:33 PM   Num examples = 40
06/27 09:12:33 PM   Batch size = 32
06/27 09:12:35 PM ***** Eval results *****
06/27 09:12:35 PM   acc = 0.575
06/27 09:12:35 PM   cls_loss = 1.932252614121688
06/27 09:12:35 PM   eval_loss = 1.9551488757133484
06/27 09:12:35 PM   global_step = 109
06/27 09:12:35 PM   loss = 1.932252614121688
06/27 09:12:35 PM ***** LOSS printing *****
06/27 09:12:35 PM loss
06/27 09:12:35 PM tensor(1.7550, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:35 PM ***** LOSS printing *****
06/27 09:12:35 PM loss
06/27 09:12:35 PM tensor(2.0899, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:35 PM ***** LOSS printing *****
06/27 09:12:35 PM loss
06/27 09:12:35 PM tensor(2.0790, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:35 PM ***** LOSS printing *****
06/27 09:12:35 PM loss
06/27 09:12:35 PM tensor(1.6236, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:36 PM ***** LOSS printing *****
06/27 09:12:36 PM loss
06/27 09:12:36 PM tensor(1.9424, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:36 PM ***** Running evaluation MLM *****
06/27 09:12:36 PM   Epoch = 3 iter 114 step
06/27 09:12:36 PM   Num examples = 40
06/27 09:12:36 PM   Batch size = 32
06/27 09:12:37 PM ***** Eval results *****
06/27 09:12:37 PM   acc = 0.475
06/27 09:12:37 PM   cls_loss = 1.9251090238491695
06/27 09:12:37 PM   eval_loss = 1.8486878871917725
06/27 09:12:37 PM   global_step = 114
06/27 09:12:37 PM   loss = 1.9251090238491695
06/27 09:12:37 PM ***** LOSS printing *****
06/27 09:12:37 PM loss
06/27 09:12:37 PM tensor(1.8352, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:37 PM ***** LOSS printing *****
06/27 09:12:37 PM loss
06/27 09:12:37 PM tensor(1.9614, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:37 PM ***** LOSS printing *****
06/27 09:12:37 PM loss
06/27 09:12:37 PM tensor(1.8609, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:38 PM ***** LOSS printing *****
06/27 09:12:38 PM loss
06/27 09:12:38 PM tensor(1.9582, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:38 PM ***** LOSS printing *****
06/27 09:12:38 PM loss
06/27 09:12:38 PM tensor(1.4943, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:38 PM ***** Running evaluation MLM *****
06/27 09:12:38 PM   Epoch = 3 iter 119 step
06/27 09:12:38 PM   Num examples = 40
06/27 09:12:38 PM   Batch size = 32
06/27 09:12:39 PM ***** Eval results *****
06/27 09:12:39 PM   acc = 0.5
06/27 09:12:39 PM   cls_loss = 1.9073308582963615
06/27 09:12:39 PM   eval_loss = 2.0713818073272705
06/27 09:12:39 PM   global_step = 119
06/27 09:12:39 PM   loss = 1.9073308582963615
06/27 09:12:39 PM ***** LOSS printing *****
06/27 09:12:39 PM loss
06/27 09:12:39 PM tensor(2.3205, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:40 PM ***** LOSS printing *****
06/27 09:12:40 PM loss
06/27 09:12:40 PM tensor(1.4368, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:40 PM ***** LOSS printing *****
06/27 09:12:40 PM loss
06/27 09:12:40 PM tensor(1.0302, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:40 PM ***** LOSS printing *****
06/27 09:12:40 PM loss
06/27 09:12:40 PM tensor(2.2446, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:40 PM ***** LOSS printing *****
06/27 09:12:40 PM loss
06/27 09:12:40 PM tensor(1.7184, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:40 PM ***** Running evaluation MLM *****
06/27 09:12:40 PM   Epoch = 4 iter 124 step
06/27 09:12:40 PM   Num examples = 40
06/27 09:12:40 PM   Batch size = 32
06/27 09:12:42 PM ***** Eval results *****
06/27 09:12:42 PM   acc = 0.5
06/27 09:12:42 PM   cls_loss = 1.607516199350357
06/27 09:12:42 PM   eval_loss = 2.390013098716736
06/27 09:12:42 PM   global_step = 124
06/27 09:12:42 PM   loss = 1.607516199350357
06/27 09:12:42 PM ***** LOSS printing *****
06/27 09:12:42 PM loss
06/27 09:12:42 PM tensor(1.1287, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:42 PM ***** LOSS printing *****
06/27 09:12:42 PM loss
06/27 09:12:42 PM tensor(1.5513, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:42 PM ***** LOSS printing *****
06/27 09:12:42 PM loss
06/27 09:12:42 PM tensor(2.2294, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:42 PM ***** LOSS printing *****
06/27 09:12:42 PM loss
06/27 09:12:42 PM tensor(2.0175, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:43 PM ***** LOSS printing *****
06/27 09:12:43 PM loss
06/27 09:12:43 PM tensor(1.3645, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:43 PM ***** Running evaluation MLM *****
06/27 09:12:43 PM   Epoch = 4 iter 129 step
06/27 09:12:43 PM   Num examples = 40
06/27 09:12:43 PM   Batch size = 32
06/27 09:12:44 PM ***** Eval results *****
06/27 09:12:44 PM   acc = 0.475
06/27 09:12:44 PM   cls_loss = 1.6357173654768202
06/27 09:12:44 PM   eval_loss = 2.3627349138259888
06/27 09:12:44 PM   global_step = 129
06/27 09:12:44 PM   loss = 1.6357173654768202
06/27 09:12:44 PM ***** LOSS printing *****
06/27 09:12:44 PM loss
06/27 09:12:44 PM tensor(2.0052, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:44 PM ***** LOSS printing *****
06/27 09:12:44 PM loss
06/27 09:12:44 PM tensor(1.5129, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:44 PM ***** LOSS printing *****
06/27 09:12:44 PM loss
06/27 09:12:44 PM tensor(1.5922, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:45 PM ***** LOSS printing *****
06/27 09:12:45 PM loss
06/27 09:12:45 PM tensor(1.3072, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:45 PM ***** LOSS printing *****
06/27 09:12:45 PM loss
06/27 09:12:45 PM tensor(1.8472, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:45 PM ***** Running evaluation MLM *****
06/27 09:12:45 PM   Epoch = 4 iter 134 step
06/27 09:12:45 PM   Num examples = 40
06/27 09:12:45 PM   Batch size = 32
06/27 09:12:46 PM ***** Eval results *****
06/27 09:12:46 PM   acc = 0.575
06/27 09:12:46 PM   cls_loss = 1.6418757438659668
06/27 09:12:46 PM   eval_loss = 1.865996927022934
06/27 09:12:46 PM   global_step = 134
06/27 09:12:46 PM   loss = 1.6418757438659668
06/27 09:12:46 PM ***** LOSS printing *****
06/27 09:12:46 PM loss
06/27 09:12:46 PM tensor(1.3253, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:47 PM ***** LOSS printing *****
06/27 09:12:47 PM loss
06/27 09:12:47 PM tensor(1.3996, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:47 PM ***** LOSS printing *****
06/27 09:12:47 PM loss
06/27 09:12:47 PM tensor(1.7668, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:47 PM ***** LOSS printing *****
06/27 09:12:47 PM loss
06/27 09:12:47 PM tensor(1.8255, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:47 PM ***** LOSS printing *****
06/27 09:12:47 PM loss
06/27 09:12:47 PM tensor(1.3557, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:12:47 PM ***** Running evaluation MLM *****
06/27 09:12:47 PM   Epoch = 4 iter 139 step
06/27 09:12:47 PM   Num examples = 40
06/27 09:12:47 PM   Batch size = 32
06/27 09:12:49 PM ***** Eval results *****
06/27 09:12:49 PM   acc = 0.6
06/27 09:12:49 PM   cls_loss = 1.6136394488184076
06/27 09:12:49 PM   eval_loss = 1.696803867816925
06/27 09:12:49 PM   global_step = 139
06/27 09:12:49 PM   loss = 1.6136394488184076
06/27 09:12:49 PM ***** Save model *****
06/27 09:12:49 PM ***** Test Dataset Eval Result *****
06/27 09:13:58 PM ***** Eval results *****
06/27 09:13:58 PM   acc = 0.5058823529411764
06/27 09:13:58 PM   cls_loss = 1.6136394488184076
06/27 09:13:58 PM   eval_loss = 2.383774355479649
06/27 09:13:58 PM   global_step = 139
06/27 09:13:58 PM   loss = 1.6136394488184076
06/27 09:14:02 PM ***** LOSS printing *****
06/27 09:14:02 PM loss
06/27 09:14:02 PM tensor(1.5062, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:02 PM ***** LOSS printing *****
06/27 09:14:02 PM loss
06/27 09:14:02 PM tensor(1.2358, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:02 PM ***** LOSS printing *****
06/27 09:14:02 PM loss
06/27 09:14:02 PM tensor(1.7706, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:03 PM ***** LOSS printing *****
06/27 09:14:03 PM loss
06/27 09:14:03 PM tensor(2.3498, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:03 PM ***** LOSS printing *****
06/27 09:14:03 PM loss
06/27 09:14:03 PM tensor(1.5909, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:03 PM ***** Running evaluation MLM *****
06/27 09:14:03 PM   Epoch = 4 iter 144 step
06/27 09:14:03 PM   Num examples = 40
06/27 09:14:03 PM   Batch size = 32
06/27 09:14:04 PM ***** Eval results *****
06/27 09:14:04 PM   acc = 0.55
06/27 09:14:04 PM   cls_loss = 1.6296848406394322
06/27 09:14:04 PM   eval_loss = 1.7546584606170654
06/27 09:14:04 PM   global_step = 144
06/27 09:14:04 PM   loss = 1.6296848406394322
06/27 09:14:04 PM ***** LOSS printing *****
06/27 09:14:04 PM loss
06/27 09:14:04 PM tensor(1.3977, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:05 PM ***** LOSS printing *****
06/27 09:14:05 PM loss
06/27 09:14:05 PM tensor(1.6559, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:05 PM ***** LOSS printing *****
06/27 09:14:05 PM loss
06/27 09:14:05 PM tensor(2.1229, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:05 PM ***** LOSS printing *****
06/27 09:14:05 PM loss
06/27 09:14:05 PM tensor(1.4081, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:05 PM ***** LOSS printing *****
06/27 09:14:05 PM loss
06/27 09:14:05 PM tensor(1.7323, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:05 PM ***** Running evaluation MLM *****
06/27 09:14:05 PM   Epoch = 4 iter 149 step
06/27 09:14:05 PM   Num examples = 40
06/27 09:14:05 PM   Batch size = 32
06/27 09:14:07 PM ***** Eval results *****
06/27 09:14:07 PM   acc = 0.475
06/27 09:14:07 PM   cls_loss = 1.6354900516312698
06/27 09:14:07 PM   eval_loss = 2.0304606556892395
06/27 09:14:07 PM   global_step = 149
06/27 09:14:07 PM   loss = 1.6354900516312698
06/27 09:14:07 PM ***** LOSS printing *****
06/27 09:14:07 PM loss
06/27 09:14:07 PM tensor(1.4130, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:07 PM ***** LOSS printing *****
06/27 09:14:07 PM loss
06/27 09:14:07 PM tensor(1.6788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:07 PM ***** LOSS printing *****
06/27 09:14:07 PM loss
06/27 09:14:07 PM tensor(1.4016, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:07 PM ***** LOSS printing *****
06/27 09:14:07 PM loss
06/27 09:14:07 PM tensor(2.0367, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:08 PM ***** LOSS printing *****
06/27 09:14:08 PM loss
06/27 09:14:08 PM tensor(1.3898, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:08 PM ***** Running evaluation MLM *****
06/27 09:14:08 PM   Epoch = 5 iter 154 step
06/27 09:14:08 PM   Num examples = 40
06/27 09:14:08 PM   Batch size = 32
06/27 09:14:09 PM ***** Eval results *****
06/27 09:14:09 PM   acc = 0.45
06/27 09:14:09 PM   cls_loss = 1.626743495464325
06/27 09:14:09 PM   eval_loss = 2.445278763771057
06/27 09:14:09 PM   global_step = 154
06/27 09:14:09 PM   loss = 1.626743495464325
06/27 09:14:09 PM ***** LOSS printing *****
06/27 09:14:09 PM loss
06/27 09:14:09 PM tensor(1.6774, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:09 PM ***** LOSS printing *****
06/27 09:14:09 PM loss
06/27 09:14:09 PM tensor(2.0739, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:09 PM ***** LOSS printing *****
06/27 09:14:09 PM loss
06/27 09:14:09 PM tensor(1.2059, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:10 PM ***** LOSS printing *****
06/27 09:14:10 PM loss
06/27 09:14:10 PM tensor(1.3276, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:10 PM ***** LOSS printing *****
06/27 09:14:10 PM loss
06/27 09:14:10 PM tensor(1.7574, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:10 PM ***** Running evaluation MLM *****
06/27 09:14:10 PM   Epoch = 5 iter 159 step
06/27 09:14:10 PM   Num examples = 40
06/27 09:14:10 PM   Batch size = 32
06/27 09:14:11 PM ***** Eval results *****
06/27 09:14:11 PM   acc = 0.55
06/27 09:14:11 PM   cls_loss = 1.6165684064229329
06/27 09:14:11 PM   eval_loss = 2.38082093000412
06/27 09:14:11 PM   global_step = 159
06/27 09:14:11 PM   loss = 1.6165684064229329
06/27 09:14:11 PM ***** LOSS printing *****
06/27 09:14:11 PM loss
06/27 09:14:11 PM tensor(1.1854, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:12 PM ***** LOSS printing *****
06/27 09:14:12 PM loss
06/27 09:14:12 PM tensor(1.1865, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:12 PM ***** LOSS printing *****
06/27 09:14:12 PM loss
06/27 09:14:12 PM tensor(1.2993, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:12 PM ***** LOSS printing *****
06/27 09:14:12 PM loss
06/27 09:14:12 PM tensor(1.2020, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:12 PM ***** LOSS printing *****
06/27 09:14:12 PM loss
06/27 09:14:12 PM tensor(1.6090, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:12 PM ***** Running evaluation MLM *****
06/27 09:14:12 PM   Epoch = 5 iter 164 step
06/27 09:14:12 PM   Num examples = 40
06/27 09:14:12 PM   Batch size = 32
06/27 09:14:14 PM ***** Eval results *****
06/27 09:14:14 PM   acc = 0.575
06/27 09:14:14 PM   cls_loss = 1.5022316064153398
06/27 09:14:14 PM   eval_loss = 2.445169448852539
06/27 09:14:14 PM   global_step = 164
06/27 09:14:14 PM   loss = 1.5022316064153398
06/27 09:14:14 PM ***** LOSS printing *****
06/27 09:14:14 PM loss
06/27 09:14:14 PM tensor(2.3193, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:14 PM ***** LOSS printing *****
06/27 09:14:14 PM loss
06/27 09:14:14 PM tensor(1.3788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:14 PM ***** LOSS printing *****
06/27 09:14:14 PM loss
06/27 09:14:14 PM tensor(1.5694, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:14 PM ***** LOSS printing *****
06/27 09:14:14 PM loss
06/27 09:14:14 PM tensor(1.4257, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:15 PM ***** LOSS printing *****
06/27 09:14:15 PM loss
06/27 09:14:15 PM tensor(1.6045, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:15 PM ***** Running evaluation MLM *****
06/27 09:14:15 PM   Epoch = 5 iter 169 step
06/27 09:14:15 PM   Num examples = 40
06/27 09:14:15 PM   Batch size = 32
06/27 09:14:16 PM ***** Eval results *****
06/27 09:14:16 PM   acc = 0.575
06/27 09:14:16 PM   cls_loss = 1.5436311081836098
06/27 09:14:16 PM   eval_loss = 2.0424720644950867
06/27 09:14:16 PM   global_step = 169
06/27 09:14:16 PM   loss = 1.5436311081836098
06/27 09:14:16 PM ***** LOSS printing *****
06/27 09:14:16 PM loss
06/27 09:14:16 PM tensor(1.2576, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:16 PM ***** LOSS printing *****
06/27 09:14:16 PM loss
06/27 09:14:16 PM tensor(1.5932, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:17 PM ***** LOSS printing *****
06/27 09:14:17 PM loss
06/27 09:14:17 PM tensor(1.8036, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:17 PM ***** LOSS printing *****
06/27 09:14:17 PM loss
06/27 09:14:17 PM tensor(1.5666, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:17 PM ***** LOSS printing *****
06/27 09:14:17 PM loss
06/27 09:14:17 PM tensor(1.2263, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:17 PM ***** Running evaluation MLM *****
06/27 09:14:17 PM   Epoch = 5 iter 174 step
06/27 09:14:17 PM   Num examples = 40
06/27 09:14:17 PM   Batch size = 32
06/27 09:14:18 PM ***** Eval results *****
06/27 09:14:18 PM   acc = 0.525
06/27 09:14:18 PM   cls_loss = 1.532348985473315
06/27 09:14:18 PM   eval_loss = 1.9666776657104492
06/27 09:14:18 PM   global_step = 174
06/27 09:14:18 PM   loss = 1.532348985473315
06/27 09:14:18 PM ***** LOSS printing *****
06/27 09:14:18 PM loss
06/27 09:14:18 PM tensor(1.5705, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:19 PM ***** LOSS printing *****
06/27 09:14:19 PM loss
06/27 09:14:19 PM tensor(1.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:19 PM ***** LOSS printing *****
06/27 09:14:19 PM loss
06/27 09:14:19 PM tensor(1.5123, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:19 PM ***** LOSS printing *****
06/27 09:14:19 PM loss
06/27 09:14:19 PM tensor(1.7287, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:19 PM ***** LOSS printing *****
06/27 09:14:19 PM loss
06/27 09:14:19 PM tensor(1.3807, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:19 PM ***** Running evaluation MLM *****
06/27 09:14:19 PM   Epoch = 5 iter 179 step
06/27 09:14:19 PM   Num examples = 40
06/27 09:14:19 PM   Batch size = 32
06/27 09:14:21 PM ***** Eval results *****
06/27 09:14:21 PM   acc = 0.525
06/27 09:14:21 PM   cls_loss = 1.5353439511923954
06/27 09:14:21 PM   eval_loss = 1.919035792350769
06/27 09:14:21 PM   global_step = 179
06/27 09:14:21 PM   loss = 1.5353439511923954
06/27 09:14:21 PM ***** LOSS printing *****
06/27 09:14:21 PM loss
06/27 09:14:21 PM tensor(1.4351, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:21 PM ***** LOSS printing *****
06/27 09:14:21 PM loss
06/27 09:14:21 PM tensor(1.2559, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:21 PM ***** LOSS printing *****
06/27 09:14:21 PM loss
06/27 09:14:21 PM tensor(1.3927, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:21 PM ***** LOSS printing *****
06/27 09:14:21 PM loss
06/27 09:14:21 PM tensor(1.0268, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:22 PM ***** LOSS printing *****
06/27 09:14:22 PM loss
06/27 09:14:22 PM tensor(0.8983, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:14:22 PM ***** Running evaluation MLM *****
06/27 09:14:22 PM   Epoch = 6 iter 184 step
06/27 09:14:22 PM   Num examples = 40
06/27 09:14:22 PM   Batch size = 32
06/27 09:14:23 PM ***** Eval results *****
06/27 09:14:23 PM   acc = 0.625
06/27 09:14:23 PM   cls_loss = 1.143447995185852
06/27 09:14:23 PM   eval_loss = 1.854837954044342
06/27 09:14:23 PM   global_step = 184
06/27 09:14:23 PM   loss = 1.143447995185852
06/27 09:14:23 PM ***** Save model *****
06/27 09:14:23 PM ***** Test Dataset Eval Result *****
06/27 09:15:32 PM ***** Eval results *****
06/27 09:15:32 PM   acc = 0.47420814479638007
06/27 09:15:32 PM   cls_loss = 1.143447995185852
06/27 09:15:32 PM   eval_loss = 2.623169381277902
06/27 09:15:32 PM   global_step = 184
06/27 09:15:32 PM   loss = 1.143447995185852
06/27 09:15:36 PM ***** LOSS printing *****
06/27 09:15:36 PM loss
06/27 09:15:36 PM tensor(1.6182, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:37 PM ***** LOSS printing *****
06/27 09:15:37 PM loss
06/27 09:15:37 PM tensor(1.4505, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:37 PM ***** LOSS printing *****
06/27 09:15:37 PM loss
06/27 09:15:37 PM tensor(1.0266, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:37 PM ***** LOSS printing *****
06/27 09:15:37 PM loss
06/27 09:15:37 PM tensor(1.2917, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:37 PM ***** LOSS printing *****
06/27 09:15:37 PM loss
06/27 09:15:37 PM tensor(1.4562, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:37 PM ***** Running evaluation MLM *****
06/27 09:15:37 PM   Epoch = 6 iter 189 step
06/27 09:15:37 PM   Num examples = 40
06/27 09:15:37 PM   Batch size = 32
06/27 09:15:39 PM ***** Eval results *****
06/27 09:15:39 PM   acc = 0.575
06/27 09:15:39 PM   cls_loss = 1.2685608996285334
06/27 09:15:39 PM   eval_loss = 1.83220374584198
06/27 09:15:39 PM   global_step = 189
06/27 09:15:39 PM   loss = 1.2685608996285334
06/27 09:15:39 PM ***** LOSS printing *****
06/27 09:15:39 PM loss
06/27 09:15:39 PM tensor(1.4166, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:39 PM ***** LOSS printing *****
06/27 09:15:39 PM loss
06/27 09:15:39 PM tensor(1.8169, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:39 PM ***** LOSS printing *****
06/27 09:15:39 PM loss
06/27 09:15:39 PM tensor(1.6377, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:39 PM ***** LOSS printing *****
06/27 09:15:39 PM loss
06/27 09:15:39 PM tensor(1.6810, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:40 PM ***** LOSS printing *****
06/27 09:15:40 PM loss
06/27 09:15:40 PM tensor(1.7058, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:40 PM ***** Running evaluation MLM *****
06/27 09:15:40 PM   Epoch = 6 iter 194 step
06/27 09:15:40 PM   Num examples = 40
06/27 09:15:40 PM   Batch size = 32
06/27 09:15:41 PM ***** Eval results *****
06/27 09:15:41 PM   acc = 0.575
06/27 09:15:41 PM   cls_loss = 1.4053573097501482
06/27 09:15:41 PM   eval_loss = 1.7406341433525085
06/27 09:15:41 PM   global_step = 194
06/27 09:15:41 PM   loss = 1.4053573097501482
06/27 09:15:41 PM ***** LOSS printing *****
06/27 09:15:41 PM loss
06/27 09:15:41 PM tensor(1.7057, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:41 PM ***** LOSS printing *****
06/27 09:15:41 PM loss
06/27 09:15:41 PM tensor(1.5008, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:41 PM ***** LOSS printing *****
06/27 09:15:41 PM loss
06/27 09:15:41 PM tensor(1.0799, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:42 PM ***** LOSS printing *****
06/27 09:15:42 PM loss
06/27 09:15:42 PM tensor(0.9946, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:42 PM ***** LOSS printing *****
06/27 09:15:42 PM loss
06/27 09:15:42 PM tensor(1.6018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:42 PM ***** Running evaluation MLM *****
06/27 09:15:42 PM   Epoch = 6 iter 199 step
06/27 09:15:42 PM   Num examples = 40
06/27 09:15:42 PM   Batch size = 32
06/27 09:15:43 PM ***** Eval results *****
06/27 09:15:43 PM   acc = 0.55
06/27 09:15:43 PM   cls_loss = 1.397776070394014
06/27 09:15:43 PM   eval_loss = 1.771446406841278
06/27 09:15:43 PM   global_step = 199
06/27 09:15:43 PM   loss = 1.397776070394014
06/27 09:15:43 PM ***** LOSS printing *****
06/27 09:15:43 PM loss
06/27 09:15:43 PM tensor(1.7494, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:44 PM ***** LOSS printing *****
06/27 09:15:44 PM loss
06/27 09:15:44 PM tensor(1.8724, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:44 PM ***** LOSS printing *****
06/27 09:15:44 PM loss
06/27 09:15:44 PM tensor(1.9392, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:44 PM ***** LOSS printing *****
06/27 09:15:44 PM loss
06/27 09:15:44 PM tensor(2.0623, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:44 PM ***** LOSS printing *****
06/27 09:15:44 PM loss
06/27 09:15:44 PM tensor(1.5564, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:44 PM ***** Running evaluation MLM *****
06/27 09:15:44 PM   Epoch = 6 iter 204 step
06/27 09:15:44 PM   Num examples = 40
06/27 09:15:44 PM   Batch size = 32
06/27 09:15:46 PM ***** Eval results *****
06/27 09:15:46 PM   acc = 0.55
06/27 09:15:46 PM   cls_loss = 1.4890614201625187
06/27 09:15:46 PM   eval_loss = 1.9492160081863403
06/27 09:15:46 PM   global_step = 204
06/27 09:15:46 PM   loss = 1.4890614201625187
06/27 09:15:46 PM ***** LOSS printing *****
06/27 09:15:46 PM loss
06/27 09:15:46 PM tensor(2.1643, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:46 PM ***** LOSS printing *****
06/27 09:15:46 PM loss
06/27 09:15:46 PM tensor(1.3923, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:46 PM ***** LOSS printing *****
06/27 09:15:46 PM loss
06/27 09:15:46 PM tensor(1.9074, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:46 PM ***** LOSS printing *****
06/27 09:15:46 PM loss
06/27 09:15:46 PM tensor(1.4072, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:47 PM ***** LOSS printing *****
06/27 09:15:47 PM loss
06/27 09:15:47 PM tensor(2.1073, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:47 PM ***** Running evaluation MLM *****
06/27 09:15:47 PM   Epoch = 6 iter 209 step
06/27 09:15:47 PM   Num examples = 40
06/27 09:15:47 PM   Batch size = 32
06/27 09:15:48 PM ***** Eval results *****
06/27 09:15:48 PM   acc = 0.575
06/27 09:15:48 PM   cls_loss = 1.5419295121883523
06/27 09:15:48 PM   eval_loss = 2.129320502281189
06/27 09:15:48 PM   global_step = 209
06/27 09:15:48 PM   loss = 1.5419295121883523
06/27 09:15:48 PM ***** LOSS printing *****
06/27 09:15:48 PM loss
06/27 09:15:48 PM tensor(1.4907, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:48 PM ***** LOSS printing *****
06/27 09:15:48 PM loss
06/27 09:15:48 PM tensor(1.0422, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:48 PM ***** LOSS printing *****
06/27 09:15:48 PM loss
06/27 09:15:48 PM tensor(1.5341, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:49 PM ***** LOSS printing *****
06/27 09:15:49 PM loss
06/27 09:15:49 PM tensor(1.4776, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:49 PM ***** LOSS printing *****
06/27 09:15:49 PM loss
06/27 09:15:49 PM tensor(1.0120, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:49 PM ***** Running evaluation MLM *****
06/27 09:15:49 PM   Epoch = 7 iter 214 step
06/27 09:15:49 PM   Num examples = 40
06/27 09:15:49 PM   Batch size = 32
06/27 09:15:50 PM ***** Eval results *****
06/27 09:15:50 PM   acc = 0.55
06/27 09:15:50 PM   cls_loss = 1.2664657235145569
06/27 09:15:50 PM   eval_loss = 2.251965641975403
06/27 09:15:50 PM   global_step = 214
06/27 09:15:50 PM   loss = 1.2664657235145569
06/27 09:15:50 PM ***** LOSS printing *****
06/27 09:15:50 PM loss
06/27 09:15:50 PM tensor(1.2053, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:51 PM ***** LOSS printing *****
06/27 09:15:51 PM loss
06/27 09:15:51 PM tensor(1.5309, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:51 PM ***** LOSS printing *****
06/27 09:15:51 PM loss
06/27 09:15:51 PM tensor(1.3894, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:51 PM ***** LOSS printing *****
06/27 09:15:51 PM loss
06/27 09:15:51 PM tensor(1.0814, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:51 PM ***** LOSS printing *****
06/27 09:15:51 PM loss
06/27 09:15:51 PM tensor(1.1159, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:51 PM ***** Running evaluation MLM *****
06/27 09:15:51 PM   Epoch = 7 iter 219 step
06/27 09:15:51 PM   Num examples = 40
06/27 09:15:51 PM   Batch size = 32
06/27 09:15:53 PM ***** Eval results *****
06/27 09:15:53 PM   acc = 0.55
06/27 09:15:53 PM   cls_loss = 1.265418569246928
06/27 09:15:53 PM   eval_loss = 1.9509333968162537
06/27 09:15:53 PM   global_step = 219
06/27 09:15:53 PM   loss = 1.265418569246928
06/27 09:15:53 PM ***** LOSS printing *****
06/27 09:15:53 PM loss
06/27 09:15:53 PM tensor(0.7768, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:53 PM ***** LOSS printing *****
06/27 09:15:53 PM loss
06/27 09:15:53 PM tensor(1.5141, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:53 PM ***** LOSS printing *****
06/27 09:15:53 PM loss
06/27 09:15:53 PM tensor(1.0365, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:53 PM ***** LOSS printing *****
06/27 09:15:53 PM loss
06/27 09:15:53 PM tensor(1.7756, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:54 PM ***** LOSS printing *****
06/27 09:15:54 PM loss
06/27 09:15:54 PM tensor(1.8065, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:54 PM ***** Running evaluation MLM *****
06/27 09:15:54 PM   Epoch = 7 iter 224 step
06/27 09:15:54 PM   Num examples = 40
06/27 09:15:54 PM   Batch size = 32
06/27 09:15:55 PM ***** Eval results *****
06/27 09:15:55 PM   acc = 0.55
06/27 09:15:55 PM   cls_loss = 1.3070251601082938
06/27 09:15:55 PM   eval_loss = 1.8370628356933594
06/27 09:15:55 PM   global_step = 224
06/27 09:15:55 PM   loss = 1.3070251601082938
06/27 09:15:55 PM ***** LOSS printing *****
06/27 09:15:55 PM loss
06/27 09:15:55 PM tensor(1.2547, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:55 PM ***** LOSS printing *****
06/27 09:15:55 PM loss
06/27 09:15:55 PM tensor(1.1393, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:56 PM ***** LOSS printing *****
06/27 09:15:56 PM loss
06/27 09:15:56 PM tensor(2.1859, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:56 PM ***** LOSS printing *****
06/27 09:15:56 PM loss
06/27 09:15:56 PM tensor(1.5171, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:56 PM ***** LOSS printing *****
06/27 09:15:56 PM loss
06/27 09:15:56 PM tensor(1.6466, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:56 PM ***** Running evaluation MLM *****
06/27 09:15:56 PM   Epoch = 7 iter 229 step
06/27 09:15:56 PM   Num examples = 40
06/27 09:15:56 PM   Batch size = 32
06/27 09:15:57 PM ***** Eval results *****
06/27 09:15:57 PM   acc = 0.55
06/27 09:15:57 PM   cls_loss = 1.3706301199762445
06/27 09:15:57 PM   eval_loss = 1.809164583683014
06/27 09:15:57 PM   global_step = 229
06/27 09:15:57 PM   loss = 1.3706301199762445
06/27 09:15:57 PM ***** LOSS printing *****
06/27 09:15:57 PM loss
06/27 09:15:57 PM tensor(1.4717, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:58 PM ***** LOSS printing *****
06/27 09:15:58 PM loss
06/27 09:15:58 PM tensor(2.1678, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:58 PM ***** LOSS printing *****
06/27 09:15:58 PM loss
06/27 09:15:58 PM tensor(1.4602, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:58 PM ***** LOSS printing *****
06/27 09:15:58 PM loss
06/27 09:15:58 PM tensor(1.3876, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:58 PM ***** LOSS printing *****
06/27 09:15:58 PM loss
06/27 09:15:58 PM tensor(1.3684, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:15:59 PM ***** Running evaluation MLM *****
06/27 09:15:59 PM   Epoch = 7 iter 234 step
06/27 09:15:59 PM   Num examples = 40
06/27 09:15:59 PM   Batch size = 32
06/27 09:16:00 PM ***** Eval results *****
06/27 09:16:00 PM   acc = 0.575
06/27 09:16:00 PM   cls_loss = 1.412404288848241
06/27 09:16:00 PM   eval_loss = 1.887042224407196
06/27 09:16:00 PM   global_step = 234
06/27 09:16:00 PM   loss = 1.412404288848241
06/27 09:16:00 PM ***** LOSS printing *****
06/27 09:16:00 PM loss
06/27 09:16:00 PM tensor(1.3881, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:00 PM ***** LOSS printing *****
06/27 09:16:00 PM loss
06/27 09:16:00 PM tensor(2.0066, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:00 PM ***** LOSS printing *****
06/27 09:16:00 PM loss
06/27 09:16:00 PM tensor(1.4462, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:00 PM ***** LOSS printing *****
06/27 09:16:00 PM loss
06/27 09:16:00 PM tensor(1.1723, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:01 PM ***** LOSS printing *****
06/27 09:16:01 PM loss
06/27 09:16:01 PM tensor(1.4604, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:01 PM ***** Running evaluation MLM *****
06/27 09:16:01 PM   Epoch = 7 iter 239 step
06/27 09:16:01 PM   Num examples = 40
06/27 09:16:01 PM   Batch size = 32
06/27 09:16:02 PM ***** Eval results *****
06/27 09:16:02 PM   acc = 0.55
06/27 09:16:02 PM   cls_loss = 1.426594693085243
06/27 09:16:02 PM   eval_loss = 2.0180951356887817
06/27 09:16:02 PM   global_step = 239
06/27 09:16:02 PM   loss = 1.426594693085243
06/27 09:16:02 PM ***** LOSS printing *****
06/27 09:16:02 PM loss
06/27 09:16:02 PM tensor(1.2990, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:02 PM ***** LOSS printing *****
06/27 09:16:02 PM loss
06/27 09:16:02 PM tensor(0.9650, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:03 PM ***** LOSS printing *****
06/27 09:16:03 PM loss
06/27 09:16:03 PM tensor(1.2764, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:03 PM ***** LOSS printing *****
06/27 09:16:03 PM loss
06/27 09:16:03 PM tensor(1.0070, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:03 PM ***** LOSS printing *****
06/27 09:16:03 PM loss
06/27 09:16:03 PM tensor(1.0217, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:03 PM ***** Running evaluation MLM *****
06/27 09:16:03 PM   Epoch = 8 iter 244 step
06/27 09:16:03 PM   Num examples = 40
06/27 09:16:03 PM   Batch size = 32
06/27 09:16:04 PM ***** Eval results *****
06/27 09:16:04 PM   acc = 0.55
06/27 09:16:04 PM   cls_loss = 1.0675137639045715
06/27 09:16:04 PM   eval_loss = 2.254387140274048
06/27 09:16:04 PM   global_step = 244
06/27 09:16:04 PM   loss = 1.0675137639045715
06/27 09:16:05 PM ***** LOSS printing *****
06/27 09:16:05 PM loss
06/27 09:16:05 PM tensor(1.4485, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:05 PM ***** LOSS printing *****
06/27 09:16:05 PM loss
06/27 09:16:05 PM tensor(1.6738, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:05 PM ***** LOSS printing *****
06/27 09:16:05 PM loss
06/27 09:16:05 PM tensor(0.8585, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:05 PM ***** LOSS printing *****
06/27 09:16:05 PM loss
06/27 09:16:05 PM tensor(0.8441, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:05 PM ***** LOSS printing *****
06/27 09:16:05 PM loss
06/27 09:16:05 PM tensor(1.5322, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:06 PM ***** Running evaluation MLM *****
06/27 09:16:06 PM   Epoch = 8 iter 249 step
06/27 09:16:06 PM   Num examples = 40
06/27 09:16:06 PM   Batch size = 32
06/27 09:16:07 PM ***** Eval results *****
06/27 09:16:07 PM   acc = 0.6
06/27 09:16:07 PM   cls_loss = 1.1808087958229914
06/27 09:16:07 PM   eval_loss = 2.057030737400055
06/27 09:16:07 PM   global_step = 249
06/27 09:16:07 PM   loss = 1.1808087958229914
06/27 09:16:07 PM ***** LOSS printing *****
06/27 09:16:07 PM loss
06/27 09:16:07 PM tensor(1.2907, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:07 PM ***** LOSS printing *****
06/27 09:16:07 PM loss
06/27 09:16:07 PM tensor(1.0475, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:07 PM ***** LOSS printing *****
06/27 09:16:07 PM loss
06/27 09:16:07 PM tensor(1.1971, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:08 PM ***** LOSS printing *****
06/27 09:16:08 PM loss
06/27 09:16:08 PM tensor(1.6395, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:08 PM ***** LOSS printing *****
06/27 09:16:08 PM loss
06/27 09:16:08 PM tensor(1.7064, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:08 PM ***** Running evaluation MLM *****
06/27 09:16:08 PM   Epoch = 8 iter 254 step
06/27 09:16:08 PM   Num examples = 40
06/27 09:16:08 PM   Batch size = 32
06/27 09:16:09 PM ***** Eval results *****
06/27 09:16:09 PM   acc = 0.6
06/27 09:16:09 PM   cls_loss = 1.2506021772112166
06/27 09:16:09 PM   eval_loss = 1.818269670009613
06/27 09:16:09 PM   global_step = 254
06/27 09:16:09 PM   loss = 1.2506021772112166
06/27 09:16:09 PM ***** LOSS printing *****
06/27 09:16:09 PM loss
06/27 09:16:09 PM tensor(1.8304, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:09 PM ***** LOSS printing *****
06/27 09:16:09 PM loss
06/27 09:16:09 PM tensor(2.2358, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:10 PM ***** LOSS printing *****
06/27 09:16:10 PM loss
06/27 09:16:10 PM tensor(1.6165, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:10 PM ***** LOSS printing *****
06/27 09:16:10 PM loss
06/27 09:16:10 PM tensor(1.4114, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:10 PM ***** LOSS printing *****
06/27 09:16:10 PM loss
06/27 09:16:10 PM tensor(1.3439, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:10 PM ***** Running evaluation MLM *****
06/27 09:16:10 PM   Epoch = 8 iter 259 step
06/27 09:16:10 PM   Num examples = 40
06/27 09:16:10 PM   Batch size = 32
06/27 09:16:12 PM ***** Eval results *****
06/27 09:16:12 PM   acc = 0.625
06/27 09:16:12 PM   cls_loss = 1.3655942678451538
06/27 09:16:12 PM   eval_loss = 1.7247780561447144
06/27 09:16:12 PM   global_step = 259
06/27 09:16:12 PM   loss = 1.3655942678451538
06/27 09:16:12 PM ***** LOSS printing *****
06/27 09:16:12 PM loss
06/27 09:16:12 PM tensor(1.2823, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:12 PM ***** LOSS printing *****
06/27 09:16:12 PM loss
06/27 09:16:12 PM tensor(1.4480, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:12 PM ***** LOSS printing *****
06/27 09:16:12 PM loss
06/27 09:16:12 PM tensor(1.0787, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:12 PM ***** LOSS printing *****
06/27 09:16:12 PM loss
06/27 09:16:12 PM tensor(1.4925, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:12 PM ***** LOSS printing *****
06/27 09:16:12 PM loss
06/27 09:16:12 PM tensor(1.4915, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:16:13 PM ***** Running evaluation MLM *****
06/27 09:16:13 PM   Epoch = 8 iter 264 step
06/27 09:16:13 PM   Num examples = 40
06/27 09:16:13 PM   Batch size = 32
06/27 09:16:14 PM ***** Eval results *****
06/27 09:16:14 PM   acc = 0.675
06/27 09:16:14 PM   cls_loss = 1.3641326328118641
06/27 09:16:14 PM   eval_loss = 1.7079241871833801
06/27 09:16:14 PM   global_step = 264
06/27 09:16:14 PM   loss = 1.3641326328118641
06/27 09:16:14 PM ***** Save model *****
06/27 09:16:14 PM ***** Test Dataset Eval Result *****
06/27 09:17:23 PM ***** Eval results *****
06/27 09:17:23 PM   acc = 0.5072398190045249
06/27 09:17:23 PM   cls_loss = 1.3641326328118641
06/27 09:17:23 PM   eval_loss = 2.538469176633017
06/27 09:17:23 PM   global_step = 264
06/27 09:17:23 PM   loss = 1.3641326328118641
06/27 09:17:27 PM ***** LOSS printing *****
06/27 09:17:27 PM loss
06/27 09:17:27 PM tensor(2.3001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:27 PM ***** LOSS printing *****
06/27 09:17:27 PM loss
06/27 09:17:27 PM tensor(1.9172, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:27 PM ***** LOSS printing *****
06/27 09:17:27 PM loss
06/27 09:17:27 PM tensor(1.4319, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:28 PM ***** LOSS printing *****
06/27 09:17:28 PM loss
06/27 09:17:28 PM tensor(1.3350, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:28 PM ***** LOSS printing *****
06/27 09:17:28 PM loss
06/27 09:17:28 PM tensor(1.4423, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:28 PM ***** Running evaluation MLM *****
06/27 09:17:28 PM   Epoch = 8 iter 269 step
06/27 09:17:28 PM   Num examples = 40
06/27 09:17:28 PM   Batch size = 32
06/27 09:17:29 PM ***** Eval results *****
06/27 09:17:29 PM   acc = 0.675
06/27 09:17:29 PM   cls_loss = 1.4195083667492043
06/27 09:17:29 PM   eval_loss = 1.8652757406234741
06/27 09:17:29 PM   global_step = 269
06/27 09:17:29 PM   loss = 1.4195083667492043
06/27 09:17:29 PM ***** LOSS printing *****
06/27 09:17:29 PM loss
06/27 09:17:29 PM tensor(1.9119, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:30 PM ***** LOSS printing *****
06/27 09:17:30 PM loss
06/27 09:17:30 PM tensor(0.8322, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:30 PM ***** LOSS printing *****
06/27 09:17:30 PM loss
06/27 09:17:30 PM tensor(0.9928, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:30 PM ***** LOSS printing *****
06/27 09:17:30 PM loss
06/27 09:17:30 PM tensor(1.1096, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:30 PM ***** LOSS printing *****
06/27 09:17:30 PM loss
06/27 09:17:30 PM tensor(1.1696, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:30 PM ***** Running evaluation MLM *****
06/27 09:17:30 PM   Epoch = 9 iter 274 step
06/27 09:17:30 PM   Num examples = 40
06/27 09:17:30 PM   Batch size = 32
06/27 09:17:32 PM ***** Eval results *****
06/27 09:17:32 PM   acc = 0.575
06/27 09:17:32 PM   cls_loss = 1.0260369032621384
06/27 09:17:32 PM   eval_loss = 2.0539652705192566
06/27 09:17:32 PM   global_step = 274
06/27 09:17:32 PM   loss = 1.0260369032621384
06/27 09:17:32 PM ***** LOSS printing *****
06/27 09:17:32 PM loss
06/27 09:17:32 PM tensor(1.5567, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:32 PM ***** LOSS printing *****
06/27 09:17:32 PM loss
06/27 09:17:32 PM tensor(0.8784, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:32 PM ***** LOSS printing *****
06/27 09:17:32 PM loss
06/27 09:17:32 PM tensor(1.2665, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:32 PM ***** LOSS printing *****
06/27 09:17:32 PM loss
06/27 09:17:32 PM tensor(0.9602, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:33 PM ***** LOSS printing *****
06/27 09:17:33 PM loss
06/27 09:17:33 PM tensor(1.3029, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:33 PM ***** Running evaluation MLM *****
06/27 09:17:33 PM   Epoch = 9 iter 279 step
06/27 09:17:33 PM   Num examples = 40
06/27 09:17:33 PM   Batch size = 32
06/27 09:17:34 PM ***** Eval results *****
06/27 09:17:34 PM   acc = 0.675
06/27 09:17:34 PM   cls_loss = 1.1187533934911091
06/27 09:17:34 PM   eval_loss = 2.0427642464637756
06/27 09:17:34 PM   global_step = 279
06/27 09:17:34 PM   loss = 1.1187533934911091
06/27 09:17:34 PM ***** LOSS printing *****
06/27 09:17:34 PM loss
06/27 09:17:34 PM tensor(1.5194, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:34 PM ***** LOSS printing *****
06/27 09:17:34 PM loss
06/27 09:17:34 PM tensor(1.5543, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:34 PM ***** LOSS printing *****
06/27 09:17:34 PM loss
06/27 09:17:34 PM tensor(1.4456, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:35 PM ***** LOSS printing *****
06/27 09:17:35 PM loss
06/27 09:17:35 PM tensor(1.4124, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:35 PM ***** LOSS printing *****
06/27 09:17:35 PM loss
06/27 09:17:35 PM tensor(1.2764, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:35 PM ***** Running evaluation MLM *****
06/27 09:17:35 PM   Epoch = 9 iter 284 step
06/27 09:17:35 PM   Num examples = 40
06/27 09:17:35 PM   Batch size = 32
06/27 09:17:36 PM ***** Eval results *****
06/27 09:17:36 PM   acc = 0.675
06/27 09:17:36 PM   cls_loss = 1.2340653198105949
06/27 09:17:36 PM   eval_loss = 1.9987839460372925
06/27 09:17:36 PM   global_step = 284
06/27 09:17:36 PM   loss = 1.2340653198105949
06/27 09:17:36 PM ***** LOSS printing *****
06/27 09:17:36 PM loss
06/27 09:17:36 PM tensor(1.0452, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:37 PM ***** LOSS printing *****
06/27 09:17:37 PM loss
06/27 09:17:37 PM tensor(1.6440, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:37 PM ***** LOSS printing *****
06/27 09:17:37 PM loss
06/27 09:17:37 PM tensor(1.6953, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:37 PM ***** LOSS printing *****
06/27 09:17:37 PM loss
06/27 09:17:37 PM tensor(1.3566, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:37 PM ***** LOSS printing *****
06/27 09:17:37 PM loss
06/27 09:17:37 PM tensor(1.1588, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:37 PM ***** Running evaluation MLM *****
06/27 09:17:37 PM   Epoch = 9 iter 289 step
06/27 09:17:37 PM   Num examples = 40
06/27 09:17:37 PM   Batch size = 32
06/27 09:17:39 PM ***** Eval results *****
06/27 09:17:39 PM   acc = 0.65
06/27 09:17:39 PM   cls_loss = 1.2724594316984479
06/27 09:17:39 PM   eval_loss = 1.958851158618927
06/27 09:17:39 PM   global_step = 289
06/27 09:17:39 PM   loss = 1.2724594316984479
06/27 09:17:39 PM ***** LOSS printing *****
06/27 09:17:39 PM loss
06/27 09:17:39 PM tensor(1.4162, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:39 PM ***** LOSS printing *****
06/27 09:17:39 PM loss
06/27 09:17:39 PM tensor(1.1507, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:39 PM ***** LOSS printing *****
06/27 09:17:39 PM loss
06/27 09:17:39 PM tensor(1.2629, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:39 PM ***** LOSS printing *****
06/27 09:17:39 PM loss
06/27 09:17:39 PM tensor(1.2173, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:40 PM ***** LOSS printing *****
06/27 09:17:40 PM loss
06/27 09:17:40 PM tensor(1.2651, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:40 PM ***** Running evaluation MLM *****
06/27 09:17:40 PM   Epoch = 9 iter 294 step
06/27 09:17:40 PM   Num examples = 40
06/27 09:17:40 PM   Batch size = 32
06/27 09:17:41 PM ***** Eval results *****
06/27 09:17:41 PM   acc = 0.55
06/27 09:17:41 PM   cls_loss = 1.2703690975904465
06/27 09:17:41 PM   eval_loss = 1.8364238739013672
06/27 09:17:41 PM   global_step = 294
06/27 09:17:41 PM   loss = 1.2703690975904465
06/27 09:17:41 PM ***** LOSS printing *****
06/27 09:17:41 PM loss
06/27 09:17:41 PM tensor(1.1840, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:41 PM ***** LOSS printing *****
06/27 09:17:41 PM loss
06/27 09:17:41 PM tensor(1.4414, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:42 PM ***** LOSS printing *****
06/27 09:17:42 PM loss
06/27 09:17:42 PM tensor(1.1288, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:42 PM ***** LOSS printing *****
06/27 09:17:42 PM loss
06/27 09:17:42 PM tensor(1.7504, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:42 PM ***** LOSS printing *****
06/27 09:17:42 PM loss
06/27 09:17:42 PM tensor(1.4347, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:17:42 PM ***** Running evaluation MLM *****
06/27 09:17:42 PM   Epoch = 9 iter 299 step
06/27 09:17:42 PM   Num examples = 40
06/27 09:17:42 PM   Batch size = 32
06/27 09:17:43 PM ***** Eval results *****
06/27 09:17:43 PM   acc = 0.575
06/27 09:17:43 PM   cls_loss = 1.2906277919637745
06/27 09:17:43 PM   eval_loss = 1.89836847782135
06/27 09:17:43 PM   global_step = 299
06/27 09:17:43 PM   loss = 1.2906277919637745
06/27 09:17:43 PM ***** LOSS printing *****
06/27 09:17:43 PM loss
06/27 09:17:43 PM tensor(1.7112, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 08:57:54 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/sst-5/8-42/', data_seed=42, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='sst-5', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 08:57:54 PM device: cuda n_gpu: 1
06/27 08:57:54 PM Writing example 0 of 120
06/27 08:57:54 PM *** Example ***
06/27 08:57:54 PM guid: train-1
06/27 08:57:54 PM tokens: <s> more Ġof Ġthe Ġsame Ġfrom Ġt ai wan ese Ġa ute ur Ġts ai Ġming - li ang Ġ, Ġwhich Ġis Ġgood Ġnews Ġto Ġanyone Ġwho Ġ' s Ġfallen Ġunder Ġthe Ġsweet Ġ, Ġmelancholy Ġspell Ġof Ġthis Ġunique Ġdirector Ġ' s Ġprevious Ġfilms Ġ. </s> ĠIt Ġis <mask>
06/27 08:57:54 PM input_ids: 0 4321 9 5 276 31 326 1439 6531 4468 10 4467 710 42270 1439 35388 12 3572 1097 2156 61 16 205 340 7 1268 54 128 29 4491 223 5 4045 2156 40602 8921 9 42 2216 736 128 29 986 3541 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:54 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:54 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:54 PM label: ['Ġgood']
06/27 08:57:54 PM Writing example 0 of 40
06/27 08:57:54 PM *** Example ***
06/27 08:57:54 PM guid: dev-1
06/27 08:57:54 PM tokens: <s> re gg io Ġand Ġglass Ġput Ġon Ġan Ġintox icating Ġshow Ġ. </s> ĠIt Ġis <mask>
06/27 08:57:54 PM input_ids: 0 241 6149 1020 8 4049 342 15 41 40524 13659 311 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:54 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:54 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:54 PM label: ['Ġgood']
06/27 08:57:55 PM Writing example 0 of 2210
06/27 08:57:55 PM *** Example ***
06/27 08:57:55 PM guid: dev-1
06/27 08:57:55 PM tokens: <s> no Ġmovement Ġ, Ġno Ġy u ks Ġ, Ġnot Ġmuch Ġof Ġanything Ġ. </s> ĠIt Ġis <mask>
06/27 08:57:55 PM input_ids: 0 2362 2079 2156 117 1423 257 2258 2156 45 203 9 932 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:55 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:55 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 08:57:55 PM label: ['Ġbad']
06/27 08:58:08 PM ***** Running training *****
06/27 08:58:08 PM   Num examples = 120
06/27 08:58:08 PM   Batch size = 4
06/27 08:58:08 PM   Num steps = 300
06/27 08:58:08 PM n: embeddings.word_embeddings.weight
06/27 08:58:08 PM n: embeddings.position_embeddings.weight
06/27 08:58:08 PM n: embeddings.token_type_embeddings.weight
06/27 08:58:08 PM n: embeddings.LayerNorm.weight
06/27 08:58:08 PM n: embeddings.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.0.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.0.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.0.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.0.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.0.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.0.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.0.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.0.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.0.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.0.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.0.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.0.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.1.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.1.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.1.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.1.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.1.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.1.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.1.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.1.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.1.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.1.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.1.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.1.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.2.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.2.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.2.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.2.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.2.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.2.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.2.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.2.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.2.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.2.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.2.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.2.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.3.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.3.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.3.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.3.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.3.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.3.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.3.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.3.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.3.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.3.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.3.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.3.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.4.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.4.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.4.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.4.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.4.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.4.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.4.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.4.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.4.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.4.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.4.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.4.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.5.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.5.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.5.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.5.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.5.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.5.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.5.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.5.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.5.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.5.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.5.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.5.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.6.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.6.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.6.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.6.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.6.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.6.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.6.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.6.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.6.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.6.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.6.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.6.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.7.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.7.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.7.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.7.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.7.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.7.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.7.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.7.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.7.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.7.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.7.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.7.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.8.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.8.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.8.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.8.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.8.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.8.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.8.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.8.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.8.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.8.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.8.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.8.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.9.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.9.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.9.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.9.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.9.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.9.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.9.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.9.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.9.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.9.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.9.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.9.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.10.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.10.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.10.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.10.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.10.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.10.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.10.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.10.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.10.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.10.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.10.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.10.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.11.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.11.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.11.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.11.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.11.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.11.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.11.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.11.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.11.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.11.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.11.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.11.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.12.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.12.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.12.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.12.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.12.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.12.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.12.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.12.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.12.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.12.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.12.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.12.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.13.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.13.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.13.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.13.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.13.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.13.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.13.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.13.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.13.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.13.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.13.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.13.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.14.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.14.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.14.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.14.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.14.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.14.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.14.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.14.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.14.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.14.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.14.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.14.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.15.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.15.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.15.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.15.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.15.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.15.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.15.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.15.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.15.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.15.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.15.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.15.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.16.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.16.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.16.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.16.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.16.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.16.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.16.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.16.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.16.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.16.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.16.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.16.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.17.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.17.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.17.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.17.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.17.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.17.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.17.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.17.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.17.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.17.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.17.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.17.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.18.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.18.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.18.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.18.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.18.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.18.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.18.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.18.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.18.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.18.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.18.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.18.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.19.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.19.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.19.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.19.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.19.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.19.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.19.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.19.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.19.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.19.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.19.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.19.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.20.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.20.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.20.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.20.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.20.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.20.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.20.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.20.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.20.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.20.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.20.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.20.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.21.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.21.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.21.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.21.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.21.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.21.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.21.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.21.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.21.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.21.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.21.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.21.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.22.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.22.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.22.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.22.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.22.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.22.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.22.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.22.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.22.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.22.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.22.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.22.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.23.attention.self.query.weight
06/27 08:58:08 PM n: encoder.layer.23.attention.self.query.bias
06/27 08:58:08 PM n: encoder.layer.23.attention.self.key.weight
06/27 08:58:08 PM n: encoder.layer.23.attention.self.key.bias
06/27 08:58:08 PM n: encoder.layer.23.attention.self.value.weight
06/27 08:58:08 PM n: encoder.layer.23.attention.self.value.bias
06/27 08:58:08 PM n: encoder.layer.23.attention.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.23.attention.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: encoder.layer.23.intermediate.dense.weight
06/27 08:58:08 PM n: encoder.layer.23.intermediate.dense.bias
06/27 08:58:08 PM n: encoder.layer.23.output.dense.weight
06/27 08:58:08 PM n: encoder.layer.23.output.dense.bias
06/27 08:58:08 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 08:58:08 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 08:58:08 PM n: pooler.dense.weight
06/27 08:58:08 PM n: pooler.dense.bias
06/27 08:58:08 PM n: roberta.embeddings.word_embeddings.weight
06/27 08:58:08 PM n: roberta.embeddings.position_embeddings.weight
06/27 08:58:08 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 08:58:08 PM n: roberta.embeddings.LayerNorm.weight
06/27 08:58:08 PM n: roberta.embeddings.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 08:58:08 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 08:58:08 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 08:58:08 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 08:58:08 PM n: roberta.pooler.dense.weight
06/27 08:58:08 PM n: roberta.pooler.dense.bias
06/27 08:58:08 PM n: lm_head.bias
06/27 08:58:08 PM n: lm_head.dense.weight
06/27 08:58:08 PM n: lm_head.dense.bias
06/27 08:58:08 PM n: lm_head.layer_norm.weight
06/27 08:58:08 PM n: lm_head.layer_norm.bias
06/27 08:58:08 PM n: lm_head.decoder.weight
06/27 08:58:08 PM Total parameters: 763292761
06/27 08:58:08 PM ***** LOSS printing *****
06/27 08:58:08 PM loss
06/27 08:58:08 PM tensor(19.1448, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:58:08 PM ***** LOSS printing *****
06/27 08:58:08 PM loss
06/27 08:58:08 PM tensor(14.5774, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:58:09 PM ***** LOSS printing *****
06/27 08:58:09 PM loss
06/27 08:58:09 PM tensor(7.6465, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:58:09 PM ***** LOSS printing *****
06/27 08:58:09 PM loss
06/27 08:58:09 PM tensor(7.9581, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:58:09 PM ***** Running evaluation MLM *****
06/27 08:58:09 PM   Epoch = 0 iter 4 step
06/27 08:58:09 PM   Num examples = 40
06/27 08:58:09 PM   Batch size = 32
06/27 08:58:10 PM ***** Eval results *****
06/27 08:58:10 PM   acc = 0.35
06/27 08:58:10 PM   cls_loss = 12.331687092781067
06/27 08:58:10 PM   eval_loss = 3.8881314992904663
06/27 08:58:10 PM   global_step = 4
06/27 08:58:10 PM   loss = 12.331687092781067
06/27 08:58:10 PM ***** Save model *****
06/27 08:58:10 PM ***** Test Dataset Eval Result *****
06/27 08:59:19 PM ***** Eval results *****
06/27 08:59:19 PM   acc = 0.32941176470588235
06/27 08:59:19 PM   cls_loss = 12.331687092781067
06/27 08:59:19 PM   eval_loss = 4.1901039804731095
06/27 08:59:19 PM   global_step = 4
06/27 08:59:19 PM   loss = 12.331687092781067
06/27 08:59:23 PM ***** LOSS printing *****
06/27 08:59:23 PM loss
06/27 08:59:23 PM tensor(4.7491, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:23 PM ***** LOSS printing *****
06/27 08:59:23 PM loss
06/27 08:59:23 PM tensor(4.1793, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:23 PM ***** LOSS printing *****
06/27 08:59:23 PM loss
06/27 08:59:23 PM tensor(3.7581, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:24 PM ***** LOSS printing *****
06/27 08:59:24 PM loss
06/27 08:59:24 PM tensor(3.9784, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:24 PM ***** LOSS printing *****
06/27 08:59:24 PM loss
06/27 08:59:24 PM tensor(4.5576, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:24 PM ***** Running evaluation MLM *****
06/27 08:59:24 PM   Epoch = 0 iter 9 step
06/27 08:59:24 PM   Num examples = 40
06/27 08:59:24 PM   Batch size = 32
06/27 08:59:25 PM ***** Eval results *****
06/27 08:59:25 PM   acc = 0.275
06/27 08:59:25 PM   cls_loss = 7.83879476123386
06/27 08:59:25 PM   eval_loss = 2.309139370918274
06/27 08:59:25 PM   global_step = 9
06/27 08:59:25 PM   loss = 7.83879476123386
06/27 08:59:25 PM ***** LOSS printing *****
06/27 08:59:25 PM loss
06/27 08:59:25 PM tensor(3.6846, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:26 PM ***** LOSS printing *****
06/27 08:59:26 PM loss
06/27 08:59:26 PM tensor(4.2825, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:26 PM ***** LOSS printing *****
06/27 08:59:26 PM loss
06/27 08:59:26 PM tensor(4.9185, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:26 PM ***** LOSS printing *****
06/27 08:59:26 PM loss
06/27 08:59:26 PM tensor(3.0847, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:26 PM ***** LOSS printing *****
06/27 08:59:26 PM loss
06/27 08:59:26 PM tensor(3.6306, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 08:59:26 PM ***** Running evaluation MLM *****
06/27 08:59:26 PM   Epoch = 0 iter 14 step
06/27 08:59:26 PM   Num examples = 40
06/27 08:59:26 PM   Batch size = 32
06/27 08:59:28 PM ***** Eval results *****
06/27 08:59:28 PM   acc = 0.475
06/27 08:59:28 PM   cls_loss = 6.439279658453805
06/27 08:59:28 PM   eval_loss = 2.590024173259735
06/27 08:59:28 PM   global_step = 14
06/27 08:59:28 PM   loss = 6.439279658453805
06/27 08:59:28 PM ***** Save model *****
06/27 08:59:28 PM ***** Test Dataset Eval Result *****
06/27 09:00:37 PM ***** Eval results *****
06/27 09:00:37 PM   acc = 0.34615384615384615
06/27 09:00:37 PM   cls_loss = 6.439279658453805
06/27 09:00:37 PM   eval_loss = 3.4215194157191684
06/27 09:00:37 PM   global_step = 14
06/27 09:00:37 PM   loss = 6.439279658453805
06/27 09:00:41 PM ***** LOSS printing *****
06/27 09:00:41 PM loss
06/27 09:00:41 PM tensor(3.1943, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:41 PM ***** LOSS printing *****
06/27 09:00:41 PM loss
06/27 09:00:41 PM tensor(3.5799, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:41 PM ***** LOSS printing *****
06/27 09:00:41 PM loss
06/27 09:00:41 PM tensor(2.4591, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:41 PM ***** LOSS printing *****
06/27 09:00:41 PM loss
06/27 09:00:41 PM tensor(2.5149, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:42 PM ***** LOSS printing *****
06/27 09:00:42 PM loss
06/27 09:00:42 PM tensor(4.1917, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:42 PM ***** Running evaluation MLM *****
06/27 09:00:42 PM   Epoch = 0 iter 19 step
06/27 09:00:42 PM   Num examples = 40
06/27 09:00:42 PM   Batch size = 32
06/27 09:00:43 PM ***** Eval results *****
06/27 09:00:43 PM   acc = 0.225
06/27 09:00:43 PM   cls_loss = 5.583672046661377
06/27 09:00:43 PM   eval_loss = 2.7295587062835693
06/27 09:00:43 PM   global_step = 19
06/27 09:00:43 PM   loss = 5.583672046661377
06/27 09:00:43 PM ***** LOSS printing *****
06/27 09:00:43 PM loss
06/27 09:00:43 PM tensor(4.0215, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:43 PM ***** LOSS printing *****
06/27 09:00:43 PM loss
06/27 09:00:43 PM tensor(3.8713, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:43 PM ***** LOSS printing *****
06/27 09:00:43 PM loss
06/27 09:00:43 PM tensor(3.8669, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:44 PM ***** LOSS printing *****
06/27 09:00:44 PM loss
06/27 09:00:44 PM tensor(2.6474, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:44 PM ***** LOSS printing *****
06/27 09:00:44 PM loss
06/27 09:00:44 PM tensor(3.6486, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:44 PM ***** Running evaluation MLM *****
06/27 09:00:44 PM   Epoch = 0 iter 24 step
06/27 09:00:44 PM   Num examples = 40
06/27 09:00:44 PM   Batch size = 32
06/27 09:00:45 PM ***** Eval results *****
06/27 09:00:45 PM   acc = 0.4
06/27 09:00:45 PM   cls_loss = 5.172730664412181
06/27 09:00:45 PM   eval_loss = 2.6368987560272217
06/27 09:00:45 PM   global_step = 24
06/27 09:00:45 PM   loss = 5.172730664412181
06/27 09:00:45 PM ***** LOSS printing *****
06/27 09:00:45 PM loss
06/27 09:00:45 PM tensor(2.9340, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:46 PM ***** LOSS printing *****
06/27 09:00:46 PM loss
06/27 09:00:46 PM tensor(3.5448, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:46 PM ***** LOSS printing *****
06/27 09:00:46 PM loss
06/27 09:00:46 PM tensor(3.5151, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:46 PM ***** LOSS printing *****
06/27 09:00:46 PM loss
06/27 09:00:46 PM tensor(3.3254, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:46 PM ***** LOSS printing *****
06/27 09:00:46 PM loss
06/27 09:00:46 PM tensor(2.4954, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:46 PM ***** Running evaluation MLM *****
06/27 09:00:46 PM   Epoch = 0 iter 29 step
06/27 09:00:46 PM   Num examples = 40
06/27 09:00:46 PM   Batch size = 32
06/27 09:00:48 PM ***** Eval results *****
06/27 09:00:48 PM   acc = 0.375
06/27 09:00:48 PM   cls_loss = 4.826216039986446
06/27 09:00:48 PM   eval_loss = 2.249455213546753
06/27 09:00:48 PM   global_step = 29
06/27 09:00:48 PM   loss = 4.826216039986446
06/27 09:00:48 PM ***** LOSS printing *****
06/27 09:00:48 PM loss
06/27 09:00:48 PM tensor(3.3744, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:48 PM ***** LOSS printing *****
06/27 09:00:48 PM loss
06/27 09:00:48 PM tensor(2.0142, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:48 PM ***** LOSS printing *****
06/27 09:00:48 PM loss
06/27 09:00:48 PM tensor(2.5879, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:48 PM ***** LOSS printing *****
06/27 09:00:48 PM loss
06/27 09:00:48 PM tensor(2.2904, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:49 PM ***** LOSS printing *****
06/27 09:00:49 PM loss
06/27 09:00:49 PM tensor(2.5484, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:49 PM ***** Running evaluation MLM *****
06/27 09:00:49 PM   Epoch = 1 iter 34 step
06/27 09:00:49 PM   Num examples = 40
06/27 09:00:49 PM   Batch size = 32
06/27 09:00:50 PM ***** Eval results *****
06/27 09:00:50 PM   acc = 0.3
06/27 09:00:50 PM   cls_loss = 2.360210597515106
06/27 09:00:50 PM   eval_loss = 2.507707118988037
06/27 09:00:50 PM   global_step = 34
06/27 09:00:50 PM   loss = 2.360210597515106
06/27 09:00:50 PM ***** LOSS printing *****
06/27 09:00:50 PM loss
06/27 09:00:50 PM tensor(2.9397, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:50 PM ***** LOSS printing *****
06/27 09:00:50 PM loss
06/27 09:00:50 PM tensor(2.8477, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:50 PM ***** LOSS printing *****
06/27 09:00:50 PM loss
06/27 09:00:50 PM tensor(2.4880, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:51 PM ***** LOSS printing *****
06/27 09:00:51 PM loss
06/27 09:00:51 PM tensor(1.3433, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:51 PM ***** LOSS printing *****
06/27 09:00:51 PM loss
06/27 09:00:51 PM tensor(2.3575, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:51 PM ***** Running evaluation MLM *****
06/27 09:00:51 PM   Epoch = 1 iter 39 step
06/27 09:00:51 PM   Num examples = 40
06/27 09:00:51 PM   Batch size = 32
06/27 09:00:52 PM ***** Eval results *****
06/27 09:00:52 PM   acc = 0.425
06/27 09:00:52 PM   cls_loss = 2.379675679736667
06/27 09:00:52 PM   eval_loss = 1.6055216491222382
06/27 09:00:52 PM   global_step = 39
06/27 09:00:52 PM   loss = 2.379675679736667
06/27 09:00:52 PM ***** LOSS printing *****
06/27 09:00:52 PM loss
06/27 09:00:52 PM tensor(1.5702, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:53 PM ***** LOSS printing *****
06/27 09:00:53 PM loss
06/27 09:00:53 PM tensor(3.3695, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:53 PM ***** LOSS printing *****
06/27 09:00:53 PM loss
06/27 09:00:53 PM tensor(2.4129, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:53 PM ***** LOSS printing *****
06/27 09:00:53 PM loss
06/27 09:00:53 PM tensor(2.7535, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:53 PM ***** LOSS printing *****
06/27 09:00:53 PM loss
06/27 09:00:53 PM tensor(2.1819, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:53 PM ***** Running evaluation MLM *****
06/27 09:00:53 PM   Epoch = 1 iter 44 step
06/27 09:00:53 PM   Num examples = 40
06/27 09:00:53 PM   Batch size = 32
06/27 09:00:55 PM ***** Eval results *****
06/27 09:00:55 PM   acc = 0.475
06/27 09:00:55 PM   cls_loss = 2.407512068748474
06/27 09:00:55 PM   eval_loss = 1.633478045463562
06/27 09:00:55 PM   global_step = 44
06/27 09:00:55 PM   loss = 2.407512068748474
06/27 09:00:55 PM ***** LOSS printing *****
06/27 09:00:55 PM loss
06/27 09:00:55 PM tensor(4.2644, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:55 PM ***** LOSS printing *****
06/27 09:00:55 PM loss
06/27 09:00:55 PM tensor(3.1403, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:55 PM ***** LOSS printing *****
06/27 09:00:55 PM loss
06/27 09:00:55 PM tensor(3.6573, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:55 PM ***** LOSS printing *****
06/27 09:00:55 PM loss
06/27 09:00:55 PM tensor(4.3629, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:56 PM ***** LOSS printing *****
06/27 09:00:56 PM loss
06/27 09:00:56 PM tensor(2.8559, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:00:56 PM ***** Running evaluation MLM *****
06/27 09:00:56 PM   Epoch = 1 iter 49 step
06/27 09:00:56 PM   Num examples = 40
06/27 09:00:56 PM   Batch size = 32
06/27 09:00:57 PM ***** Eval results *****
06/27 09:00:57 PM   acc = 0.6
06/27 09:00:57 PM   cls_loss = 2.736102304960552
06/27 09:00:57 PM   eval_loss = 1.7333019971847534
06/27 09:00:57 PM   global_step = 49
06/27 09:00:57 PM   loss = 2.736102304960552
06/27 09:00:57 PM ***** Save model *****
06/27 09:00:57 PM ***** Test Dataset Eval Result *****
06/27 09:02:06 PM ***** Eval results *****
06/27 09:02:06 PM   acc = 0.48823529411764705
06/27 09:02:06 PM   cls_loss = 2.736102304960552
06/27 09:02:06 PM   eval_loss = 2.4368443965911863
06/27 09:02:06 PM   global_step = 49
06/27 09:02:06 PM   loss = 2.736102304960552
06/27 09:02:10 PM ***** LOSS printing *****
06/27 09:02:10 PM loss
06/27 09:02:10 PM tensor(3.4729, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:10 PM ***** LOSS printing *****
06/27 09:02:10 PM loss
06/27 09:02:10 PM tensor(3.0858, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:11 PM ***** LOSS printing *****
06/27 09:02:11 PM loss
06/27 09:02:11 PM tensor(1.1570, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:11 PM ***** LOSS printing *****
06/27 09:02:11 PM loss
06/27 09:02:11 PM tensor(2.8038, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:11 PM ***** LOSS printing *****
06/27 09:02:11 PM loss
06/27 09:02:11 PM tensor(2.1380, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:11 PM ***** Running evaluation MLM *****
06/27 09:02:11 PM   Epoch = 1 iter 54 step
06/27 09:02:11 PM   Num examples = 40
06/27 09:02:11 PM   Batch size = 32
06/27 09:02:12 PM ***** Eval results *****
06/27 09:02:12 PM   acc = 0.425
06/27 09:02:12 PM   cls_loss = 2.6934710989395776
06/27 09:02:12 PM   eval_loss = 2.734574556350708
06/27 09:02:12 PM   global_step = 54
06/27 09:02:12 PM   loss = 2.6934710989395776
06/27 09:02:13 PM ***** LOSS printing *****
06/27 09:02:13 PM loss
06/27 09:02:13 PM tensor(2.3097, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:13 PM ***** LOSS printing *****
06/27 09:02:13 PM loss
06/27 09:02:13 PM tensor(2.4900, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:13 PM ***** LOSS printing *****
06/27 09:02:13 PM loss
06/27 09:02:13 PM tensor(2.3031, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:13 PM ***** LOSS printing *****
06/27 09:02:13 PM loss
06/27 09:02:13 PM tensor(1.7695, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:13 PM ***** LOSS printing *****
06/27 09:02:13 PM loss
06/27 09:02:13 PM tensor(3.6131, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:14 PM ***** Running evaluation MLM *****
06/27 09:02:14 PM   Epoch = 1 iter 59 step
06/27 09:02:14 PM   Num examples = 40
06/27 09:02:14 PM   Batch size = 32
06/27 09:02:15 PM ***** Eval results *****
06/27 09:02:15 PM   acc = 0.525
06/27 09:02:15 PM   cls_loss = 2.659608302445247
06/27 09:02:15 PM   eval_loss = 3.349485397338867
06/27 09:02:15 PM   global_step = 59
06/27 09:02:15 PM   loss = 2.659608302445247
06/27 09:02:15 PM ***** LOSS printing *****
06/27 09:02:15 PM loss
06/27 09:02:15 PM tensor(2.5554, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:15 PM ***** LOSS printing *****
06/27 09:02:15 PM loss
06/27 09:02:15 PM tensor(1.8397, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:15 PM ***** LOSS printing *****
06/27 09:02:15 PM loss
06/27 09:02:15 PM tensor(1.6680, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:16 PM ***** LOSS printing *****
06/27 09:02:16 PM loss
06/27 09:02:16 PM tensor(1.8480, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:16 PM ***** LOSS printing *****
06/27 09:02:16 PM loss
06/27 09:02:16 PM tensor(2.3810, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:16 PM ***** Running evaluation MLM *****
06/27 09:02:16 PM   Epoch = 2 iter 64 step
06/27 09:02:16 PM   Num examples = 40
06/27 09:02:16 PM   Batch size = 32
06/27 09:02:17 PM ***** Eval results *****
06/27 09:02:17 PM   acc = 0.55
06/27 09:02:17 PM   cls_loss = 1.9341650307178497
06/27 09:02:17 PM   eval_loss = 2.854336738586426
06/27 09:02:17 PM   global_step = 64
06/27 09:02:17 PM   loss = 1.9341650307178497
06/27 09:02:17 PM ***** LOSS printing *****
06/27 09:02:17 PM loss
06/27 09:02:17 PM tensor(2.7297, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:17 PM ***** LOSS printing *****
06/27 09:02:17 PM loss
06/27 09:02:17 PM tensor(4.1525, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:18 PM ***** LOSS printing *****
06/27 09:02:18 PM loss
06/27 09:02:18 PM tensor(1.1758, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:18 PM ***** LOSS printing *****
06/27 09:02:18 PM loss
06/27 09:02:18 PM tensor(2.6492, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:18 PM ***** LOSS printing *****
06/27 09:02:18 PM loss
06/27 09:02:18 PM tensor(2.0189, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:18 PM ***** Running evaluation MLM *****
06/27 09:02:18 PM   Epoch = 2 iter 69 step
06/27 09:02:18 PM   Num examples = 40
06/27 09:02:18 PM   Batch size = 32
06/27 09:02:20 PM ***** Eval results *****
06/27 09:02:20 PM   acc = 0.5
06/27 09:02:20 PM   cls_loss = 2.2736235989464655
06/27 09:02:20 PM   eval_loss = 2.2799386382102966
06/27 09:02:20 PM   global_step = 69
06/27 09:02:20 PM   loss = 2.2736235989464655
06/27 09:02:20 PM ***** LOSS printing *****
06/27 09:02:20 PM loss
06/27 09:02:20 PM tensor(2.2939, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:20 PM ***** LOSS printing *****
06/27 09:02:20 PM loss
06/27 09:02:20 PM tensor(1.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:20 PM ***** LOSS printing *****
06/27 09:02:20 PM loss
06/27 09:02:20 PM tensor(1.8162, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:20 PM ***** LOSS printing *****
06/27 09:02:20 PM loss
06/27 09:02:20 PM tensor(1.7211, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:20 PM ***** LOSS printing *****
06/27 09:02:20 PM loss
06/27 09:02:20 PM tensor(2.2284, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:21 PM ***** Running evaluation MLM *****
06/27 09:02:21 PM   Epoch = 2 iter 74 step
06/27 09:02:21 PM   Num examples = 40
06/27 09:02:21 PM   Batch size = 32
06/27 09:02:22 PM ***** Eval results *****
06/27 09:02:22 PM   acc = 0.375
06/27 09:02:22 PM   cls_loss = 2.15403436762946
06/27 09:02:22 PM   eval_loss = 2.059875786304474
06/27 09:02:22 PM   global_step = 74
06/27 09:02:22 PM   loss = 2.15403436762946
06/27 09:02:22 PM ***** LOSS printing *****
06/27 09:02:22 PM loss
06/27 09:02:22 PM tensor(1.8208, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:22 PM ***** LOSS printing *****
06/27 09:02:22 PM loss
06/27 09:02:22 PM tensor(2.4924, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:22 PM ***** LOSS printing *****
06/27 09:02:22 PM loss
06/27 09:02:22 PM tensor(2.4249, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:23 PM ***** LOSS printing *****
06/27 09:02:23 PM loss
06/27 09:02:23 PM tensor(1.8690, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:23 PM ***** LOSS printing *****
06/27 09:02:23 PM loss
06/27 09:02:23 PM tensor(1.2067, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:23 PM ***** Running evaluation MLM *****
06/27 09:02:23 PM   Epoch = 2 iter 79 step
06/27 09:02:23 PM   Num examples = 40
06/27 09:02:23 PM   Batch size = 32
06/27 09:02:24 PM ***** Eval results *****
06/27 09:02:24 PM   acc = 0.4
06/27 09:02:24 PM   cls_loss = 2.103702187538147
06/27 09:02:24 PM   eval_loss = 1.971845269203186
06/27 09:02:24 PM   global_step = 79
06/27 09:02:24 PM   loss = 2.103702187538147
06/27 09:02:24 PM ***** LOSS printing *****
06/27 09:02:24 PM loss
06/27 09:02:24 PM tensor(2.3222, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:24 PM ***** LOSS printing *****
06/27 09:02:24 PM loss
06/27 09:02:24 PM tensor(1.4727, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:25 PM ***** LOSS printing *****
06/27 09:02:25 PM loss
06/27 09:02:25 PM tensor(2.2848, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:25 PM ***** LOSS printing *****
06/27 09:02:25 PM loss
06/27 09:02:25 PM tensor(1.9355, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:25 PM ***** LOSS printing *****
06/27 09:02:25 PM loss
06/27 09:02:25 PM tensor(1.6111, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:25 PM ***** Running evaluation MLM *****
06/27 09:02:25 PM   Epoch = 2 iter 84 step
06/27 09:02:25 PM   Num examples = 40
06/27 09:02:25 PM   Batch size = 32
06/27 09:02:27 PM ***** Eval results *****
06/27 09:02:27 PM   acc = 0.475
06/27 09:02:27 PM   cls_loss = 2.06652661661307
06/27 09:02:27 PM   eval_loss = 1.8676241040229797
06/27 09:02:27 PM   global_step = 84
06/27 09:02:27 PM   loss = 2.06652661661307
06/27 09:02:27 PM ***** LOSS printing *****
06/27 09:02:27 PM loss
06/27 09:02:27 PM tensor(1.6024, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:27 PM ***** LOSS printing *****
06/27 09:02:27 PM loss
06/27 09:02:27 PM tensor(1.8264, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:27 PM ***** LOSS printing *****
06/27 09:02:27 PM loss
06/27 09:02:27 PM tensor(2.2433, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:27 PM ***** LOSS printing *****
06/27 09:02:27 PM loss
06/27 09:02:27 PM tensor(2.2473, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:27 PM ***** LOSS printing *****
06/27 09:02:27 PM loss
06/27 09:02:27 PM tensor(1.5562, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:28 PM ***** Running evaluation MLM *****
06/27 09:02:28 PM   Epoch = 2 iter 89 step
06/27 09:02:28 PM   Num examples = 40
06/27 09:02:28 PM   Batch size = 32
06/27 09:02:29 PM ***** Eval results *****
06/27 09:02:29 PM   acc = 0.55
06/27 09:02:29 PM   cls_loss = 2.036974758937441
06/27 09:02:29 PM   eval_loss = 1.8263095021247864
06/27 09:02:29 PM   global_step = 89
06/27 09:02:29 PM   loss = 2.036974758937441
06/27 09:02:29 PM ***** LOSS printing *****
06/27 09:02:29 PM loss
06/27 09:02:29 PM tensor(2.4948, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:29 PM ***** LOSS printing *****
06/27 09:02:29 PM loss
06/27 09:02:29 PM tensor(2.2802, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:29 PM ***** LOSS printing *****
06/27 09:02:29 PM loss
06/27 09:02:29 PM tensor(1.0225, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:30 PM ***** LOSS printing *****
06/27 09:02:30 PM loss
06/27 09:02:30 PM tensor(1.7763, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:30 PM ***** LOSS printing *****
06/27 09:02:30 PM loss
06/27 09:02:30 PM tensor(1.7381, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:30 PM ***** Running evaluation MLM *****
06/27 09:02:30 PM   Epoch = 3 iter 94 step
06/27 09:02:30 PM   Num examples = 40
06/27 09:02:30 PM   Batch size = 32
06/27 09:02:31 PM ***** Eval results *****
06/27 09:02:31 PM   acc = 0.5
06/27 09:02:31 PM   cls_loss = 1.7042939960956573
06/27 09:02:31 PM   eval_loss = 1.9651901721954346
06/27 09:02:31 PM   global_step = 94
06/27 09:02:31 PM   loss = 1.7042939960956573
06/27 09:02:31 PM ***** LOSS printing *****
06/27 09:02:31 PM loss
06/27 09:02:31 PM tensor(1.5446, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:32 PM ***** LOSS printing *****
06/27 09:02:32 PM loss
06/27 09:02:32 PM tensor(1.1972, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:32 PM ***** LOSS printing *****
06/27 09:02:32 PM loss
06/27 09:02:32 PM tensor(1.5246, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:32 PM ***** LOSS printing *****
06/27 09:02:32 PM loss
06/27 09:02:32 PM tensor(1.5024, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:32 PM ***** LOSS printing *****
06/27 09:02:32 PM loss
06/27 09:02:32 PM tensor(1.3251, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:02:32 PM ***** Running evaluation MLM *****
06/27 09:02:32 PM   Epoch = 3 iter 99 step
06/27 09:02:32 PM   Num examples = 40
06/27 09:02:32 PM   Batch size = 32
06/27 09:02:34 PM ***** Eval results *****
06/27 09:02:34 PM   acc = 0.625
06/27 09:02:34 PM   cls_loss = 1.545680046081543
06/27 09:02:34 PM   eval_loss = 1.9619715213775635
06/27 09:02:34 PM   global_step = 99
06/27 09:02:34 PM   loss = 1.545680046081543
06/27 09:02:34 PM ***** Save model *****
06/27 09:02:34 PM ***** Test Dataset Eval Result *****
06/27 09:03:43 PM ***** Eval results *****
06/27 09:03:43 PM   acc = 0.4583710407239819
06/27 09:03:43 PM   cls_loss = 1.545680046081543
06/27 09:03:43 PM   eval_loss = 2.889222141674587
06/27 09:03:43 PM   global_step = 99
06/27 09:03:43 PM   loss = 1.545680046081543
06/27 09:03:47 PM ***** LOSS printing *****
06/27 09:03:47 PM loss
06/27 09:03:47 PM tensor(1.5238, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:47 PM ***** LOSS printing *****
06/27 09:03:47 PM loss
06/27 09:03:47 PM tensor(1.6760, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:47 PM ***** LOSS printing *****
06/27 09:03:47 PM loss
06/27 09:03:47 PM tensor(1.4176, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:48 PM ***** LOSS printing *****
06/27 09:03:48 PM loss
06/27 09:03:48 PM tensor(1.1839, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:48 PM ***** LOSS printing *****
06/27 09:03:48 PM loss
06/27 09:03:48 PM tensor(2.0022, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:48 PM ***** Running evaluation MLM *****
06/27 09:03:48 PM   Epoch = 3 iter 104 step
06/27 09:03:48 PM   Num examples = 40
06/27 09:03:48 PM   Batch size = 32
06/27 09:03:49 PM ***** Eval results *****
06/27 09:03:49 PM   acc = 0.55
06/27 09:03:49 PM   cls_loss = 1.5510390060288566
06/27 09:03:49 PM   eval_loss = 2.1032001972198486
06/27 09:03:49 PM   global_step = 104
06/27 09:03:49 PM   loss = 1.5510390060288566
06/27 09:03:49 PM ***** LOSS printing *****
06/27 09:03:49 PM loss
06/27 09:03:49 PM tensor(1.3962, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:50 PM ***** LOSS printing *****
06/27 09:03:50 PM loss
06/27 09:03:50 PM tensor(2.2433, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:50 PM ***** LOSS printing *****
06/27 09:03:50 PM loss
06/27 09:03:50 PM tensor(2.0364, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:50 PM ***** LOSS printing *****
06/27 09:03:50 PM loss
06/27 09:03:50 PM tensor(1.9706, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:50 PM ***** LOSS printing *****
06/27 09:03:50 PM loss
06/27 09:03:50 PM tensor(1.5245, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:50 PM ***** Running evaluation MLM *****
06/27 09:03:50 PM   Epoch = 3 iter 109 step
06/27 09:03:50 PM   Num examples = 40
06/27 09:03:50 PM   Batch size = 32
06/27 09:03:52 PM ***** Eval results *****
06/27 09:03:52 PM   acc = 0.575
06/27 09:03:52 PM   cls_loss = 1.6255528613140708
06/27 09:03:52 PM   eval_loss = 2.124268412590027
06/27 09:03:52 PM   global_step = 109
06/27 09:03:52 PM   loss = 1.6255528613140708
06/27 09:03:52 PM ***** LOSS printing *****
06/27 09:03:52 PM loss
06/27 09:03:52 PM tensor(2.4803, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:52 PM ***** LOSS printing *****
06/27 09:03:52 PM loss
06/27 09:03:52 PM tensor(1.7310, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:52 PM ***** LOSS printing *****
06/27 09:03:52 PM loss
06/27 09:03:52 PM tensor(2.1758, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:52 PM ***** LOSS printing *****
06/27 09:03:52 PM loss
06/27 09:03:52 PM tensor(2.9965, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:53 PM ***** LOSS printing *****
06/27 09:03:53 PM loss
06/27 09:03:53 PM tensor(1.9142, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:53 PM ***** Running evaluation MLM *****
06/27 09:03:53 PM   Epoch = 3 iter 114 step
06/27 09:03:53 PM   Num examples = 40
06/27 09:03:53 PM   Batch size = 32
06/27 09:03:54 PM ***** Eval results *****
06/27 09:03:54 PM   acc = 0.525
06/27 09:03:54 PM   cls_loss = 1.75763638317585
06/27 09:03:54 PM   eval_loss = 1.844658374786377
06/27 09:03:54 PM   global_step = 114
06/27 09:03:54 PM   loss = 1.75763638317585
06/27 09:03:54 PM ***** LOSS printing *****
06/27 09:03:54 PM loss
06/27 09:03:54 PM tensor(2.2067, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:54 PM ***** LOSS printing *****
06/27 09:03:54 PM loss
06/27 09:03:54 PM tensor(1.9011, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:54 PM ***** LOSS printing *****
06/27 09:03:54 PM loss
06/27 09:03:54 PM tensor(1.6181, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:55 PM ***** LOSS printing *****
06/27 09:03:55 PM loss
06/27 09:03:55 PM tensor(2.2828, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:55 PM ***** LOSS printing *****
06/27 09:03:55 PM loss
06/27 09:03:55 PM tensor(1.2177, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:55 PM ***** Running evaluation MLM *****
06/27 09:03:55 PM   Epoch = 3 iter 119 step
06/27 09:03:55 PM   Num examples = 40
06/27 09:03:55 PM   Batch size = 32
06/27 09:03:56 PM ***** Eval results *****
06/27 09:03:56 PM   acc = 0.5
06/27 09:03:56 PM   cls_loss = 1.772747833153297
06/27 09:03:56 PM   eval_loss = 1.8229977786540985
06/27 09:03:56 PM   global_step = 119
06/27 09:03:56 PM   loss = 1.772747833153297
06/27 09:03:56 PM ***** LOSS printing *****
06/27 09:03:56 PM loss
06/27 09:03:56 PM tensor(2.2672, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:57 PM ***** LOSS printing *****
06/27 09:03:57 PM loss
06/27 09:03:57 PM tensor(1.8180, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:57 PM ***** LOSS printing *****
06/27 09:03:57 PM loss
06/27 09:03:57 PM tensor(1.0220, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:57 PM ***** LOSS printing *****
06/27 09:03:57 PM loss
06/27 09:03:57 PM tensor(1.5949, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:57 PM ***** LOSS printing *****
06/27 09:03:57 PM loss
06/27 09:03:57 PM tensor(1.2497, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:57 PM ***** Running evaluation MLM *****
06/27 09:03:57 PM   Epoch = 4 iter 124 step
06/27 09:03:57 PM   Num examples = 40
06/27 09:03:57 PM   Batch size = 32
06/27 09:03:59 PM ***** Eval results *****
06/27 09:03:59 PM   acc = 0.5
06/27 09:03:59 PM   cls_loss = 1.421131819486618
06/27 09:03:59 PM   eval_loss = 1.6200473010540009
06/27 09:03:59 PM   global_step = 124
06/27 09:03:59 PM   loss = 1.421131819486618
06/27 09:03:59 PM ***** LOSS printing *****
06/27 09:03:59 PM loss
06/27 09:03:59 PM tensor(2.6981, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:59 PM ***** LOSS printing *****
06/27 09:03:59 PM loss
06/27 09:03:59 PM tensor(1.8569, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:59 PM ***** LOSS printing *****
06/27 09:03:59 PM loss
06/27 09:03:59 PM tensor(1.6421, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:03:59 PM ***** LOSS printing *****
06/27 09:03:59 PM loss
06/27 09:03:59 PM tensor(2.0652, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:00 PM ***** LOSS printing *****
06/27 09:04:00 PM loss
06/27 09:04:00 PM tensor(0.9163, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:00 PM ***** Running evaluation MLM *****
06/27 09:04:00 PM   Epoch = 4 iter 129 step
06/27 09:04:00 PM   Num examples = 40
06/27 09:04:00 PM   Batch size = 32
06/27 09:04:01 PM ***** Eval results *****
06/27 09:04:01 PM   acc = 0.575
06/27 09:04:01 PM   cls_loss = 1.6514635350969102
06/27 09:04:01 PM   eval_loss = 1.5339411497116089
06/27 09:04:01 PM   global_step = 129
06/27 09:04:01 PM   loss = 1.6514635350969102
06/27 09:04:01 PM ***** LOSS printing *****
06/27 09:04:01 PM loss
06/27 09:04:01 PM tensor(1.8540, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:01 PM ***** LOSS printing *****
06/27 09:04:01 PM loss
06/27 09:04:01 PM tensor(2.0281, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:01 PM ***** LOSS printing *****
06/27 09:04:01 PM loss
06/27 09:04:01 PM tensor(1.8299, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:02 PM ***** LOSS printing *****
06/27 09:04:02 PM loss
06/27 09:04:02 PM tensor(1.6879, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:02 PM ***** LOSS printing *****
06/27 09:04:02 PM loss
06/27 09:04:02 PM tensor(1.7587, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:02 PM ***** Running evaluation MLM *****
06/27 09:04:02 PM   Epoch = 4 iter 134 step
06/27 09:04:02 PM   Num examples = 40
06/27 09:04:02 PM   Batch size = 32
06/27 09:04:03 PM ***** Eval results *****
06/27 09:04:03 PM   acc = 0.6
06/27 09:04:03 PM   cls_loss = 1.7158344302858626
06/27 09:04:03 PM   eval_loss = 1.49689120054245
06/27 09:04:03 PM   global_step = 134
06/27 09:04:03 PM   loss = 1.7158344302858626
06/27 09:04:03 PM ***** LOSS printing *****
06/27 09:04:03 PM loss
06/27 09:04:03 PM tensor(1.8338, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:04 PM ***** LOSS printing *****
06/27 09:04:04 PM loss
06/27 09:04:04 PM tensor(1.7236, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:04 PM ***** LOSS printing *****
06/27 09:04:04 PM loss
06/27 09:04:04 PM tensor(2.0922, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:04 PM ***** LOSS printing *****
06/27 09:04:04 PM loss
06/27 09:04:04 PM tensor(1.7891, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:04 PM ***** LOSS printing *****
06/27 09:04:04 PM loss
06/27 09:04:04 PM tensor(1.5023, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:04 PM ***** Running evaluation MLM *****
06/27 09:04:04 PM   Epoch = 4 iter 139 step
06/27 09:04:04 PM   Num examples = 40
06/27 09:04:04 PM   Batch size = 32
06/27 09:04:06 PM ***** Eval results *****
06/27 09:04:06 PM   acc = 0.5
06/27 09:04:06 PM   cls_loss = 1.7348784772973311
06/27 09:04:06 PM   eval_loss = 1.714674949645996
06/27 09:04:06 PM   global_step = 139
06/27 09:04:06 PM   loss = 1.7348784772973311
06/27 09:04:06 PM ***** LOSS printing *****
06/27 09:04:06 PM loss
06/27 09:04:06 PM tensor(1.8187, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:06 PM ***** LOSS printing *****
06/27 09:04:06 PM loss
06/27 09:04:06 PM tensor(1.5958, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:06 PM ***** LOSS printing *****
06/27 09:04:06 PM loss
06/27 09:04:06 PM tensor(2.2666, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:06 PM ***** LOSS printing *****
06/27 09:04:06 PM loss
06/27 09:04:06 PM tensor(1.3564, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:07 PM ***** LOSS printing *****
06/27 09:04:07 PM loss
06/27 09:04:07 PM tensor(1.7286, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:07 PM ***** Running evaluation MLM *****
06/27 09:04:07 PM   Epoch = 4 iter 144 step
06/27 09:04:07 PM   Num examples = 40
06/27 09:04:07 PM   Batch size = 32
06/27 09:04:08 PM ***** Eval results *****
06/27 09:04:08 PM   acc = 0.6
06/27 09:04:08 PM   cls_loss = 1.73869485159715
06/27 09:04:08 PM   eval_loss = 2.0705045461654663
06/27 09:04:08 PM   global_step = 144
06/27 09:04:08 PM   loss = 1.73869485159715
06/27 09:04:08 PM ***** LOSS printing *****
06/27 09:04:08 PM loss
06/27 09:04:08 PM tensor(1.6146, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:08 PM ***** LOSS printing *****
06/27 09:04:08 PM loss
06/27 09:04:08 PM tensor(1.7367, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:09 PM ***** LOSS printing *****
06/27 09:04:09 PM loss
06/27 09:04:09 PM tensor(1.3426, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:09 PM ***** LOSS printing *****
06/27 09:04:09 PM loss
06/27 09:04:09 PM tensor(1.6642, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:09 PM ***** LOSS printing *****
06/27 09:04:09 PM loss
06/27 09:04:09 PM tensor(2.1520, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:09 PM ***** Running evaluation MLM *****
06/27 09:04:09 PM   Epoch = 4 iter 149 step
06/27 09:04:09 PM   Num examples = 40
06/27 09:04:09 PM   Batch size = 32
06/27 09:04:10 PM ***** Eval results *****
06/27 09:04:10 PM   acc = 0.525
06/27 09:04:10 PM   cls_loss = 1.7323705533455158
06/27 09:04:10 PM   eval_loss = 2.401596426963806
06/27 09:04:10 PM   global_step = 149
06/27 09:04:10 PM   loss = 1.7323705533455158
06/27 09:04:10 PM ***** LOSS printing *****
06/27 09:04:10 PM loss
06/27 09:04:10 PM tensor(1.7157, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:11 PM ***** LOSS printing *****
06/27 09:04:11 PM loss
06/27 09:04:11 PM tensor(1.7031, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:11 PM ***** LOSS printing *****
06/27 09:04:11 PM loss
06/27 09:04:11 PM tensor(2.4923, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:11 PM ***** LOSS printing *****
06/27 09:04:11 PM loss
06/27 09:04:11 PM tensor(1.3783, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:11 PM ***** LOSS printing *****
06/27 09:04:11 PM loss
06/27 09:04:11 PM tensor(2.1342, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:11 PM ***** Running evaluation MLM *****
06/27 09:04:11 PM   Epoch = 5 iter 154 step
06/27 09:04:11 PM   Num examples = 40
06/27 09:04:11 PM   Batch size = 32
06/27 09:04:13 PM ***** Eval results *****
06/27 09:04:13 PM   acc = 0.5
06/27 09:04:13 PM   cls_loss = 1.9269841313362122
06/27 09:04:13 PM   eval_loss = 2.318059802055359
06/27 09:04:13 PM   global_step = 154
06/27 09:04:13 PM   loss = 1.9269841313362122
06/27 09:04:13 PM ***** LOSS printing *****
06/27 09:04:13 PM loss
06/27 09:04:13 PM tensor(1.2505, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:13 PM ***** LOSS printing *****
06/27 09:04:13 PM loss
06/27 09:04:13 PM tensor(1.3220, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:13 PM ***** LOSS printing *****
06/27 09:04:13 PM loss
06/27 09:04:13 PM tensor(1.3074, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:13 PM ***** LOSS printing *****
06/27 09:04:13 PM loss
06/27 09:04:13 PM tensor(1.5448, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:14 PM ***** LOSS printing *****
06/27 09:04:14 PM loss
06/27 09:04:14 PM tensor(1.8241, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:14 PM ***** Running evaluation MLM *****
06/27 09:04:14 PM   Epoch = 5 iter 159 step
06/27 09:04:14 PM   Num examples = 40
06/27 09:04:14 PM   Batch size = 32
06/27 09:04:15 PM ***** Eval results *****
06/27 09:04:15 PM   acc = 0.55
06/27 09:04:15 PM   cls_loss = 1.6618557373682659
06/27 09:04:15 PM   eval_loss = 1.922129511833191
06/27 09:04:15 PM   global_step = 159
06/27 09:04:15 PM   loss = 1.6618557373682659
06/27 09:04:15 PM ***** LOSS printing *****
06/27 09:04:15 PM loss
06/27 09:04:15 PM tensor(1.7543, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:15 PM ***** LOSS printing *****
06/27 09:04:15 PM loss
06/27 09:04:15 PM tensor(1.7447, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:16 PM ***** LOSS printing *****
06/27 09:04:16 PM loss
06/27 09:04:16 PM tensor(1.0811, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:16 PM ***** LOSS printing *****
06/27 09:04:16 PM loss
06/27 09:04:16 PM tensor(1.7869, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:16 PM ***** LOSS printing *****
06/27 09:04:16 PM loss
06/27 09:04:16 PM tensor(1.8867, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:16 PM ***** Running evaluation MLM *****
06/27 09:04:16 PM   Epoch = 5 iter 164 step
06/27 09:04:16 PM   Num examples = 40
06/27 09:04:16 PM   Batch size = 32
06/27 09:04:17 PM ***** Eval results *****
06/27 09:04:17 PM   acc = 0.625
06/27 09:04:17 PM   cls_loss = 1.6578882506915502
06/27 09:04:17 PM   eval_loss = 1.752604365348816
06/27 09:04:17 PM   global_step = 164
06/27 09:04:17 PM   loss = 1.6578882506915502
06/27 09:04:17 PM ***** LOSS printing *****
06/27 09:04:17 PM loss
06/27 09:04:17 PM tensor(1.3399, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:18 PM ***** LOSS printing *****
06/27 09:04:18 PM loss
06/27 09:04:18 PM tensor(2.0957, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:18 PM ***** LOSS printing *****
06/27 09:04:18 PM loss
06/27 09:04:18 PM tensor(1.6176, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:18 PM ***** LOSS printing *****
06/27 09:04:18 PM loss
06/27 09:04:18 PM tensor(1.4148, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:18 PM ***** LOSS printing *****
06/27 09:04:18 PM loss
06/27 09:04:18 PM tensor(1.9965, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:19 PM ***** Running evaluation MLM *****
06/27 09:04:19 PM   Epoch = 5 iter 169 step
06/27 09:04:19 PM   Num examples = 40
06/27 09:04:19 PM   Batch size = 32
06/27 09:04:20 PM ***** Eval results *****
06/27 09:04:20 PM   acc = 0.525
06/27 09:04:20 PM   cls_loss = 1.6671016529986733
06/27 09:04:20 PM   eval_loss = 1.7325060963630676
06/27 09:04:20 PM   global_step = 169
06/27 09:04:20 PM   loss = 1.6671016529986733
06/27 09:04:20 PM ***** LOSS printing *****
06/27 09:04:20 PM loss
06/27 09:04:20 PM tensor(1.5476, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:20 PM ***** LOSS printing *****
06/27 09:04:20 PM loss
06/27 09:04:20 PM tensor(1.6642, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:20 PM ***** LOSS printing *****
06/27 09:04:20 PM loss
06/27 09:04:20 PM tensor(2.0179, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:20 PM ***** LOSS printing *****
06/27 09:04:20 PM loss
06/27 09:04:20 PM tensor(1.5339, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:21 PM ***** LOSS printing *****
06/27 09:04:21 PM loss
06/27 09:04:21 PM tensor(2.0662, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:21 PM ***** Running evaluation MLM *****
06/27 09:04:21 PM   Epoch = 5 iter 174 step
06/27 09:04:21 PM   Num examples = 40
06/27 09:04:21 PM   Batch size = 32
06/27 09:04:22 PM ***** Eval results *****
06/27 09:04:22 PM   acc = 0.525
06/27 09:04:22 PM   cls_loss = 1.6876950810352962
06/27 09:04:22 PM   eval_loss = 1.8969340920448303
06/27 09:04:22 PM   global_step = 174
06/27 09:04:22 PM   loss = 1.6876950810352962
06/27 09:04:22 PM ***** LOSS printing *****
06/27 09:04:22 PM loss
06/27 09:04:22 PM tensor(1.7527, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:22 PM ***** LOSS printing *****
06/27 09:04:22 PM loss
06/27 09:04:22 PM tensor(2.2787, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:23 PM ***** LOSS printing *****
06/27 09:04:23 PM loss
06/27 09:04:23 PM tensor(1.4520, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:23 PM ***** LOSS printing *****
06/27 09:04:23 PM loss
06/27 09:04:23 PM tensor(1.4279, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:23 PM ***** LOSS printing *****
06/27 09:04:23 PM loss
06/27 09:04:23 PM tensor(1.4697, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:23 PM ***** Running evaluation MLM *****
06/27 09:04:23 PM   Epoch = 5 iter 179 step
06/27 09:04:23 PM   Num examples = 40
06/27 09:04:23 PM   Batch size = 32
06/27 09:04:24 PM ***** Eval results *****
06/27 09:04:24 PM   acc = 0.625
06/27 09:04:24 PM   cls_loss = 1.6857122922765797
06/27 09:04:24 PM   eval_loss = 2.2228128910064697
06/27 09:04:24 PM   global_step = 179
06/27 09:04:24 PM   loss = 1.6857122922765797
06/27 09:04:25 PM ***** LOSS printing *****
06/27 09:04:25 PM loss
06/27 09:04:25 PM tensor(1.4292, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:25 PM ***** LOSS printing *****
06/27 09:04:25 PM loss
06/27 09:04:25 PM tensor(1.5170, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:25 PM ***** LOSS printing *****
06/27 09:04:25 PM loss
06/27 09:04:25 PM tensor(1.1618, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:25 PM ***** LOSS printing *****
06/27 09:04:25 PM loss
06/27 09:04:25 PM tensor(1.7388, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:25 PM ***** LOSS printing *****
06/27 09:04:25 PM loss
06/27 09:04:25 PM tensor(1.4382, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:26 PM ***** Running evaluation MLM *****
06/27 09:04:26 PM   Epoch = 6 iter 184 step
06/27 09:04:26 PM   Num examples = 40
06/27 09:04:26 PM   Batch size = 32
06/27 09:04:27 PM ***** Eval results *****
06/27 09:04:27 PM   acc = 0.6
06/27 09:04:27 PM   cls_loss = 1.4639424979686737
06/27 09:04:27 PM   eval_loss = 2.4899555444717407
06/27 09:04:27 PM   global_step = 184
06/27 09:04:27 PM   loss = 1.4639424979686737
06/27 09:04:27 PM ***** LOSS printing *****
06/27 09:04:27 PM loss
06/27 09:04:27 PM tensor(1.5070, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:27 PM ***** LOSS printing *****
06/27 09:04:27 PM loss
06/27 09:04:27 PM tensor(1.4763, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:27 PM ***** LOSS printing *****
06/27 09:04:27 PM loss
06/27 09:04:27 PM tensor(1.3753, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:28 PM ***** LOSS printing *****
06/27 09:04:28 PM loss
06/27 09:04:28 PM tensor(1.4580, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:28 PM ***** LOSS printing *****
06/27 09:04:28 PM loss
06/27 09:04:28 PM tensor(1.5575, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:28 PM ***** Running evaluation MLM *****
06/27 09:04:28 PM   Epoch = 6 iter 189 step
06/27 09:04:28 PM   Num examples = 40
06/27 09:04:28 PM   Batch size = 32
06/27 09:04:29 PM ***** Eval results *****
06/27 09:04:29 PM   acc = 0.6
06/27 09:04:29 PM   cls_loss = 1.4699815511703491
06/27 09:04:29 PM   eval_loss = 2.1599329113960266
06/27 09:04:29 PM   global_step = 189
06/27 09:04:29 PM   loss = 1.4699815511703491
06/27 09:04:29 PM ***** LOSS printing *****
06/27 09:04:29 PM loss
06/27 09:04:29 PM tensor(1.6162, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:29 PM ***** LOSS printing *****
06/27 09:04:29 PM loss
06/27 09:04:29 PM tensor(1.3740, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:30 PM ***** LOSS printing *****
06/27 09:04:30 PM loss
06/27 09:04:30 PM tensor(1.2345, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:30 PM ***** LOSS printing *****
06/27 09:04:30 PM loss
06/27 09:04:30 PM tensor(1.5375, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:30 PM ***** LOSS printing *****
06/27 09:04:30 PM loss
06/27 09:04:30 PM tensor(1.3509, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:30 PM ***** Running evaluation MLM *****
06/27 09:04:30 PM   Epoch = 6 iter 194 step
06/27 09:04:30 PM   Num examples = 40
06/27 09:04:30 PM   Batch size = 32
06/27 09:04:32 PM ***** Eval results *****
06/27 09:04:32 PM   acc = 0.5
06/27 09:04:32 PM   cls_loss = 1.4530696017401559
06/27 09:04:32 PM   eval_loss = 1.9741533398628235
06/27 09:04:32 PM   global_step = 194
06/27 09:04:32 PM   loss = 1.4530696017401559
06/27 09:04:32 PM ***** LOSS printing *****
06/27 09:04:32 PM loss
06/27 09:04:32 PM tensor(1.3714, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:32 PM ***** LOSS printing *****
06/27 09:04:32 PM loss
06/27 09:04:32 PM tensor(1.2448, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:32 PM ***** LOSS printing *****
06/27 09:04:32 PM loss
06/27 09:04:32 PM tensor(1.2418, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:32 PM ***** LOSS printing *****
06/27 09:04:32 PM loss
06/27 09:04:32 PM tensor(1.4281, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:32 PM ***** LOSS printing *****
06/27 09:04:32 PM loss
06/27 09:04:32 PM tensor(1.3806, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:33 PM ***** Running evaluation MLM *****
06/27 09:04:33 PM   Epoch = 6 iter 199 step
06/27 09:04:33 PM   Num examples = 40
06/27 09:04:33 PM   Batch size = 32
06/27 09:04:34 PM ***** Eval results *****
06/27 09:04:34 PM   acc = 0.55
06/27 09:04:34 PM   cls_loss = 1.4215630669342845
06/27 09:04:34 PM   eval_loss = 1.9425106048583984
06/27 09:04:34 PM   global_step = 199
06/27 09:04:34 PM   loss = 1.4215630669342845
06/27 09:04:34 PM ***** LOSS printing *****
06/27 09:04:34 PM loss
06/27 09:04:34 PM tensor(1.3968, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:34 PM ***** LOSS printing *****
06/27 09:04:34 PM loss
06/27 09:04:34 PM tensor(1.7227, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:34 PM ***** LOSS printing *****
06/27 09:04:34 PM loss
06/27 09:04:34 PM tensor(1.1297, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:35 PM ***** LOSS printing *****
06/27 09:04:35 PM loss
06/27 09:04:35 PM tensor(1.2612, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:35 PM ***** LOSS printing *****
06/27 09:04:35 PM loss
06/27 09:04:35 PM tensor(1.8059, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:35 PM ***** Running evaluation MLM *****
06/27 09:04:35 PM   Epoch = 6 iter 204 step
06/27 09:04:35 PM   Num examples = 40
06/27 09:04:35 PM   Batch size = 32
06/27 09:04:36 PM ***** Eval results *****
06/27 09:04:36 PM   acc = 0.45
06/27 09:04:36 PM   cls_loss = 1.4302465816338856
06/27 09:04:36 PM   eval_loss = 1.9040607213974
06/27 09:04:36 PM   global_step = 204
06/27 09:04:36 PM   loss = 1.4302465816338856
06/27 09:04:36 PM ***** LOSS printing *****
06/27 09:04:36 PM loss
06/27 09:04:36 PM tensor(1.2240, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:36 PM ***** LOSS printing *****
06/27 09:04:36 PM loss
06/27 09:04:36 PM tensor(1.7958, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:37 PM ***** LOSS printing *****
06/27 09:04:37 PM loss
06/27 09:04:37 PM tensor(1.4065, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:37 PM ***** LOSS printing *****
06/27 09:04:37 PM loss
06/27 09:04:37 PM tensor(1.5629, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:37 PM ***** LOSS printing *****
06/27 09:04:37 PM loss
06/27 09:04:37 PM tensor(1.3483, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:37 PM ***** Running evaluation MLM *****
06/27 09:04:37 PM   Epoch = 6 iter 209 step
06/27 09:04:37 PM   Num examples = 40
06/27 09:04:37 PM   Batch size = 32
06/27 09:04:39 PM ***** Eval results *****
06/27 09:04:39 PM   acc = 0.5
06/27 09:04:39 PM   cls_loss = 1.4366692263504555
06/27 09:04:39 PM   eval_loss = 1.850507378578186
06/27 09:04:39 PM   global_step = 209
06/27 09:04:39 PM   loss = 1.4366692263504555
06/27 09:04:39 PM ***** LOSS printing *****
06/27 09:04:39 PM loss
06/27 09:04:39 PM tensor(1.4169, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:39 PM ***** LOSS printing *****
06/27 09:04:39 PM loss
06/27 09:04:39 PM tensor(1.3459, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:39 PM ***** LOSS printing *****
06/27 09:04:39 PM loss
06/27 09:04:39 PM tensor(1.8227, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:39 PM ***** LOSS printing *****
06/27 09:04:39 PM loss
06/27 09:04:39 PM tensor(1.1484, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:39 PM ***** LOSS printing *****
06/27 09:04:39 PM loss
06/27 09:04:39 PM tensor(0.9352, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:40 PM ***** Running evaluation MLM *****
06/27 09:04:40 PM   Epoch = 7 iter 214 step
06/27 09:04:40 PM   Num examples = 40
06/27 09:04:40 PM   Batch size = 32
06/27 09:04:41 PM ***** Eval results *****
06/27 09:04:41 PM   acc = 0.5
06/27 09:04:41 PM   cls_loss = 1.313051089644432
06/27 09:04:41 PM   eval_loss = 1.9002016186714172
06/27 09:04:41 PM   global_step = 214
06/27 09:04:41 PM   loss = 1.313051089644432
06/27 09:04:41 PM ***** LOSS printing *****
06/27 09:04:41 PM loss
06/27 09:04:41 PM tensor(0.9128, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:41 PM ***** LOSS printing *****
06/27 09:04:41 PM loss
06/27 09:04:41 PM tensor(1.3679, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:41 PM ***** LOSS printing *****
06/27 09:04:41 PM loss
06/27 09:04:41 PM tensor(1.1753, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:42 PM ***** LOSS printing *****
06/27 09:04:42 PM loss
06/27 09:04:42 PM tensor(1.9995, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:42 PM ***** LOSS printing *****
06/27 09:04:42 PM loss
06/27 09:04:42 PM tensor(1.0047, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:42 PM ***** Running evaluation MLM *****
06/27 09:04:42 PM   Epoch = 7 iter 219 step
06/27 09:04:42 PM   Num examples = 40
06/27 09:04:42 PM   Batch size = 32
06/27 09:04:43 PM ***** Eval results *****
06/27 09:04:43 PM   acc = 0.6
06/27 09:04:43 PM   cls_loss = 1.301374614238739
06/27 09:04:43 PM   eval_loss = 1.973783791065216
06/27 09:04:43 PM   global_step = 219
06/27 09:04:43 PM   loss = 1.301374614238739
06/27 09:04:43 PM ***** LOSS printing *****
06/27 09:04:43 PM loss
06/27 09:04:43 PM tensor(1.1763, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:44 PM ***** LOSS printing *****
06/27 09:04:44 PM loss
06/27 09:04:44 PM tensor(1.2190, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:44 PM ***** LOSS printing *****
06/27 09:04:44 PM loss
06/27 09:04:44 PM tensor(1.4701, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:44 PM ***** LOSS printing *****
06/27 09:04:44 PM loss
06/27 09:04:44 PM tensor(1.0159, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:44 PM ***** LOSS printing *****
06/27 09:04:44 PM loss
06/27 09:04:44 PM tensor(2.4895, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:44 PM ***** Running evaluation MLM *****
06/27 09:04:44 PM   Epoch = 7 iter 224 step
06/27 09:04:44 PM   Num examples = 40
06/27 09:04:44 PM   Batch size = 32
06/27 09:04:46 PM ***** Eval results *****
06/27 09:04:46 PM   acc = 0.525
06/27 09:04:46 PM   cls_loss = 1.363086508853095
06/27 09:04:46 PM   eval_loss = 1.9986339807510376
06/27 09:04:46 PM   global_step = 224
06/27 09:04:46 PM   loss = 1.363086508853095
06/27 09:04:46 PM ***** LOSS printing *****
06/27 09:04:46 PM loss
06/27 09:04:46 PM tensor(1.1799, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:46 PM ***** LOSS printing *****
06/27 09:04:46 PM loss
06/27 09:04:46 PM tensor(1.2155, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:46 PM ***** LOSS printing *****
06/27 09:04:46 PM loss
06/27 09:04:46 PM tensor(1.7357, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:46 PM ***** LOSS printing *****
06/27 09:04:46 PM loss
06/27 09:04:46 PM tensor(2.1522, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:47 PM ***** LOSS printing *****
06/27 09:04:47 PM loss
06/27 09:04:47 PM tensor(1.5465, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:47 PM ***** Running evaluation MLM *****
06/27 09:04:47 PM   Epoch = 7 iter 229 step
06/27 09:04:47 PM   Num examples = 40
06/27 09:04:47 PM   Batch size = 32
06/27 09:04:48 PM ***** Eval results *****
06/27 09:04:48 PM   acc = 0.55
06/27 09:04:48 PM   cls_loss = 1.4164753869960183
06/27 09:04:48 PM   eval_loss = 2.003529131412506
06/27 09:04:48 PM   global_step = 229
06/27 09:04:48 PM   loss = 1.4164753869960183
06/27 09:04:48 PM ***** LOSS printing *****
06/27 09:04:48 PM loss
06/27 09:04:48 PM tensor(1.0415, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:48 PM ***** LOSS printing *****
06/27 09:04:48 PM loss
06/27 09:04:48 PM tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:48 PM ***** LOSS printing *****
06/27 09:04:48 PM loss
06/27 09:04:48 PM tensor(1.3560, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:49 PM ***** LOSS printing *****
06/27 09:04:49 PM loss
06/27 09:04:49 PM tensor(1.2622, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:49 PM ***** LOSS printing *****
06/27 09:04:49 PM loss
06/27 09:04:49 PM tensor(1.8986, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:49 PM ***** Running evaluation MLM *****
06/27 09:04:49 PM   Epoch = 7 iter 234 step
06/27 09:04:49 PM   Num examples = 40
06/27 09:04:49 PM   Batch size = 32
06/27 09:04:50 PM ***** Eval results *****
06/27 09:04:50 PM   acc = 0.575
06/27 09:04:50 PM   cls_loss = 1.4217027947306633
06/27 09:04:50 PM   eval_loss = 1.871535837650299
06/27 09:04:50 PM   global_step = 234
06/27 09:04:50 PM   loss = 1.4217027947306633
06/27 09:04:50 PM ***** LOSS printing *****
06/27 09:04:50 PM loss
06/27 09:04:50 PM tensor(1.6013, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:51 PM ***** LOSS printing *****
06/27 09:04:51 PM loss
06/27 09:04:51 PM tensor(1.3501, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:51 PM ***** LOSS printing *****
06/27 09:04:51 PM loss
06/27 09:04:51 PM tensor(1.4801, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:51 PM ***** LOSS printing *****
06/27 09:04:51 PM loss
06/27 09:04:51 PM tensor(1.1787, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:51 PM ***** LOSS printing *****
06/27 09:04:51 PM loss
06/27 09:04:51 PM tensor(1.7533, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:51 PM ***** Running evaluation MLM *****
06/27 09:04:51 PM   Epoch = 7 iter 239 step
06/27 09:04:51 PM   Num examples = 40
06/27 09:04:51 PM   Batch size = 32
06/27 09:04:53 PM ***** Eval results *****
06/27 09:04:53 PM   acc = 0.475
06/27 09:04:53 PM   cls_loss = 1.4304951121067178
06/27 09:04:53 PM   eval_loss = 1.8291195631027222
06/27 09:04:53 PM   global_step = 239
06/27 09:04:53 PM   loss = 1.4304951121067178
06/27 09:04:53 PM ***** LOSS printing *****
06/27 09:04:53 PM loss
06/27 09:04:53 PM tensor(1.2623, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:53 PM ***** LOSS printing *****
06/27 09:04:53 PM loss
06/27 09:04:53 PM tensor(0.9556, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:53 PM ***** LOSS printing *****
06/27 09:04:53 PM loss
06/27 09:04:53 PM tensor(1.2065, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:53 PM ***** LOSS printing *****
06/27 09:04:53 PM loss
06/27 09:04:53 PM tensor(1.2540, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:54 PM ***** LOSS printing *****
06/27 09:04:54 PM loss
06/27 09:04:54 PM tensor(1.4685, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:54 PM ***** Running evaluation MLM *****
06/27 09:04:54 PM   Epoch = 8 iter 244 step
06/27 09:04:54 PM   Num examples = 40
06/27 09:04:54 PM   Batch size = 32
06/27 09:04:55 PM ***** Eval results *****
06/27 09:04:55 PM   acc = 0.45
06/27 09:04:55 PM   cls_loss = 1.2211408019065857
06/27 09:04:55 PM   eval_loss = 1.9051764011383057
06/27 09:04:55 PM   global_step = 244
06/27 09:04:55 PM   loss = 1.2211408019065857
06/27 09:04:55 PM ***** LOSS printing *****
06/27 09:04:55 PM loss
06/27 09:04:55 PM tensor(1.5779, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:55 PM ***** LOSS printing *****
06/27 09:04:55 PM loss
06/27 09:04:55 PM tensor(1.3476, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:55 PM ***** LOSS printing *****
06/27 09:04:55 PM loss
06/27 09:04:55 PM tensor(1.0204, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:56 PM ***** LOSS printing *****
06/27 09:04:56 PM loss
06/27 09:04:56 PM tensor(1.5278, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:56 PM ***** LOSS printing *****
06/27 09:04:56 PM loss
06/27 09:04:56 PM tensor(1.5886, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:56 PM ***** Running evaluation MLM *****
06/27 09:04:56 PM   Epoch = 8 iter 249 step
06/27 09:04:56 PM   Num examples = 40
06/27 09:04:56 PM   Batch size = 32
06/27 09:04:57 PM ***** Eval results *****
06/27 09:04:57 PM   acc = 0.5
06/27 09:04:57 PM   cls_loss = 1.327433745066325
06/27 09:04:57 PM   eval_loss = 2.114054024219513
06/27 09:04:57 PM   global_step = 249
06/27 09:04:57 PM   loss = 1.327433745066325
06/27 09:04:57 PM ***** LOSS printing *****
06/27 09:04:57 PM loss
06/27 09:04:57 PM tensor(1.2849, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:58 PM ***** LOSS printing *****
06/27 09:04:58 PM loss
06/27 09:04:58 PM tensor(1.8092, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:58 PM ***** LOSS printing *****
06/27 09:04:58 PM loss
06/27 09:04:58 PM tensor(1.7561, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:58 PM ***** LOSS printing *****
06/27 09:04:58 PM loss
06/27 09:04:58 PM tensor(1.2900, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:58 PM ***** LOSS printing *****
06/27 09:04:58 PM loss
06/27 09:04:58 PM tensor(1.9026, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:04:58 PM ***** Running evaluation MLM *****
06/27 09:04:58 PM   Epoch = 8 iter 254 step
06/27 09:04:58 PM   Num examples = 40
06/27 09:04:58 PM   Batch size = 32
06/27 09:05:00 PM ***** Eval results *****
06/27 09:05:00 PM   acc = 0.475
06/27 09:05:00 PM   cls_loss = 1.4278289335114616
06/27 09:05:00 PM   eval_loss = 2.113291025161743
06/27 09:05:00 PM   global_step = 254
06/27 09:05:00 PM   loss = 1.4278289335114616
06/27 09:05:00 PM ***** LOSS printing *****
06/27 09:05:00 PM loss
06/27 09:05:00 PM tensor(1.2059, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:00 PM ***** LOSS printing *****
06/27 09:05:00 PM loss
06/27 09:05:00 PM tensor(1.4551, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:00 PM ***** LOSS printing *****
06/27 09:05:00 PM loss
06/27 09:05:00 PM tensor(1.5900, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:00 PM ***** LOSS printing *****
06/27 09:05:00 PM loss
06/27 09:05:00 PM tensor(1.6819, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:01 PM ***** LOSS printing *****
06/27 09:05:01 PM loss
06/27 09:05:01 PM tensor(1.2924, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:01 PM ***** Running evaluation MLM *****
06/27 09:05:01 PM   Epoch = 8 iter 259 step
06/27 09:05:01 PM   Num examples = 40
06/27 09:05:01 PM   Batch size = 32
06/27 09:05:02 PM ***** Eval results *****
06/27 09:05:02 PM   acc = 0.525
06/27 09:05:02 PM   cls_loss = 1.4323604985287315
06/27 09:05:02 PM   eval_loss = 1.9225175976753235
06/27 09:05:02 PM   global_step = 259
06/27 09:05:02 PM   loss = 1.4323604985287315
06/27 09:05:02 PM ***** LOSS printing *****
06/27 09:05:02 PM loss
06/27 09:05:02 PM tensor(1.5751, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:02 PM ***** LOSS printing *****
06/27 09:05:02 PM loss
06/27 09:05:02 PM tensor(1.7025, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:03 PM ***** LOSS printing *****
06/27 09:05:03 PM loss
06/27 09:05:03 PM tensor(1.6836, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:03 PM ***** LOSS printing *****
06/27 09:05:03 PM loss
06/27 09:05:03 PM tensor(1.3324, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:03 PM ***** LOSS printing *****
06/27 09:05:03 PM loss
06/27 09:05:03 PM tensor(1.6058, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:03 PM ***** Running evaluation MLM *****
06/27 09:05:03 PM   Epoch = 8 iter 264 step
06/27 09:05:03 PM   Num examples = 40
06/27 09:05:03 PM   Batch size = 32
06/27 09:05:04 PM ***** Eval results *****
06/27 09:05:04 PM   acc = 0.55
06/27 09:05:04 PM   cls_loss = 1.4630954811970394
06/27 09:05:04 PM   eval_loss = 1.8418689966201782
06/27 09:05:04 PM   global_step = 264
06/27 09:05:04 PM   loss = 1.4630954811970394
06/27 09:05:04 PM ***** LOSS printing *****
06/27 09:05:04 PM loss
06/27 09:05:04 PM tensor(1.2269, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:05 PM ***** LOSS printing *****
06/27 09:05:05 PM loss
06/27 09:05:05 PM tensor(1.5637, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:05 PM ***** LOSS printing *****
06/27 09:05:05 PM loss
06/27 09:05:05 PM tensor(1.6300, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:05 PM ***** LOSS printing *****
06/27 09:05:05 PM loss
06/27 09:05:05 PM tensor(0.8904, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:05 PM ***** LOSS printing *****
06/27 09:05:05 PM loss
06/27 09:05:05 PM tensor(1.2430, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:06 PM ***** Running evaluation MLM *****
06/27 09:05:06 PM   Epoch = 8 iter 269 step
06/27 09:05:06 PM   Num examples = 40
06/27 09:05:06 PM   Batch size = 32
06/27 09:05:07 PM ***** Eval results *****
06/27 09:05:07 PM   acc = 0.55
06/27 09:05:07 PM   cls_loss = 1.4368414673311958
06/27 09:05:07 PM   eval_loss = 1.9126466512680054
06/27 09:05:07 PM   global_step = 269
06/27 09:05:07 PM   loss = 1.4368414673311958
06/27 09:05:07 PM ***** LOSS printing *****
06/27 09:05:07 PM loss
06/27 09:05:07 PM tensor(1.5006, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:07 PM ***** LOSS printing *****
06/27 09:05:07 PM loss
06/27 09:05:07 PM tensor(0.9265, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:07 PM ***** LOSS printing *****
06/27 09:05:07 PM loss
06/27 09:05:07 PM tensor(1.1106, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:07 PM ***** LOSS printing *****
06/27 09:05:07 PM loss
06/27 09:05:07 PM tensor(1.3447, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:08 PM ***** LOSS printing *****
06/27 09:05:08 PM loss
06/27 09:05:08 PM tensor(1.8898, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:08 PM ***** Running evaluation MLM *****
06/27 09:05:08 PM   Epoch = 9 iter 274 step
06/27 09:05:08 PM   Num examples = 40
06/27 09:05:08 PM   Batch size = 32
06/27 09:05:09 PM ***** Eval results *****
06/27 09:05:09 PM   acc = 0.575
06/27 09:05:09 PM   cls_loss = 1.3178993314504623
06/27 09:05:09 PM   eval_loss = 1.9280973076820374
06/27 09:05:09 PM   global_step = 274
06/27 09:05:09 PM   loss = 1.3178993314504623
06/27 09:05:09 PM ***** LOSS printing *****
06/27 09:05:09 PM loss
06/27 09:05:09 PM tensor(1.9384, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:09 PM ***** LOSS printing *****
06/27 09:05:09 PM loss
06/27 09:05:09 PM tensor(1.6438, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:10 PM ***** LOSS printing *****
06/27 09:05:10 PM loss
06/27 09:05:10 PM tensor(1.4433, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:10 PM ***** LOSS printing *****
06/27 09:05:10 PM loss
06/27 09:05:10 PM tensor(1.2814, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:10 PM ***** LOSS printing *****
06/27 09:05:10 PM loss
06/27 09:05:10 PM tensor(1.2136, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:10 PM ***** Running evaluation MLM *****
06/27 09:05:10 PM   Epoch = 9 iter 279 step
06/27 09:05:10 PM   Num examples = 40
06/27 09:05:10 PM   Batch size = 32
06/27 09:05:11 PM ***** Eval results *****
06/27 09:05:11 PM   acc = 0.575
06/27 09:05:11 PM   cls_loss = 1.4213591880268521
06/27 09:05:11 PM   eval_loss = 1.9318466782569885
06/27 09:05:11 PM   global_step = 279
06/27 09:05:11 PM   loss = 1.4213591880268521
06/27 09:05:12 PM ***** LOSS printing *****
06/27 09:05:12 PM loss
06/27 09:05:12 PM tensor(1.2236, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:12 PM ***** LOSS printing *****
06/27 09:05:12 PM loss
06/27 09:05:12 PM tensor(1.4286, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:12 PM ***** LOSS printing *****
06/27 09:05:12 PM loss
06/27 09:05:12 PM tensor(1.3428, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:12 PM ***** LOSS printing *****
06/27 09:05:12 PM loss
06/27 09:05:12 PM tensor(0.9523, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:12 PM ***** LOSS printing *****
06/27 09:05:12 PM loss
06/27 09:05:12 PM tensor(0.9327, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:13 PM ***** Running evaluation MLM *****
06/27 09:05:13 PM   Epoch = 9 iter 284 step
06/27 09:05:13 PM   Num examples = 40
06/27 09:05:13 PM   Batch size = 32
06/27 09:05:14 PM ***** Eval results *****
06/27 09:05:14 PM   acc = 0.575
06/27 09:05:14 PM   cls_loss = 1.3337358151163374
06/27 09:05:14 PM   eval_loss = 2.039043128490448
06/27 09:05:14 PM   global_step = 284
06/27 09:05:14 PM   loss = 1.3337358151163374
06/27 09:05:14 PM ***** LOSS printing *****
06/27 09:05:14 PM loss
06/27 09:05:14 PM tensor(1.3142, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:14 PM ***** LOSS printing *****
06/27 09:05:14 PM loss
06/27 09:05:14 PM tensor(1.1817, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:14 PM ***** LOSS printing *****
06/27 09:05:14 PM loss
06/27 09:05:14 PM tensor(1.4289, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:15 PM ***** LOSS printing *****
06/27 09:05:15 PM loss
06/27 09:05:15 PM tensor(1.9780, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:15 PM ***** LOSS printing *****
06/27 09:05:15 PM loss
06/27 09:05:15 PM tensor(0.7607, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:15 PM ***** Running evaluation MLM *****
06/27 09:05:15 PM   Epoch = 9 iter 289 step
06/27 09:05:15 PM   Num examples = 40
06/27 09:05:15 PM   Batch size = 32
06/27 09:05:16 PM ***** Eval results *****
06/27 09:05:16 PM   acc = 0.525
06/27 09:05:16 PM   cls_loss = 1.3334578275680542
06/27 09:05:16 PM   eval_loss = 2.1943402886390686
06/27 09:05:16 PM   global_step = 289
06/27 09:05:16 PM   loss = 1.3334578275680542
06/27 09:05:16 PM ***** LOSS printing *****
06/27 09:05:16 PM loss
06/27 09:05:16 PM tensor(1.5830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:16 PM ***** LOSS printing *****
06/27 09:05:16 PM loss
06/27 09:05:16 PM tensor(1.6260, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:17 PM ***** LOSS printing *****
06/27 09:05:17 PM loss
06/27 09:05:17 PM tensor(1.5320, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:17 PM ***** LOSS printing *****
06/27 09:05:17 PM loss
06/27 09:05:17 PM tensor(1.5477, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:17 PM ***** LOSS printing *****
06/27 09:05:17 PM loss
06/27 09:05:17 PM tensor(1.9874, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:17 PM ***** Running evaluation MLM *****
06/27 09:05:17 PM   Epoch = 9 iter 294 step
06/27 09:05:17 PM   Num examples = 40
06/27 09:05:17 PM   Batch size = 32
06/27 09:05:19 PM ***** Eval results *****
06/27 09:05:19 PM   acc = 0.55
06/27 09:05:19 PM   cls_loss = 1.4004902144273121
06/27 09:05:19 PM   eval_loss = 2.3295661211013794
06/27 09:05:19 PM   global_step = 294
06/27 09:05:19 PM   loss = 1.4004902144273121
06/27 09:05:19 PM ***** LOSS printing *****
06/27 09:05:19 PM loss
06/27 09:05:19 PM tensor(1.5636, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:19 PM ***** LOSS printing *****
06/27 09:05:19 PM loss
06/27 09:05:19 PM tensor(1.4763, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:19 PM ***** LOSS printing *****
06/27 09:05:19 PM loss
06/27 09:05:19 PM tensor(1.2327, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:19 PM ***** LOSS printing *****
06/27 09:05:19 PM loss
06/27 09:05:19 PM tensor(1.5620, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:19 PM ***** LOSS printing *****
06/27 09:05:19 PM loss
06/27 09:05:19 PM tensor(1.9960, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 09:05:20 PM ***** Running evaluation MLM *****
06/27 09:05:20 PM   Epoch = 9 iter 299 step
06/27 09:05:20 PM   Num examples = 40
06/27 09:05:20 PM   Batch size = 32
06/27 09:05:21 PM ***** Eval results *****
06/27 09:05:21 PM   acc = 0.5
06/27 09:05:21 PM   cls_loss = 1.429051164923043
06/27 09:05:21 PM   eval_loss = 2.33881676197052
06/27 09:05:21 PM   global_step = 299
06/27 09:05:21 PM   loss = 1.429051164923043
06/27 09:05:21 PM ***** LOSS printing *****
06/27 09:05:21 PM loss
06/27 09:05:21 PM tensor(1.5568, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 07:33:46 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-42/', data_seed=42, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 07:33:46 PM device: cuda n_gpu: 1
06/27 07:33:46 PM Writing example 0 of 48
06/27 07:33:46 PM *** Example ***
06/27 07:33:46 PM guid: train-1
06/27 07:33:46 PM tokens: <s> also Ġthe Ġbattery Ġlife Ġisn Ġ' t Ġgreat Ġbut Ġit Ġ' s Ġsufficient Ġfor Ġmy Ġneeds Ġ. </s> ĠIt Ġis <mask>
06/27 07:33:46 PM input_ids: 0 19726 5 3822 301 965 128 90 372 53 24 128 29 7719 13 127 782 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:46 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:46 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:46 PM label: ['Ġterrible']
06/27 07:33:46 PM Writing example 0 of 16
06/27 07:33:46 PM *** Example ***
06/27 07:33:46 PM guid: dev-1
06/27 07:33:46 PM tokens: <s> negative Ġ: Ġimp ossibly Ġtiny Ġand Ġdifficult Ġto Ġoperate Ġ, Ġbarely Ġvisible Ġ, Ġpower Ġbutton Ġ. </s> ĠIt Ġis <mask>
06/27 07:33:46 PM input_ids: 0 33407 4832 4023 39890 5262 8 1202 7 4303 2156 6254 7097 2156 476 6148 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:46 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:46 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:46 PM label: ['Ġterrible']
06/27 07:33:47 PM Writing example 0 of 2000
06/27 07:33:47 PM *** Example ***
06/27 07:33:47 PM guid: dev-1
06/27 07:33:47 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 07:33:47 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:47 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:47 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:33:47 PM label: ['Ġterrible']
06/27 07:34:00 PM ***** Running training *****
06/27 07:34:00 PM   Num examples = 48
06/27 07:34:00 PM   Batch size = 4
06/27 07:34:00 PM   Num steps = 120
06/27 07:34:00 PM n: embeddings.word_embeddings.weight
06/27 07:34:00 PM n: embeddings.position_embeddings.weight
06/27 07:34:00 PM n: embeddings.token_type_embeddings.weight
06/27 07:34:00 PM n: embeddings.LayerNorm.weight
06/27 07:34:00 PM n: embeddings.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.0.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.0.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.0.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.0.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.0.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.0.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.0.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.0.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.0.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.0.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.0.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.0.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.1.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.1.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.1.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.1.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.1.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.1.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.1.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.1.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.1.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.1.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.1.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.1.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.2.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.2.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.2.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.2.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.2.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.2.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.2.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.2.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.2.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.2.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.2.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.2.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.3.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.3.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.3.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.3.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.3.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.3.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.3.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.3.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.3.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.3.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.3.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.3.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.4.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.4.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.4.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.4.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.4.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.4.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.4.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.4.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.4.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.4.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.4.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.4.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.5.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.5.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.5.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.5.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.5.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.5.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.5.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.5.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.5.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.5.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.5.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.5.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.6.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.6.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.6.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.6.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.6.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.6.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.6.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.6.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.6.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.6.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.6.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.6.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.7.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.7.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.7.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.7.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.7.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.7.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.7.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.7.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.7.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.7.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.7.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.7.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.8.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.8.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.8.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.8.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.8.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.8.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.8.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.8.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.8.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.8.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.8.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.8.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.9.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.9.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.9.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.9.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.9.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.9.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.9.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.9.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.9.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.9.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.9.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.9.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.10.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.10.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.10.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.10.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.10.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.10.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.10.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.10.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.10.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.10.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.10.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.10.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.11.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.11.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.11.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.11.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.11.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.11.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.11.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.11.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.11.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.11.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.11.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.11.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.12.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.12.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.12.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.12.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.12.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.12.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.12.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.12.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.12.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.12.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.12.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.12.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.13.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.13.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.13.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.13.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.13.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.13.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.13.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.13.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.13.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.13.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.13.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.13.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.14.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.14.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.14.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.14.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.14.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.14.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.14.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.14.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.14.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.14.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.14.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.14.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.15.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.15.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.15.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.15.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.15.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.15.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.15.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.15.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.15.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.15.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.15.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.15.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.16.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.16.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.16.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.16.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.16.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.16.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.16.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.16.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.16.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.16.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.16.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.16.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.17.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.17.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.17.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.17.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.17.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.17.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.17.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.17.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.17.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.17.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.17.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.17.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.18.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.18.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.18.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.18.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.18.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.18.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.18.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.18.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.18.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.18.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.18.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.18.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.19.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.19.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.19.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.19.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.19.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.19.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.19.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.19.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.19.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.19.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.19.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.19.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.20.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.20.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.20.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.20.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.20.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.20.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.20.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.20.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.20.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.20.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.20.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.20.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.21.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.21.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.21.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.21.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.21.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.21.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.21.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.21.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.21.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.21.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.21.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.21.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.22.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.22.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.22.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.22.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.22.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.22.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.22.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.22.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.22.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.22.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.22.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.22.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.23.attention.self.query.weight
06/27 07:34:00 PM n: encoder.layer.23.attention.self.query.bias
06/27 07:34:00 PM n: encoder.layer.23.attention.self.key.weight
06/27 07:34:00 PM n: encoder.layer.23.attention.self.key.bias
06/27 07:34:00 PM n: encoder.layer.23.attention.self.value.weight
06/27 07:34:00 PM n: encoder.layer.23.attention.self.value.bias
06/27 07:34:00 PM n: encoder.layer.23.attention.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.23.attention.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: encoder.layer.23.intermediate.dense.weight
06/27 07:34:00 PM n: encoder.layer.23.intermediate.dense.bias
06/27 07:34:00 PM n: encoder.layer.23.output.dense.weight
06/27 07:34:00 PM n: encoder.layer.23.output.dense.bias
06/27 07:34:00 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 07:34:00 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 07:34:00 PM n: pooler.dense.weight
06/27 07:34:00 PM n: pooler.dense.bias
06/27 07:34:00 PM n: roberta.embeddings.word_embeddings.weight
06/27 07:34:00 PM n: roberta.embeddings.position_embeddings.weight
06/27 07:34:00 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 07:34:00 PM n: roberta.embeddings.LayerNorm.weight
06/27 07:34:00 PM n: roberta.embeddings.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 07:34:00 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 07:34:00 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 07:34:00 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 07:34:00 PM n: roberta.pooler.dense.weight
06/27 07:34:00 PM n: roberta.pooler.dense.bias
06/27 07:34:00 PM n: lm_head.bias
06/27 07:34:00 PM n: lm_head.dense.weight
06/27 07:34:00 PM n: lm_head.dense.bias
06/27 07:34:00 PM n: lm_head.layer_norm.weight
06/27 07:34:00 PM n: lm_head.layer_norm.bias
06/27 07:34:00 PM n: lm_head.decoder.weight
06/27 07:34:00 PM Total parameters: 763292761
06/27 07:34:00 PM ***** LOSS printing *****
06/27 07:34:00 PM loss
06/27 07:34:00 PM tensor(18.0584, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:34:00 PM ***** LOSS printing *****
06/27 07:34:00 PM loss
06/27 07:34:00 PM tensor(13.3096, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:34:00 PM ***** LOSS printing *****
06/27 07:34:00 PM loss
06/27 07:34:00 PM tensor(7.8855, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:34:00 PM ***** LOSS printing *****
06/27 07:34:00 PM loss
06/27 07:34:00 PM tensor(5.0018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:34:01 PM ***** Running evaluation MLM *****
06/27 07:34:01 PM   Epoch = 0 iter 4 step
06/27 07:34:01 PM   Num examples = 16
06/27 07:34:01 PM   Batch size = 32
06/27 07:34:01 PM ***** Eval results *****
06/27 07:34:01 PM   acc = 0.75
06/27 07:34:01 PM   cls_loss = 11.063828706741333
06/27 07:34:01 PM   eval_loss = 2.8594303131103516
06/27 07:34:01 PM   global_step = 4
06/27 07:34:01 PM   loss = 11.063828706741333
06/27 07:34:01 PM ***** Save model *****
06/27 07:34:01 PM ***** Test Dataset Eval Result *****
06/27 07:35:04 PM ***** Eval results *****
06/27 07:35:04 PM   acc = 0.6725
06/27 07:35:04 PM   cls_loss = 11.063828706741333
06/27 07:35:04 PM   eval_loss = 3.047227640000601
06/27 07:35:04 PM   global_step = 4
06/27 07:35:04 PM   loss = 11.063828706741333
06/27 07:35:08 PM ***** LOSS printing *****
06/27 07:35:08 PM loss
06/27 07:35:08 PM tensor(4.1804, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:09 PM ***** LOSS printing *****
06/27 07:35:09 PM loss
06/27 07:35:09 PM tensor(2.8465, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:09 PM ***** LOSS printing *****
06/27 07:35:09 PM loss
06/27 07:35:09 PM tensor(2.2986, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:09 PM ***** LOSS printing *****
06/27 07:35:09 PM loss
06/27 07:35:09 PM tensor(2.5031, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:09 PM ***** LOSS printing *****
06/27 07:35:09 PM loss
06/27 07:35:09 PM tensor(5.5960, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:09 PM ***** Running evaluation MLM *****
06/27 07:35:09 PM   Epoch = 0 iter 9 step
06/27 07:35:09 PM   Num examples = 16
06/27 07:35:09 PM   Batch size = 32
06/27 07:35:10 PM ***** Eval results *****
06/27 07:35:10 PM   acc = 0.75
06/27 07:35:10 PM   cls_loss = 6.853332334094578
06/27 07:35:10 PM   eval_loss = 3.2198288440704346
06/27 07:35:10 PM   global_step = 9
06/27 07:35:10 PM   loss = 6.853332334094578
06/27 07:35:10 PM ***** LOSS printing *****
06/27 07:35:10 PM loss
06/27 07:35:10 PM tensor(2.5266, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:10 PM ***** LOSS printing *****
06/27 07:35:10 PM loss
06/27 07:35:10 PM tensor(4.2165, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:10 PM ***** LOSS printing *****
06/27 07:35:10 PM loss
06/27 07:35:10 PM tensor(2.3607, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:11 PM ***** LOSS printing *****
06/27 07:35:11 PM loss
06/27 07:35:11 PM tensor(1.6059, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:11 PM ***** LOSS printing *****
06/27 07:35:11 PM loss
06/27 07:35:11 PM tensor(3.3390, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:11 PM ***** Running evaluation MLM *****
06/27 07:35:11 PM   Epoch = 1 iter 14 step
06/27 07:35:11 PM   Num examples = 16
06/27 07:35:11 PM   Batch size = 32
06/27 07:35:12 PM ***** Eval results *****
06/27 07:35:12 PM   acc = 0.6875
06/27 07:35:12 PM   cls_loss = 2.472432255744934
06/27 07:35:12 PM   eval_loss = 1.492879867553711
06/27 07:35:12 PM   global_step = 14
06/27 07:35:12 PM   loss = 2.472432255744934
06/27 07:35:12 PM ***** LOSS printing *****
06/27 07:35:12 PM loss
06/27 07:35:12 PM tensor(2.4808, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:12 PM ***** LOSS printing *****
06/27 07:35:12 PM loss
06/27 07:35:12 PM tensor(2.9128, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:12 PM ***** LOSS printing *****
06/27 07:35:12 PM loss
06/27 07:35:12 PM tensor(2.5767, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:12 PM ***** LOSS printing *****
06/27 07:35:12 PM loss
06/27 07:35:12 PM tensor(1.1376, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:12 PM ***** LOSS printing *****
06/27 07:35:12 PM loss
06/27 07:35:12 PM tensor(1.6466, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:35:13 PM ***** Running evaluation MLM *****
06/27 07:35:13 PM   Epoch = 1 iter 19 step
06/27 07:35:13 PM   Num examples = 16
06/27 07:35:13 PM   Batch size = 32
06/27 07:35:13 PM ***** Eval results *****
06/27 07:35:13 PM   acc = 0.875
06/27 07:35:13 PM   cls_loss = 2.2427737542561124
06/27 07:35:13 PM   eval_loss = 2.1497104167938232
06/27 07:35:13 PM   global_step = 19
06/27 07:35:13 PM   loss = 2.2427737542561124
06/27 07:35:13 PM ***** Save model *****
06/27 07:35:13 PM ***** Test Dataset Eval Result *****
06/27 07:36:16 PM ***** Eval results *****
06/27 07:36:16 PM   acc = 0.7545
06/27 07:36:16 PM   cls_loss = 2.2427737542561124
06/27 07:36:16 PM   eval_loss = 2.0396656725141735
06/27 07:36:16 PM   global_step = 19
06/27 07:36:16 PM   loss = 2.2427737542561124
06/27 07:36:20 PM ***** LOSS printing *****
06/27 07:36:20 PM loss
06/27 07:36:20 PM tensor(1.5596, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:21 PM ***** LOSS printing *****
06/27 07:36:21 PM loss
06/27 07:36:21 PM tensor(2.1959, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:21 PM ***** LOSS printing *****
06/27 07:36:21 PM loss
06/27 07:36:21 PM tensor(2.5579, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:21 PM ***** LOSS printing *****
06/27 07:36:21 PM loss
06/27 07:36:21 PM tensor(1.8604, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:21 PM ***** LOSS printing *****
06/27 07:36:21 PM loss
06/27 07:36:21 PM tensor(2.4581, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:21 PM ***** Running evaluation MLM *****
06/27 07:36:21 PM   Epoch = 1 iter 24 step
06/27 07:36:21 PM   Num examples = 16
06/27 07:36:21 PM   Batch size = 32
06/27 07:36:22 PM ***** Eval results *****
06/27 07:36:22 PM   acc = 0.875
06/27 07:36:22 PM   cls_loss = 2.1942771077156067
06/27 07:36:22 PM   eval_loss = 1.8480260372161865
06/27 07:36:22 PM   global_step = 24
06/27 07:36:22 PM   loss = 2.1942771077156067
06/27 07:36:22 PM ***** LOSS printing *****
06/27 07:36:22 PM loss
06/27 07:36:22 PM tensor(2.3776, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:22 PM ***** LOSS printing *****
06/27 07:36:22 PM loss
06/27 07:36:22 PM tensor(1.2291, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:22 PM ***** LOSS printing *****
06/27 07:36:22 PM loss
06/27 07:36:22 PM tensor(0.8126, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:23 PM ***** LOSS printing *****
06/27 07:36:23 PM loss
06/27 07:36:23 PM tensor(1.7303, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:23 PM ***** LOSS printing *****
06/27 07:36:23 PM loss
06/27 07:36:23 PM tensor(1.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:23 PM ***** Running evaluation MLM *****
06/27 07:36:23 PM   Epoch = 2 iter 29 step
06/27 07:36:23 PM   Num examples = 16
06/27 07:36:23 PM   Batch size = 32
06/27 07:36:24 PM ***** Eval results *****
06/27 07:36:24 PM   acc = 0.625
06/27 07:36:24 PM   cls_loss = 1.5809381127357482
06/27 07:36:24 PM   eval_loss = 2.4462902545928955
06/27 07:36:24 PM   global_step = 29
06/27 07:36:24 PM   loss = 1.5809381127357482
06/27 07:36:24 PM ***** LOSS printing *****
06/27 07:36:24 PM loss
06/27 07:36:24 PM tensor(1.3354, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:24 PM ***** LOSS printing *****
06/27 07:36:24 PM loss
06/27 07:36:24 PM tensor(1.8722, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:24 PM ***** LOSS printing *****
06/27 07:36:24 PM loss
06/27 07:36:24 PM tensor(1.9163, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:24 PM ***** LOSS printing *****
06/27 07:36:24 PM loss
06/27 07:36:24 PM tensor(1.3446, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:25 PM ***** LOSS printing *****
06/27 07:36:25 PM loss
06/27 07:36:25 PM tensor(1.3623, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:25 PM ***** Running evaluation MLM *****
06/27 07:36:25 PM   Epoch = 2 iter 34 step
06/27 07:36:25 PM   Num examples = 16
06/27 07:36:25 PM   Batch size = 32
06/27 07:36:25 PM ***** Eval results *****
06/27 07:36:25 PM   acc = 0.875
06/27 07:36:25 PM   cls_loss = 1.5735534846782684
06/27 07:36:25 PM   eval_loss = 2.0888173580169678
06/27 07:36:25 PM   global_step = 34
06/27 07:36:25 PM   loss = 1.5735534846782684
06/27 07:36:25 PM ***** LOSS printing *****
06/27 07:36:25 PM loss
06/27 07:36:25 PM tensor(1.8707, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:26 PM ***** LOSS printing *****
06/27 07:36:26 PM loss
06/27 07:36:26 PM tensor(1.7585, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:26 PM ***** LOSS printing *****
06/27 07:36:26 PM loss
06/27 07:36:26 PM tensor(1.4042, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:26 PM ***** LOSS printing *****
06/27 07:36:26 PM loss
06/27 07:36:26 PM tensor(0.9819, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:26 PM ***** LOSS printing *****
06/27 07:36:26 PM loss
06/27 07:36:26 PM tensor(1.2249, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:26 PM ***** Running evaluation MLM *****
06/27 07:36:26 PM   Epoch = 3 iter 39 step
06/27 07:36:26 PM   Num examples = 16
06/27 07:36:26 PM   Batch size = 32
06/27 07:36:27 PM ***** Eval results *****
06/27 07:36:27 PM   acc = 0.75
06/27 07:36:27 PM   cls_loss = 1.2036668062210083
06/27 07:36:27 PM   eval_loss = 1.932070016860962
06/27 07:36:27 PM   global_step = 39
06/27 07:36:27 PM   loss = 1.2036668062210083
06/27 07:36:27 PM ***** LOSS printing *****
06/27 07:36:27 PM loss
06/27 07:36:27 PM tensor(3.3339, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:27 PM ***** LOSS printing *****
06/27 07:36:27 PM loss
06/27 07:36:27 PM tensor(2.5141, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:27 PM ***** LOSS printing *****
06/27 07:36:27 PM loss
06/27 07:36:27 PM tensor(1.3465, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:28 PM ***** LOSS printing *****
06/27 07:36:28 PM loss
06/27 07:36:28 PM tensor(4.7488, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:28 PM ***** LOSS printing *****
06/27 07:36:28 PM loss
06/27 07:36:28 PM tensor(3.9819, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:28 PM ***** Running evaluation MLM *****
06/27 07:36:28 PM   Epoch = 3 iter 44 step
06/27 07:36:28 PM   Num examples = 16
06/27 07:36:28 PM   Batch size = 32
06/27 07:36:28 PM ***** Eval results *****
06/27 07:36:28 PM   acc = 0.875
06/27 07:36:28 PM   cls_loss = 2.442033529281616
06/27 07:36:28 PM   eval_loss = 1.85128915309906
06/27 07:36:28 PM   global_step = 44
06/27 07:36:28 PM   loss = 2.442033529281616
06/27 07:36:29 PM ***** LOSS printing *****
06/27 07:36:29 PM loss
06/27 07:36:29 PM tensor(0.9859, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:29 PM ***** LOSS printing *****
06/27 07:36:29 PM loss
06/27 07:36:29 PM tensor(1.8130, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:29 PM ***** LOSS printing *****
06/27 07:36:29 PM loss
06/27 07:36:29 PM tensor(1.9143, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:29 PM ***** LOSS printing *****
06/27 07:36:29 PM loss
06/27 07:36:29 PM tensor(0.8909, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:29 PM ***** LOSS printing *****
06/27 07:36:29 PM loss
06/27 07:36:29 PM tensor(1.2734, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:30 PM ***** Running evaluation MLM *****
06/27 07:36:30 PM   Epoch = 4 iter 49 step
06/27 07:36:30 PM   Num examples = 16
06/27 07:36:30 PM   Batch size = 32
06/27 07:36:30 PM ***** Eval results *****
06/27 07:36:30 PM   acc = 0.875
06/27 07:36:30 PM   cls_loss = 1.2734225988388062
06/27 07:36:30 PM   eval_loss = 1.1350386142730713
06/27 07:36:30 PM   global_step = 49
06/27 07:36:30 PM   loss = 1.2734225988388062
06/27 07:36:30 PM ***** LOSS printing *****
06/27 07:36:30 PM loss
06/27 07:36:30 PM tensor(2.0751, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:30 PM ***** LOSS printing *****
06/27 07:36:30 PM loss
06/27 07:36:30 PM tensor(1.9152, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:31 PM ***** LOSS printing *****
06/27 07:36:31 PM loss
06/27 07:36:31 PM tensor(1.2074, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:31 PM ***** LOSS printing *****
06/27 07:36:31 PM loss
06/27 07:36:31 PM tensor(0.9891, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:31 PM ***** LOSS printing *****
06/27 07:36:31 PM loss
06/27 07:36:31 PM tensor(1.3746, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:31 PM ***** Running evaluation MLM *****
06/27 07:36:31 PM   Epoch = 4 iter 54 step
06/27 07:36:31 PM   Num examples = 16
06/27 07:36:31 PM   Batch size = 32
06/27 07:36:32 PM ***** Eval results *****
06/27 07:36:32 PM   acc = 0.8125
06/27 07:36:32 PM   cls_loss = 1.4724728564421337
06/27 07:36:32 PM   eval_loss = 1.4934736490249634
06/27 07:36:32 PM   global_step = 54
06/27 07:36:32 PM   loss = 1.4724728564421337
06/27 07:36:32 PM ***** LOSS printing *****
06/27 07:36:32 PM loss
06/27 07:36:32 PM tensor(1.2437, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:32 PM ***** LOSS printing *****
06/27 07:36:32 PM loss
06/27 07:36:32 PM tensor(1.3171, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:32 PM ***** LOSS printing *****
06/27 07:36:32 PM loss
06/27 07:36:32 PM tensor(1.5150, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:32 PM ***** LOSS printing *****
06/27 07:36:32 PM loss
06/27 07:36:32 PM tensor(1.5356, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:33 PM ***** LOSS printing *****
06/27 07:36:33 PM loss
06/27 07:36:33 PM tensor(1.3953, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:33 PM ***** Running evaluation MLM *****
06/27 07:36:33 PM   Epoch = 4 iter 59 step
06/27 07:36:33 PM   Num examples = 16
06/27 07:36:33 PM   Batch size = 32
06/27 07:36:33 PM ***** Eval results *****
06/27 07:36:33 PM   acc = 0.875
06/27 07:36:33 PM   cls_loss = 1.4401392123915933
06/27 07:36:33 PM   eval_loss = 1.3785804510116577
06/27 07:36:33 PM   global_step = 59
06/27 07:36:33 PM   loss = 1.4401392123915933
06/27 07:36:33 PM ***** LOSS printing *****
06/27 07:36:33 PM loss
06/27 07:36:33 PM tensor(1.2689, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:34 PM ***** LOSS printing *****
06/27 07:36:34 PM loss
06/27 07:36:34 PM tensor(1.1958, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:34 PM ***** LOSS printing *****
06/27 07:36:34 PM loss
06/27 07:36:34 PM tensor(1.2732, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:34 PM ***** LOSS printing *****
06/27 07:36:34 PM loss
06/27 07:36:34 PM tensor(1.1257, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:34 PM ***** LOSS printing *****
06/27 07:36:34 PM loss
06/27 07:36:34 PM tensor(1.0586, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:34 PM ***** Running evaluation MLM *****
06/27 07:36:34 PM   Epoch = 5 iter 64 step
06/27 07:36:34 PM   Num examples = 16
06/27 07:36:34 PM   Batch size = 32
06/27 07:36:35 PM ***** Eval results *****
06/27 07:36:35 PM   acc = 0.875
06/27 07:36:35 PM   cls_loss = 1.1633111834526062
06/27 07:36:35 PM   eval_loss = 1.6030211448669434
06/27 07:36:35 PM   global_step = 64
06/27 07:36:35 PM   loss = 1.1633111834526062
06/27 07:36:35 PM ***** LOSS printing *****
06/27 07:36:35 PM loss
06/27 07:36:35 PM tensor(1.8273, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:35 PM ***** LOSS printing *****
06/27 07:36:35 PM loss
06/27 07:36:35 PM tensor(1.2367, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:35 PM ***** LOSS printing *****
06/27 07:36:35 PM loss
06/27 07:36:35 PM tensor(1.3760, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:36 PM ***** LOSS printing *****
06/27 07:36:36 PM loss
06/27 07:36:36 PM tensor(1.3454, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:36 PM ***** LOSS printing *****
06/27 07:36:36 PM loss
06/27 07:36:36 PM tensor(1.3965, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:36 PM ***** Running evaluation MLM *****
06/27 07:36:36 PM   Epoch = 5 iter 69 step
06/27 07:36:36 PM   Num examples = 16
06/27 07:36:36 PM   Batch size = 32
06/27 07:36:37 PM ***** Eval results *****
06/27 07:36:37 PM   acc = 0.875
06/27 07:36:37 PM   cls_loss = 1.3150208791097004
06/27 07:36:37 PM   eval_loss = 2.2523555755615234
06/27 07:36:37 PM   global_step = 69
06/27 07:36:37 PM   loss = 1.3150208791097004
06/27 07:36:37 PM ***** LOSS printing *****
06/27 07:36:37 PM loss
06/27 07:36:37 PM tensor(0.9128, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:37 PM ***** LOSS printing *****
06/27 07:36:37 PM loss
06/27 07:36:37 PM tensor(1.2494, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:37 PM ***** LOSS printing *****
06/27 07:36:37 PM loss
06/27 07:36:37 PM tensor(1.4836, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:37 PM ***** LOSS printing *****
06/27 07:36:37 PM loss
06/27 07:36:37 PM tensor(1.3679, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:37 PM ***** LOSS printing *****
06/27 07:36:37 PM loss
06/27 07:36:37 PM tensor(1.4918, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:38 PM ***** Running evaluation MLM *****
06/27 07:36:38 PM   Epoch = 6 iter 74 step
06/27 07:36:38 PM   Num examples = 16
06/27 07:36:38 PM   Batch size = 32
06/27 07:36:38 PM ***** Eval results *****
06/27 07:36:38 PM   acc = 0.875
06/27 07:36:38 PM   cls_loss = 1.429844319820404
06/27 07:36:38 PM   eval_loss = 2.278404474258423
06/27 07:36:38 PM   global_step = 74
06/27 07:36:38 PM   loss = 1.429844319820404
06/27 07:36:38 PM ***** LOSS printing *****
06/27 07:36:38 PM loss
06/27 07:36:38 PM tensor(0.9373, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:38 PM ***** LOSS printing *****
06/27 07:36:38 PM loss
06/27 07:36:38 PM tensor(0.9587, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:39 PM ***** LOSS printing *****
06/27 07:36:39 PM loss
06/27 07:36:39 PM tensor(1.3907, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:39 PM ***** LOSS printing *****
06/27 07:36:39 PM loss
06/27 07:36:39 PM tensor(1.1095, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:39 PM ***** LOSS printing *****
06/27 07:36:39 PM loss
06/27 07:36:39 PM tensor(1.7139, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:39 PM ***** Running evaluation MLM *****
06/27 07:36:39 PM   Epoch = 6 iter 79 step
06/27 07:36:39 PM   Num examples = 16
06/27 07:36:39 PM   Batch size = 32
06/27 07:36:40 PM ***** Eval results *****
06/27 07:36:40 PM   acc = 0.875
06/27 07:36:40 PM   cls_loss = 1.2814090507371085
06/27 07:36:40 PM   eval_loss = 1.7059979438781738
06/27 07:36:40 PM   global_step = 79
06/27 07:36:40 PM   loss = 1.2814090507371085
06/27 07:36:40 PM ***** LOSS printing *****
06/27 07:36:40 PM loss
06/27 07:36:40 PM tensor(1.9982, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:40 PM ***** LOSS printing *****
06/27 07:36:40 PM loss
06/27 07:36:40 PM tensor(1.1448, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:40 PM ***** LOSS printing *****
06/27 07:36:40 PM loss
06/27 07:36:40 PM tensor(1.0868, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:40 PM ***** LOSS printing *****
06/27 07:36:40 PM loss
06/27 07:36:40 PM tensor(1.7274, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:41 PM ***** LOSS printing *****
06/27 07:36:41 PM loss
06/27 07:36:41 PM tensor(1.4561, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:41 PM ***** Running evaluation MLM *****
06/27 07:36:41 PM   Epoch = 6 iter 84 step
06/27 07:36:41 PM   Num examples = 16
06/27 07:36:41 PM   Batch size = 32
06/27 07:36:41 PM ***** Eval results *****
06/27 07:36:41 PM   acc = 0.875
06/27 07:36:41 PM   cls_loss = 1.3652587781349819
06/27 07:36:41 PM   eval_loss = 1.472359299659729
06/27 07:36:41 PM   global_step = 84
06/27 07:36:41 PM   loss = 1.3652587781349819
06/27 07:36:41 PM ***** LOSS printing *****
06/27 07:36:41 PM loss
06/27 07:36:41 PM tensor(1.2527, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:42 PM ***** LOSS printing *****
06/27 07:36:42 PM loss
06/27 07:36:42 PM tensor(1.0152, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:42 PM ***** LOSS printing *****
06/27 07:36:42 PM loss
06/27 07:36:42 PM tensor(1.2201, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:42 PM ***** LOSS printing *****
06/27 07:36:42 PM loss
06/27 07:36:42 PM tensor(1.1234, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:42 PM ***** LOSS printing *****
06/27 07:36:42 PM loss
06/27 07:36:42 PM tensor(1.2692, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:43 PM ***** Running evaluation MLM *****
06/27 07:36:43 PM   Epoch = 7 iter 89 step
06/27 07:36:43 PM   Num examples = 16
06/27 07:36:43 PM   Batch size = 32
06/27 07:36:43 PM ***** Eval results *****
06/27 07:36:43 PM   acc = 0.875
06/27 07:36:43 PM   cls_loss = 1.176120400428772
06/27 07:36:43 PM   eval_loss = 1.8041634559631348
06/27 07:36:43 PM   global_step = 89
06/27 07:36:43 PM   loss = 1.176120400428772
06/27 07:36:43 PM ***** LOSS printing *****
06/27 07:36:43 PM loss
06/27 07:36:43 PM tensor(1.0834, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:43 PM ***** LOSS printing *****
06/27 07:36:43 PM loss
06/27 07:36:43 PM tensor(1.1584, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:44 PM ***** LOSS printing *****
06/27 07:36:44 PM loss
06/27 07:36:44 PM tensor(1.0583, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:44 PM ***** LOSS printing *****
06/27 07:36:44 PM loss
06/27 07:36:44 PM tensor(1.4047, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:44 PM ***** LOSS printing *****
06/27 07:36:44 PM loss
06/27 07:36:44 PM tensor(1.6100, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:44 PM ***** Running evaluation MLM *****
06/27 07:36:44 PM   Epoch = 7 iter 94 step
06/27 07:36:44 PM   Num examples = 16
06/27 07:36:44 PM   Batch size = 32
06/27 07:36:45 PM ***** Eval results *****
06/27 07:36:45 PM   acc = 0.875
06/27 07:36:45 PM   cls_loss = 1.21954448223114
06/27 07:36:45 PM   eval_loss = 2.007577657699585
06/27 07:36:45 PM   global_step = 94
06/27 07:36:45 PM   loss = 1.21954448223114
06/27 07:36:45 PM ***** LOSS printing *****
06/27 07:36:45 PM loss
06/27 07:36:45 PM tensor(1.0912, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:45 PM ***** LOSS printing *****
06/27 07:36:45 PM loss
06/27 07:36:45 PM tensor(1.3180, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:45 PM ***** LOSS printing *****
06/27 07:36:45 PM loss
06/27 07:36:45 PM tensor(1.5687, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:45 PM ***** LOSS printing *****
06/27 07:36:45 PM loss
06/27 07:36:45 PM tensor(1.0124, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:46 PM ***** LOSS printing *****
06/27 07:36:46 PM loss
06/27 07:36:46 PM tensor(1.1462, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:46 PM ***** Running evaluation MLM *****
06/27 07:36:46 PM   Epoch = 8 iter 99 step
06/27 07:36:46 PM   Num examples = 16
06/27 07:36:46 PM   Batch size = 32
06/27 07:36:46 PM ***** Eval results *****
06/27 07:36:46 PM   acc = 0.875
06/27 07:36:46 PM   cls_loss = 1.2424413760503132
06/27 07:36:46 PM   eval_loss = 2.0267069339752197
06/27 07:36:46 PM   global_step = 99
06/27 07:36:46 PM   loss = 1.2424413760503132
06/27 07:36:46 PM ***** LOSS printing *****
06/27 07:36:46 PM loss
06/27 07:36:46 PM tensor(1.7807, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:47 PM ***** LOSS printing *****
06/27 07:36:47 PM loss
06/27 07:36:47 PM tensor(1.0157, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:47 PM ***** LOSS printing *****
06/27 07:36:47 PM loss
06/27 07:36:47 PM tensor(1.0841, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:47 PM ***** LOSS printing *****
06/27 07:36:47 PM loss
06/27 07:36:47 PM tensor(1.2889, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:47 PM ***** LOSS printing *****
06/27 07:36:47 PM loss
06/27 07:36:47 PM tensor(1.1586, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:47 PM ***** Running evaluation MLM *****
06/27 07:36:47 PM   Epoch = 8 iter 104 step
06/27 07:36:47 PM   Num examples = 16
06/27 07:36:47 PM   Batch size = 32
06/27 07:36:48 PM ***** Eval results *****
06/27 07:36:48 PM   acc = 0.875
06/27 07:36:48 PM   cls_loss = 1.2569080889225006
06/27 07:36:48 PM   eval_loss = 2.247807025909424
06/27 07:36:48 PM   global_step = 104
06/27 07:36:48 PM   loss = 1.2569080889225006
06/27 07:36:48 PM ***** LOSS printing *****
06/27 07:36:48 PM loss
06/27 07:36:48 PM tensor(1.7037, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:48 PM ***** LOSS printing *****
06/27 07:36:48 PM loss
06/27 07:36:48 PM tensor(1.5494, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:48 PM ***** LOSS printing *****
06/27 07:36:48 PM loss
06/27 07:36:48 PM tensor(1.1684, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:49 PM ***** LOSS printing *****
06/27 07:36:49 PM loss
06/27 07:36:49 PM tensor(1.1463, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:49 PM ***** LOSS printing *****
06/27 07:36:49 PM loss
06/27 07:36:49 PM tensor(1.1390, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:49 PM ***** Running evaluation MLM *****
06/27 07:36:49 PM   Epoch = 9 iter 109 step
06/27 07:36:49 PM   Num examples = 16
06/27 07:36:49 PM   Batch size = 32
06/27 07:36:49 PM ***** Eval results *****
06/27 07:36:49 PM   acc = 0.875
06/27 07:36:49 PM   cls_loss = 1.1390101909637451
06/27 07:36:49 PM   eval_loss = 1.8947546482086182
06/27 07:36:49 PM   global_step = 109
06/27 07:36:49 PM   loss = 1.1390101909637451
06/27 07:36:50 PM ***** LOSS printing *****
06/27 07:36:50 PM loss
06/27 07:36:50 PM tensor(1.1326, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:50 PM ***** LOSS printing *****
06/27 07:36:50 PM loss
06/27 07:36:50 PM tensor(1.3050, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:50 PM ***** LOSS printing *****
06/27 07:36:50 PM loss
06/27 07:36:50 PM tensor(1.3941, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:50 PM ***** LOSS printing *****
06/27 07:36:50 PM loss
06/27 07:36:50 PM tensor(1.3131, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:50 PM ***** LOSS printing *****
06/27 07:36:50 PM loss
06/27 07:36:50 PM tensor(1.3973, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:51 PM ***** Running evaluation MLM *****
06/27 07:36:51 PM   Epoch = 9 iter 114 step
06/27 07:36:51 PM   Num examples = 16
06/27 07:36:51 PM   Batch size = 32
06/27 07:36:51 PM ***** Eval results *****
06/27 07:36:51 PM   acc = 0.875
06/27 07:36:51 PM   cls_loss = 1.2801767985026042
06/27 07:36:51 PM   eval_loss = 1.7563735246658325
06/27 07:36:51 PM   global_step = 114
06/27 07:36:51 PM   loss = 1.2801767985026042
06/27 07:36:51 PM ***** LOSS printing *****
06/27 07:36:51 PM loss
06/27 07:36:51 PM tensor(1.0366, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:51 PM ***** LOSS printing *****
06/27 07:36:51 PM loss
06/27 07:36:51 PM tensor(1.1688, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:52 PM ***** LOSS printing *****
06/27 07:36:52 PM loss
06/27 07:36:52 PM tensor(1.1398, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:52 PM ***** LOSS printing *****
06/27 07:36:52 PM loss
06/27 07:36:52 PM tensor(1.7595, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:52 PM ***** LOSS printing *****
06/27 07:36:52 PM loss
06/27 07:36:52 PM tensor(1.4162, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:36:52 PM ***** Running evaluation MLM *****
06/27 07:36:52 PM   Epoch = 9 iter 119 step
06/27 07:36:52 PM   Num examples = 16
06/27 07:36:52 PM   Batch size = 32
06/27 07:36:53 PM ***** Eval results *****
06/27 07:36:53 PM   acc = 0.875
06/27 07:36:53 PM   cls_loss = 1.2910814393650403
06/27 07:36:53 PM   eval_loss = 2.2930991649627686
06/27 07:36:53 PM   global_step = 119
06/27 07:36:53 PM   loss = 1.2910814393650403
06/27 07:36:53 PM ***** LOSS printing *****
06/27 07:36:53 PM loss
06/27 07:36:53 PM tensor(1.1876, device='cuda:0', grad_fn=<NllLossBackward0>)

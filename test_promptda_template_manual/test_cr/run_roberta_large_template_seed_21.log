06/27 07:36:54 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-21/', data_seed=21, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 07:36:54 PM device: cuda n_gpu: 1
06/27 07:36:55 PM Writing example 0 of 48
06/27 07:36:55 PM *** Example ***
06/27 07:36:55 PM guid: train-1
06/27 07:36:55 PM tokens: <s> the Ġn okia Ġ66 00 Ġis Ġa Ġdecent Ġextension Ġof Ġthe Ġsmart Ġphone Ġline Ġ. </s> ĠIt Ġis <mask>
06/27 07:36:55 PM input_ids: 0 627 295 43946 5138 612 16 10 7297 5064 9 5 2793 1028 516 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM label: ['Ġgood']
06/27 07:36:55 PM Writing example 0 of 16
06/27 07:36:55 PM *** Example ***
06/27 07:36:55 PM guid: dev-1
06/27 07:36:55 PM tokens: <s> the Ġclarity Ġis Ġunbelievable Ġ, Ġwhich Ġmeans Ġyou Ġcan Ġcram Ġalmost Ġthe Ġfull Ġ2 Ġ, Ġ5 Ġ! Ġ. </s> ĠIt Ġis <mask>
06/27 07:36:55 PM input_ids: 0 627 10498 16 14011 2156 61 839 47 64 39011 818 5 455 132 2156 195 27785 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM label: ['Ġgood']
06/27 07:36:55 PM Writing example 0 of 2000
06/27 07:36:55 PM *** Example ***
06/27 07:36:55 PM guid: dev-1
06/27 07:36:55 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 07:36:55 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:36:55 PM label: ['Ġterrible']
06/27 07:37:08 PM ***** Running training *****
06/27 07:37:08 PM   Num examples = 48
06/27 07:37:08 PM   Batch size = 4
06/27 07:37:08 PM   Num steps = 120
06/27 07:37:08 PM n: embeddings.word_embeddings.weight
06/27 07:37:08 PM n: embeddings.position_embeddings.weight
06/27 07:37:08 PM n: embeddings.token_type_embeddings.weight
06/27 07:37:08 PM n: embeddings.LayerNorm.weight
06/27 07:37:08 PM n: embeddings.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.0.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.0.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.0.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.0.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.0.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.0.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.0.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.0.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.0.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.0.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.0.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.0.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.1.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.1.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.1.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.1.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.1.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.1.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.1.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.1.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.1.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.1.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.1.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.1.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.2.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.2.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.2.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.2.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.2.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.2.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.2.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.2.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.2.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.2.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.2.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.2.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.3.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.3.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.3.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.3.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.3.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.3.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.3.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.3.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.3.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.3.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.3.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.3.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.4.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.4.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.4.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.4.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.4.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.4.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.4.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.4.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.4.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.4.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.4.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.4.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.5.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.5.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.5.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.5.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.5.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.5.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.5.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.5.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.5.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.5.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.5.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.5.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.6.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.6.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.6.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.6.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.6.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.6.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.6.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.6.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.6.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.6.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.6.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.6.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.7.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.7.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.7.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.7.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.7.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.7.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.7.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.7.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.7.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.7.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.7.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.7.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.8.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.8.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.8.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.8.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.8.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.8.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.8.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.8.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.8.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.8.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.8.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.8.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.9.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.9.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.9.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.9.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.9.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.9.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.9.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.9.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.9.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.9.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.9.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.9.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.10.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.10.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.10.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.10.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.10.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.10.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.10.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.10.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.10.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.10.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.10.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.10.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.11.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.11.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.11.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.11.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.11.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.11.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.11.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.11.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.11.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.11.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.11.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.11.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.12.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.12.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.12.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.12.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.12.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.12.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.12.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.12.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.12.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.12.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.12.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.12.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.13.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.13.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.13.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.13.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.13.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.13.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.13.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.13.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.13.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.13.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.13.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.13.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.14.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.14.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.14.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.14.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.14.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.14.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.14.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.14.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.14.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.14.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.14.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.14.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.15.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.15.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.15.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.15.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.15.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.15.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.15.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.15.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.15.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.15.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.15.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.15.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.16.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.16.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.16.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.16.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.16.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.16.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.16.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.16.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.16.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.16.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.16.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.16.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.17.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.17.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.17.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.17.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.17.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.17.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.17.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.17.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.17.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.17.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.17.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.17.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.18.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.18.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.18.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.18.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.18.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.18.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.18.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.18.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.18.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.18.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.18.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.18.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.19.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.19.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.19.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.19.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.19.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.19.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.19.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.19.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.19.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.19.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.19.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.19.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.20.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.20.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.20.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.20.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.20.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.20.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.20.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.20.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.20.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.20.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.20.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.20.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.21.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.21.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.21.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.21.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.21.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.21.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.21.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.21.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.21.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.21.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.21.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.21.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.22.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.22.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.22.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.22.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.22.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.22.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.22.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.22.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.22.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.22.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.22.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.22.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.23.attention.self.query.weight
06/27 07:37:08 PM n: encoder.layer.23.attention.self.query.bias
06/27 07:37:08 PM n: encoder.layer.23.attention.self.key.weight
06/27 07:37:08 PM n: encoder.layer.23.attention.self.key.bias
06/27 07:37:08 PM n: encoder.layer.23.attention.self.value.weight
06/27 07:37:08 PM n: encoder.layer.23.attention.self.value.bias
06/27 07:37:08 PM n: encoder.layer.23.attention.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.23.attention.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: encoder.layer.23.intermediate.dense.weight
06/27 07:37:08 PM n: encoder.layer.23.intermediate.dense.bias
06/27 07:37:08 PM n: encoder.layer.23.output.dense.weight
06/27 07:37:08 PM n: encoder.layer.23.output.dense.bias
06/27 07:37:08 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 07:37:08 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 07:37:08 PM n: pooler.dense.weight
06/27 07:37:08 PM n: pooler.dense.bias
06/27 07:37:08 PM n: roberta.embeddings.word_embeddings.weight
06/27 07:37:08 PM n: roberta.embeddings.position_embeddings.weight
06/27 07:37:08 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 07:37:08 PM n: roberta.embeddings.LayerNorm.weight
06/27 07:37:08 PM n: roberta.embeddings.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 07:37:08 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 07:37:08 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 07:37:08 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 07:37:08 PM n: roberta.pooler.dense.weight
06/27 07:37:08 PM n: roberta.pooler.dense.bias
06/27 07:37:08 PM n: lm_head.bias
06/27 07:37:08 PM n: lm_head.dense.weight
06/27 07:37:08 PM n: lm_head.dense.bias
06/27 07:37:08 PM n: lm_head.layer_norm.weight
06/27 07:37:08 PM n: lm_head.layer_norm.bias
06/27 07:37:08 PM n: lm_head.decoder.weight
06/27 07:37:08 PM Total parameters: 763292761
06/27 07:37:08 PM ***** LOSS printing *****
06/27 07:37:08 PM loss
06/27 07:37:08 PM tensor(19.7368, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:37:08 PM ***** LOSS printing *****
06/27 07:37:08 PM loss
06/27 07:37:08 PM tensor(15.1081, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:37:08 PM ***** LOSS printing *****
06/27 07:37:08 PM loss
06/27 07:37:08 PM tensor(10.5912, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:37:09 PM ***** LOSS printing *****
06/27 07:37:09 PM loss
06/27 07:37:09 PM tensor(5.0880, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:37:09 PM ***** Running evaluation MLM *****
06/27 07:37:09 PM   Epoch = 0 iter 4 step
06/27 07:37:09 PM   Num examples = 16
06/27 07:37:09 PM   Batch size = 32
06/27 07:37:09 PM ***** Eval results *****
06/27 07:37:09 PM   acc = 0.75
06/27 07:37:09 PM   cls_loss = 12.63103723526001
06/27 07:37:09 PM   eval_loss = 3.1992313861846924
06/27 07:37:09 PM   global_step = 4
06/27 07:37:09 PM   loss = 12.63103723526001
06/27 07:37:09 PM ***** Save model *****
06/27 07:37:09 PM ***** Test Dataset Eval Result *****
06/27 07:38:12 PM ***** Eval results *****
06/27 07:38:12 PM   acc = 0.7145
06/27 07:38:12 PM   cls_loss = 12.63103723526001
06/27 07:38:12 PM   eval_loss = 3.0598851272038052
06/27 07:38:12 PM   global_step = 4
06/27 07:38:12 PM   loss = 12.63103723526001
06/27 07:38:17 PM ***** LOSS printing *****
06/27 07:38:17 PM loss
06/27 07:38:17 PM tensor(4.4012, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:17 PM ***** LOSS printing *****
06/27 07:38:17 PM loss
06/27 07:38:17 PM tensor(3.3507, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:17 PM ***** LOSS printing *****
06/27 07:38:17 PM loss
06/27 07:38:17 PM tensor(2.9825, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:17 PM ***** LOSS printing *****
06/27 07:38:17 PM loss
06/27 07:38:17 PM tensor(3.4628, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:18 PM ***** LOSS printing *****
06/27 07:38:18 PM loss
06/27 07:38:18 PM tensor(1.9118, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:18 PM ***** Running evaluation MLM *****
06/27 07:38:18 PM   Epoch = 0 iter 9 step
06/27 07:38:18 PM   Num examples = 16
06/27 07:38:18 PM   Batch size = 32
06/27 07:38:18 PM ***** Eval results *****
06/27 07:38:18 PM   acc = 0.75
06/27 07:38:18 PM   cls_loss = 7.403688748677571
06/27 07:38:18 PM   eval_loss = 1.633124589920044
06/27 07:38:18 PM   global_step = 9
06/27 07:38:18 PM   loss = 7.403688748677571
06/27 07:38:18 PM ***** LOSS printing *****
06/27 07:38:18 PM loss
06/27 07:38:18 PM tensor(1.8446, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:18 PM ***** LOSS printing *****
06/27 07:38:18 PM loss
06/27 07:38:18 PM tensor(2.8576, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:19 PM ***** LOSS printing *****
06/27 07:38:19 PM loss
06/27 07:38:19 PM tensor(5.8992, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:19 PM ***** LOSS printing *****
06/27 07:38:19 PM loss
06/27 07:38:19 PM tensor(3.2471, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:19 PM ***** LOSS printing *****
06/27 07:38:19 PM loss
06/27 07:38:19 PM tensor(2.5602, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:19 PM ***** Running evaluation MLM *****
06/27 07:38:19 PM   Epoch = 1 iter 14 step
06/27 07:38:19 PM   Num examples = 16
06/27 07:38:19 PM   Batch size = 32
06/27 07:38:20 PM ***** Eval results *****
06/27 07:38:20 PM   acc = 0.75
06/27 07:38:20 PM   cls_loss = 2.9036518335342407
06/27 07:38:20 PM   eval_loss = 1.939928412437439
06/27 07:38:20 PM   global_step = 14
06/27 07:38:20 PM   loss = 2.9036518335342407
06/27 07:38:20 PM ***** LOSS printing *****
06/27 07:38:20 PM loss
06/27 07:38:20 PM tensor(1.3503, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:20 PM ***** LOSS printing *****
06/27 07:38:20 PM loss
06/27 07:38:20 PM tensor(1.7529, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:20 PM ***** LOSS printing *****
06/27 07:38:20 PM loss
06/27 07:38:20 PM tensor(1.7040, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:21 PM ***** LOSS printing *****
06/27 07:38:21 PM loss
06/27 07:38:21 PM tensor(1.5010, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:21 PM ***** LOSS printing *****
06/27 07:38:21 PM loss
06/27 07:38:21 PM tensor(1.1915, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:21 PM ***** Running evaluation MLM *****
06/27 07:38:21 PM   Epoch = 1 iter 19 step
06/27 07:38:21 PM   Num examples = 16
06/27 07:38:21 PM   Batch size = 32
06/27 07:38:21 PM ***** Eval results *****
06/27 07:38:21 PM   acc = 0.75
06/27 07:38:21 PM   cls_loss = 1.9009758063725062
06/27 07:38:21 PM   eval_loss = 2.141450881958008
06/27 07:38:21 PM   global_step = 19
06/27 07:38:21 PM   loss = 1.9009758063725062
06/27 07:38:21 PM ***** LOSS printing *****
06/27 07:38:21 PM loss
06/27 07:38:21 PM tensor(1.9941, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:22 PM ***** LOSS printing *****
06/27 07:38:22 PM loss
06/27 07:38:22 PM tensor(1.3637, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:22 PM ***** LOSS printing *****
06/27 07:38:22 PM loss
06/27 07:38:22 PM tensor(2.2371, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:22 PM ***** LOSS printing *****
06/27 07:38:22 PM loss
06/27 07:38:22 PM tensor(1.1316, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:22 PM ***** LOSS printing *****
06/27 07:38:22 PM loss
06/27 07:38:22 PM tensor(2.0920, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:38:23 PM ***** Running evaluation MLM *****
06/27 07:38:23 PM   Epoch = 1 iter 24 step
06/27 07:38:23 PM   Num examples = 16
06/27 07:38:23 PM   Batch size = 32
06/27 07:38:23 PM ***** Eval results *****
06/27 07:38:23 PM   acc = 0.875
06/27 07:38:23 PM   cls_loss = 1.8437765836715698
06/27 07:38:23 PM   eval_loss = 1.9292309284210205
06/27 07:38:23 PM   global_step = 24
06/27 07:38:23 PM   loss = 1.8437765836715698
06/27 07:38:23 PM ***** Save model *****
06/27 07:38:23 PM ***** Test Dataset Eval Result *****
06/27 07:39:26 PM ***** Eval results *****
06/27 07:39:26 PM   acc = 0.902
06/27 07:39:26 PM   cls_loss = 1.8437765836715698
06/27 07:39:26 PM   eval_loss = 1.8132431488188485
06/27 07:39:26 PM   global_step = 24
06/27 07:39:26 PM   loss = 1.8437765836715698
06/27 07:39:30 PM ***** LOSS printing *****
06/27 07:39:30 PM loss
06/27 07:39:30 PM tensor(1.9542, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:39:30 PM ***** LOSS printing *****
06/27 07:39:30 PM loss
06/27 07:39:30 PM tensor(1.3707, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:39:30 PM ***** LOSS printing *****
06/27 07:39:30 PM loss
06/27 07:39:30 PM tensor(1.3622, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:39:31 PM ***** LOSS printing *****
06/27 07:39:31 PM loss
06/27 07:39:31 PM tensor(2.6789, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:39:31 PM ***** LOSS printing *****
06/27 07:39:31 PM loss
06/27 07:39:31 PM tensor(1.2574, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:39:31 PM ***** Running evaluation MLM *****
06/27 07:39:31 PM   Epoch = 2 iter 29 step
06/27 07:39:31 PM   Num examples = 16
06/27 07:39:31 PM   Batch size = 32
06/27 07:39:32 PM ***** Eval results *****
06/27 07:39:32 PM   acc = 0.9375
06/27 07:39:32 PM   cls_loss = 1.7246836423873901
06/27 07:39:32 PM   eval_loss = 2.4146814346313477
06/27 07:39:32 PM   global_step = 29
06/27 07:39:32 PM   loss = 1.7246836423873901
06/27 07:39:32 PM ***** Save model *****
06/27 07:39:32 PM ***** Test Dataset Eval Result *****
06/27 07:40:34 PM ***** Eval results *****
06/27 07:40:34 PM   acc = 0.8715
06/27 07:40:34 PM   cls_loss = 1.7246836423873901
06/27 07:40:34 PM   eval_loss = 2.4020939951851252
06/27 07:40:34 PM   global_step = 29
06/27 07:40:34 PM   loss = 1.7246836423873901
06/27 07:40:38 PM ***** LOSS printing *****
06/27 07:40:38 PM loss
06/27 07:40:38 PM tensor(1.0967, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:39 PM ***** LOSS printing *****
06/27 07:40:39 PM loss
06/27 07:40:39 PM tensor(1.1589, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:39 PM ***** LOSS printing *****
06/27 07:40:39 PM loss
06/27 07:40:39 PM tensor(2.3583, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:39 PM ***** LOSS printing *****
06/27 07:40:39 PM loss
06/27 07:40:39 PM tensor(1.0525, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:39 PM ***** LOSS printing *****
06/27 07:40:39 PM loss
06/27 07:40:39 PM tensor(1.7615, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:39 PM ***** Running evaluation MLM *****
06/27 07:40:39 PM   Epoch = 2 iter 34 step
06/27 07:40:39 PM   Num examples = 16
06/27 07:40:39 PM   Batch size = 32
06/27 07:40:40 PM ***** Eval results *****
06/27 07:40:40 PM   acc = 0.8125
06/27 07:40:40 PM   cls_loss = 1.605127465724945
06/27 07:40:40 PM   eval_loss = 1.6071598529815674
06/27 07:40:40 PM   global_step = 34
06/27 07:40:40 PM   loss = 1.605127465724945
06/27 07:40:40 PM ***** LOSS printing *****
06/27 07:40:40 PM loss
06/27 07:40:40 PM tensor(2.5580, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:40 PM ***** LOSS printing *****
06/27 07:40:40 PM loss
06/27 07:40:40 PM tensor(1.7051, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:40 PM ***** LOSS printing *****
06/27 07:40:40 PM loss
06/27 07:40:40 PM tensor(1.1289, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:41 PM ***** LOSS printing *****
06/27 07:40:41 PM loss
06/27 07:40:41 PM tensor(1.4005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:41 PM ***** LOSS printing *****
06/27 07:40:41 PM loss
06/27 07:40:41 PM tensor(1.3830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:41 PM ***** Running evaluation MLM *****
06/27 07:40:41 PM   Epoch = 3 iter 39 step
06/27 07:40:41 PM   Num examples = 16
06/27 07:40:41 PM   Batch size = 32
06/27 07:40:42 PM ***** Eval results *****
06/27 07:40:42 PM   acc = 0.8125
06/27 07:40:42 PM   cls_loss = 1.3041319052378337
06/27 07:40:42 PM   eval_loss = 1.9038872718811035
06/27 07:40:42 PM   global_step = 39
06/27 07:40:42 PM   loss = 1.3041319052378337
06/27 07:40:42 PM ***** LOSS printing *****
06/27 07:40:42 PM loss
06/27 07:40:42 PM tensor(1.4923, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:42 PM ***** LOSS printing *****
06/27 07:40:42 PM loss
06/27 07:40:42 PM tensor(1.4160, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:42 PM ***** LOSS printing *****
06/27 07:40:42 PM loss
06/27 07:40:42 PM tensor(1.2159, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:42 PM ***** LOSS printing *****
06/27 07:40:42 PM loss
06/27 07:40:42 PM tensor(2.3960, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:42 PM ***** LOSS printing *****
06/27 07:40:42 PM loss
06/27 07:40:42 PM tensor(1.3455, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:43 PM ***** Running evaluation MLM *****
06/27 07:40:43 PM   Epoch = 3 iter 44 step
06/27 07:40:43 PM   Num examples = 16
06/27 07:40:43 PM   Batch size = 32
06/27 07:40:43 PM ***** Eval results *****
06/27 07:40:43 PM   acc = 0.75
06/27 07:40:43 PM   cls_loss = 1.4722705781459808
06/27 07:40:43 PM   eval_loss = 2.366412401199341
06/27 07:40:43 PM   global_step = 44
06/27 07:40:43 PM   loss = 1.4722705781459808
06/27 07:40:43 PM ***** LOSS printing *****
06/27 07:40:43 PM loss
06/27 07:40:43 PM tensor(1.2576, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:43 PM ***** LOSS printing *****
06/27 07:40:43 PM loss
06/27 07:40:43 PM tensor(1.9470, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:44 PM ***** LOSS printing *****
06/27 07:40:44 PM loss
06/27 07:40:44 PM tensor(1.9593, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:44 PM ***** LOSS printing *****
06/27 07:40:44 PM loss
06/27 07:40:44 PM tensor(1.2530, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:44 PM ***** LOSS printing *****
06/27 07:40:44 PM loss
06/27 07:40:44 PM tensor(1.0788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:44 PM ***** Running evaluation MLM *****
06/27 07:40:44 PM   Epoch = 4 iter 49 step
06/27 07:40:44 PM   Num examples = 16
06/27 07:40:44 PM   Batch size = 32
06/27 07:40:45 PM ***** Eval results *****
06/27 07:40:45 PM   acc = 0.8125
06/27 07:40:45 PM   cls_loss = 1.078832745552063
06/27 07:40:45 PM   eval_loss = 1.9498341083526611
06/27 07:40:45 PM   global_step = 49
06/27 07:40:45 PM   loss = 1.078832745552063
06/27 07:40:45 PM ***** LOSS printing *****
06/27 07:40:45 PM loss
06/27 07:40:45 PM tensor(1.5631, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:45 PM ***** LOSS printing *****
06/27 07:40:45 PM loss
06/27 07:40:45 PM tensor(1.4885, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:45 PM ***** LOSS printing *****
06/27 07:40:45 PM loss
06/27 07:40:45 PM tensor(1.6447, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:45 PM ***** LOSS printing *****
06/27 07:40:45 PM loss
06/27 07:40:45 PM tensor(1.7046, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:46 PM ***** LOSS printing *****
06/27 07:40:46 PM loss
06/27 07:40:46 PM tensor(1.5028, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:46 PM ***** Running evaluation MLM *****
06/27 07:40:46 PM   Epoch = 4 iter 54 step
06/27 07:40:46 PM   Num examples = 16
06/27 07:40:46 PM   Batch size = 32
06/27 07:40:46 PM ***** Eval results *****
06/27 07:40:46 PM   acc = 0.9375
06/27 07:40:46 PM   cls_loss = 1.4970657030741374
06/27 07:40:46 PM   eval_loss = 1.4462987184524536
06/27 07:40:46 PM   global_step = 54
06/27 07:40:46 PM   loss = 1.4970657030741374
06/27 07:40:46 PM ***** LOSS printing *****
06/27 07:40:46 PM loss
06/27 07:40:46 PM tensor(1.1030, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:47 PM ***** LOSS printing *****
06/27 07:40:47 PM loss
06/27 07:40:47 PM tensor(1.6313, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:47 PM ***** LOSS printing *****
06/27 07:40:47 PM loss
06/27 07:40:47 PM tensor(1.8471, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:47 PM ***** LOSS printing *****
06/27 07:40:47 PM loss
06/27 07:40:47 PM tensor(1.3768, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:47 PM ***** LOSS printing *****
06/27 07:40:47 PM loss
06/27 07:40:47 PM tensor(1.5252, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:47 PM ***** Running evaluation MLM *****
06/27 07:40:47 PM   Epoch = 4 iter 59 step
06/27 07:40:47 PM   Num examples = 16
06/27 07:40:47 PM   Batch size = 32
06/27 07:40:48 PM ***** Eval results *****
06/27 07:40:48 PM   acc = 0.9375
06/27 07:40:48 PM   cls_loss = 1.4968945546583696
06/27 07:40:48 PM   eval_loss = 1.6806449890136719
06/27 07:40:48 PM   global_step = 59
06/27 07:40:48 PM   loss = 1.4968945546583696
06/27 07:40:48 PM ***** LOSS printing *****
06/27 07:40:48 PM loss
06/27 07:40:48 PM tensor(1.8736, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:48 PM ***** LOSS printing *****
06/27 07:40:48 PM loss
06/27 07:40:48 PM tensor(0.8872, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:48 PM ***** LOSS printing *****
06/27 07:40:48 PM loss
06/27 07:40:48 PM tensor(1.3217, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:49 PM ***** LOSS printing *****
06/27 07:40:49 PM loss
06/27 07:40:49 PM tensor(1.3788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:49 PM ***** LOSS printing *****
06/27 07:40:49 PM loss
06/27 07:40:49 PM tensor(1.1972, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:49 PM ***** Running evaluation MLM *****
06/27 07:40:49 PM   Epoch = 5 iter 64 step
06/27 07:40:49 PM   Num examples = 16
06/27 07:40:49 PM   Batch size = 32
06/27 07:40:50 PM ***** Eval results *****
06/27 07:40:50 PM   acc = 0.8125
06/27 07:40:50 PM   cls_loss = 1.196232259273529
06/27 07:40:50 PM   eval_loss = 1.8617873191833496
06/27 07:40:50 PM   global_step = 64
06/27 07:40:50 PM   loss = 1.196232259273529
06/27 07:40:50 PM ***** LOSS printing *****
06/27 07:40:50 PM loss
06/27 07:40:50 PM tensor(1.1031, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:50 PM ***** LOSS printing *****
06/27 07:40:50 PM loss
06/27 07:40:50 PM tensor(1.3753, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:50 PM ***** LOSS printing *****
06/27 07:40:50 PM loss
06/27 07:40:50 PM tensor(0.9788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:50 PM ***** LOSS printing *****
06/27 07:40:50 PM loss
06/27 07:40:50 PM tensor(1.1052, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:50 PM ***** LOSS printing *****
06/27 07:40:50 PM loss
06/27 07:40:50 PM tensor(1.1264, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:51 PM ***** Running evaluation MLM *****
06/27 07:40:51 PM   Epoch = 5 iter 69 step
06/27 07:40:51 PM   Num examples = 16
06/27 07:40:51 PM   Batch size = 32
06/27 07:40:51 PM ***** Eval results *****
06/27 07:40:51 PM   acc = 0.8125
06/27 07:40:51 PM   cls_loss = 1.1637517081366644
06/27 07:40:51 PM   eval_loss = 2.6670002937316895
06/27 07:40:51 PM   global_step = 69
06/27 07:40:51 PM   loss = 1.1637517081366644
06/27 07:40:51 PM ***** LOSS printing *****
06/27 07:40:51 PM loss
06/27 07:40:51 PM tensor(1.7690, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:51 PM ***** LOSS printing *****
06/27 07:40:51 PM loss
06/27 07:40:51 PM tensor(1.6200, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:52 PM ***** LOSS printing *****
06/27 07:40:52 PM loss
06/27 07:40:52 PM tensor(1.2067, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:52 PM ***** LOSS printing *****
06/27 07:40:52 PM loss
06/27 07:40:52 PM tensor(1.6684, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:52 PM ***** LOSS printing *****
06/27 07:40:52 PM loss
06/27 07:40:52 PM tensor(0.8579, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:52 PM ***** Running evaluation MLM *****
06/27 07:40:52 PM   Epoch = 6 iter 74 step
06/27 07:40:52 PM   Num examples = 16
06/27 07:40:52 PM   Batch size = 32
06/27 07:40:53 PM ***** Eval results *****
06/27 07:40:53 PM   acc = 0.8125
06/27 07:40:53 PM   cls_loss = 1.263156533241272
06/27 07:40:53 PM   eval_loss = 2.59773850440979
06/27 07:40:53 PM   global_step = 74
06/27 07:40:53 PM   loss = 1.263156533241272
06/27 07:40:53 PM ***** LOSS printing *****
06/27 07:40:53 PM loss
06/27 07:40:53 PM tensor(1.2619, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:53 PM ***** LOSS printing *****
06/27 07:40:53 PM loss
06/27 07:40:53 PM tensor(1.3669, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:53 PM ***** LOSS printing *****
06/27 07:40:53 PM loss
06/27 07:40:53 PM tensor(2.0029, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:53 PM ***** LOSS printing *****
06/27 07:40:53 PM loss
06/27 07:40:53 PM tensor(1.7159, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:54 PM ***** LOSS printing *****
06/27 07:40:54 PM loss
06/27 07:40:54 PM tensor(1.3363, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:54 PM ***** Running evaluation MLM *****
06/27 07:40:54 PM   Epoch = 6 iter 79 step
06/27 07:40:54 PM   Num examples = 16
06/27 07:40:54 PM   Batch size = 32
06/27 07:40:54 PM ***** Eval results *****
06/27 07:40:54 PM   acc = 0.8125
06/27 07:40:54 PM   cls_loss = 1.4585922615868705
06/27 07:40:54 PM   eval_loss = 2.345327377319336
06/27 07:40:54 PM   global_step = 79
06/27 07:40:54 PM   loss = 1.4585922615868705
06/27 07:40:54 PM ***** LOSS printing *****
06/27 07:40:54 PM loss
06/27 07:40:54 PM tensor(1.2966, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:55 PM ***** LOSS printing *****
06/27 07:40:55 PM loss
06/27 07:40:55 PM tensor(1.1767, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:55 PM ***** LOSS printing *****
06/27 07:40:55 PM loss
06/27 07:40:55 PM tensor(1.4059, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:55 PM ***** LOSS printing *****
06/27 07:40:55 PM loss
06/27 07:40:55 PM tensor(1.3675, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:55 PM ***** LOSS printing *****
06/27 07:40:55 PM loss
06/27 07:40:55 PM tensor(1.2911, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:55 PM ***** Running evaluation MLM *****
06/27 07:40:55 PM   Epoch = 6 iter 84 step
06/27 07:40:55 PM   Num examples = 16
06/27 07:40:55 PM   Batch size = 32
06/27 07:40:56 PM ***** Eval results *****
06/27 07:40:56 PM   acc = 0.8125
06/27 07:40:56 PM   cls_loss = 1.3956623077392578
06/27 07:40:56 PM   eval_loss = 2.0910301208496094
06/27 07:40:56 PM   global_step = 84
06/27 07:40:56 PM   loss = 1.3956623077392578
06/27 07:40:56 PM ***** LOSS printing *****
06/27 07:40:56 PM loss
06/27 07:40:56 PM tensor(1.2825, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:56 PM ***** LOSS printing *****
06/27 07:40:56 PM loss
06/27 07:40:56 PM tensor(1.0750, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:56 PM ***** LOSS printing *****
06/27 07:40:56 PM loss
06/27 07:40:56 PM tensor(0.9794, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:57 PM ***** LOSS printing *****
06/27 07:40:57 PM loss
06/27 07:40:57 PM tensor(1.8482, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:57 PM ***** LOSS printing *****
06/27 07:40:57 PM loss
06/27 07:40:57 PM tensor(1.4283, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:57 PM ***** Running evaluation MLM *****
06/27 07:40:57 PM   Epoch = 7 iter 89 step
06/27 07:40:57 PM   Num examples = 16
06/27 07:40:57 PM   Batch size = 32
06/27 07:40:58 PM ***** Eval results *****
06/27 07:40:58 PM   acc = 0.8125
06/27 07:40:58 PM   cls_loss = 1.3226690292358398
06/27 07:40:58 PM   eval_loss = 1.5625663995742798
06/27 07:40:58 PM   global_step = 89
06/27 07:40:58 PM   loss = 1.3226690292358398
06/27 07:40:58 PM ***** LOSS printing *****
06/27 07:40:58 PM loss
06/27 07:40:58 PM tensor(1.3687, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:58 PM ***** LOSS printing *****
06/27 07:40:58 PM loss
06/27 07:40:58 PM tensor(1.4260, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:58 PM ***** LOSS printing *****
06/27 07:40:58 PM loss
06/27 07:40:58 PM tensor(0.7559, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:58 PM ***** LOSS printing *****
06/27 07:40:58 PM loss
06/27 07:40:58 PM tensor(1.2023, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:59 PM ***** LOSS printing *****
06/27 07:40:59 PM loss
06/27 07:40:59 PM tensor(1.2754, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:59 PM ***** Running evaluation MLM *****
06/27 07:40:59 PM   Epoch = 7 iter 94 step
06/27 07:40:59 PM   Num examples = 16
06/27 07:40:59 PM   Batch size = 32
06/27 07:40:59 PM ***** Eval results *****
06/27 07:40:59 PM   acc = 0.8125
06/27 07:40:59 PM   cls_loss = 1.2641667902469635
06/27 07:40:59 PM   eval_loss = 1.595454454421997
06/27 07:40:59 PM   global_step = 94
06/27 07:40:59 PM   loss = 1.2641667902469635
06/27 07:40:59 PM ***** LOSS printing *****
06/27 07:40:59 PM loss
06/27 07:40:59 PM tensor(1.9747, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:40:59 PM ***** LOSS printing *****
06/27 07:40:59 PM loss
06/27 07:40:59 PM tensor(1.6739, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:00 PM ***** LOSS printing *****
06/27 07:41:00 PM loss
06/27 07:41:00 PM tensor(1.3666, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:00 PM ***** LOSS printing *****
06/27 07:41:00 PM loss
06/27 07:41:00 PM tensor(1.1729, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:00 PM ***** LOSS printing *****
06/27 07:41:00 PM loss
06/27 07:41:00 PM tensor(0.9270, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:00 PM ***** Running evaluation MLM *****
06/27 07:41:00 PM   Epoch = 8 iter 99 step
06/27 07:41:00 PM   Num examples = 16
06/27 07:41:00 PM   Batch size = 32
06/27 07:41:01 PM ***** Eval results *****
06/27 07:41:01 PM   acc = 0.8125
06/27 07:41:01 PM   cls_loss = 1.1554837226867676
06/27 07:41:01 PM   eval_loss = 2.3337435722351074
06/27 07:41:01 PM   global_step = 99
06/27 07:41:01 PM   loss = 1.1554837226867676
06/27 07:41:01 PM ***** LOSS printing *****
06/27 07:41:01 PM loss
06/27 07:41:01 PM tensor(1.5909, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:01 PM ***** LOSS printing *****
06/27 07:41:01 PM loss
06/27 07:41:01 PM tensor(1.0595, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:01 PM ***** LOSS printing *****
06/27 07:41:01 PM loss
06/27 07:41:01 PM tensor(2.1939, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:02 PM ***** LOSS printing *****
06/27 07:41:02 PM loss
06/27 07:41:02 PM tensor(1.0528, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:02 PM ***** LOSS printing *****
06/27 07:41:02 PM loss
06/27 07:41:02 PM tensor(1.5686, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:02 PM ***** Running evaluation MLM *****
06/27 07:41:02 PM   Epoch = 8 iter 104 step
06/27 07:41:02 PM   Num examples = 16
06/27 07:41:02 PM   Batch size = 32
06/27 07:41:02 PM ***** Eval results *****
06/27 07:41:02 PM   acc = 0.8125
06/27 07:41:02 PM   cls_loss = 1.3665219992399216
06/27 07:41:02 PM   eval_loss = 2.5468313694000244
06/27 07:41:02 PM   global_step = 104
06/27 07:41:02 PM   loss = 1.3665219992399216
06/27 07:41:02 PM ***** LOSS printing *****
06/27 07:41:02 PM loss
06/27 07:41:02 PM tensor(0.9001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:03 PM ***** LOSS printing *****
06/27 07:41:03 PM loss
06/27 07:41:03 PM tensor(1.8188, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:03 PM ***** LOSS printing *****
06/27 07:41:03 PM loss
06/27 07:41:03 PM tensor(1.4660, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:03 PM ***** LOSS printing *****
06/27 07:41:03 PM loss
06/27 07:41:03 PM tensor(1.1939, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:03 PM ***** LOSS printing *****
06/27 07:41:03 PM loss
06/27 07:41:03 PM tensor(1.0273, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:04 PM ***** Running evaluation MLM *****
06/27 07:41:04 PM   Epoch = 9 iter 109 step
06/27 07:41:04 PM   Num examples = 16
06/27 07:41:04 PM   Batch size = 32
06/27 07:41:04 PM ***** Eval results *****
06/27 07:41:04 PM   acc = 0.75
06/27 07:41:04 PM   cls_loss = 1.0273168087005615
06/27 07:41:04 PM   eval_loss = 2.323789596557617
06/27 07:41:04 PM   global_step = 109
06/27 07:41:04 PM   loss = 1.0273168087005615
06/27 07:41:04 PM ***** LOSS printing *****
06/27 07:41:04 PM loss
06/27 07:41:04 PM tensor(1.0671, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:04 PM ***** LOSS printing *****
06/27 07:41:04 PM loss
06/27 07:41:04 PM tensor(1.5874, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:05 PM ***** LOSS printing *****
06/27 07:41:05 PM loss
06/27 07:41:05 PM tensor(1.1788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:05 PM ***** LOSS printing *****
06/27 07:41:05 PM loss
06/27 07:41:05 PM tensor(1.1925, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:05 PM ***** LOSS printing *****
06/27 07:41:05 PM loss
06/27 07:41:05 PM tensor(1.0936, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:05 PM ***** Running evaluation MLM *****
06/27 07:41:05 PM   Epoch = 9 iter 114 step
06/27 07:41:05 PM   Num examples = 16
06/27 07:41:05 PM   Batch size = 32
06/27 07:41:06 PM ***** Eval results *****
06/27 07:41:06 PM   acc = 0.8125
06/27 07:41:06 PM   cls_loss = 1.1911214192708333
06/27 07:41:06 PM   eval_loss = 1.7959351539611816
06/27 07:41:06 PM   global_step = 114
06/27 07:41:06 PM   loss = 1.1911214192708333
06/27 07:41:06 PM ***** LOSS printing *****
06/27 07:41:06 PM loss
06/27 07:41:06 PM tensor(1.1443, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:06 PM ***** LOSS printing *****
06/27 07:41:06 PM loss
06/27 07:41:06 PM tensor(1.0014, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:06 PM ***** LOSS printing *****
06/27 07:41:06 PM loss
06/27 07:41:06 PM tensor(1.1669, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:06 PM ***** LOSS printing *****
06/27 07:41:06 PM loss
06/27 07:41:06 PM tensor(1.0753, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:07 PM ***** LOSS printing *****
06/27 07:41:07 PM loss
06/27 07:41:07 PM tensor(1.7222, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:41:07 PM ***** Running evaluation MLM *****
06/27 07:41:07 PM   Epoch = 9 iter 119 step
06/27 07:41:07 PM   Num examples = 16
06/27 07:41:07 PM   Batch size = 32
06/27 07:41:07 PM ***** Eval results *****
06/27 07:41:07 PM   acc = 0.8125
06/27 07:41:07 PM   cls_loss = 1.205169677734375
06/27 07:41:07 PM   eval_loss = 1.6703460216522217
06/27 07:41:07 PM   global_step = 119
06/27 07:41:07 PM   loss = 1.205169677734375
06/27 07:41:07 PM ***** LOSS printing *****
06/27 07:41:07 PM loss
06/27 07:41:07 PM tensor(1.4881, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 07:24:12 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/cr/8-100/', data_seed=100, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='cr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 07:24:12 PM device: cuda n_gpu: 1
06/27 07:24:12 PM Writing example 0 of 48
06/27 07:24:12 PM *** Example ***
06/27 07:24:12 PM guid: train-1
06/27 07:24:12 PM tokens: <s> love Ġthe Ġspeaker Ġphone Ġ. </s> ĠIt Ġis <mask>
06/27 07:24:12 PM input_ids: 0 17693 5 5385 1028 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM label: ['Ġgood']
06/27 07:24:12 PM Writing example 0 of 16
06/27 07:24:12 PM *** Example ***
06/27 07:24:12 PM guid: dev-1
06/27 07:24:12 PM tokens: <s> i Ġhave Ġhad Ġthe Ġip od Ġmini Ġand Ġit Ġperforms Ġon Ġthe Ġlevel Ġwith Ġmy Ġsound Ġcard Ġbut Ġthe Ġmicro Ġbeats Ġit Ġoutright Ġ. </s> ĠIt Ġis <mask>
06/27 07:24:12 PM input_ids: 0 118 33 56 5 36180 1630 7983 8 24 14023 15 5 672 19 127 2369 1886 53 5 5177 13410 24 12162 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM label: ['Ġgood']
06/27 07:24:12 PM Writing example 0 of 2000
06/27 07:24:12 PM *** Example ***
06/27 07:24:12 PM guid: dev-1
06/27 07:24:12 PM tokens: <s> weak nesses Ġare Ġminor Ġ: Ġthe Ġfeel Ġand Ġlayout Ġof Ġthe Ġremote Ġcontrol Ġare Ġonly Ġso - so Ġ; Ġ. Ġit Ġdoes Ġn Ġ' t Ġshow Ġthe Ġcomplete Ġfile Ġnames Ġof Ġmp 3 s Ġwith Ġreally Ġlong Ġnames Ġ; Ġ. Ġyou Ġmust Ġcycle Ġthrough Ġevery Ġzoom Ġsetting Ġ( Ġ2 x Ġ, Ġ3 x Ġ, Ġ4 x Ġ, Ġ1 / 2 x Ġ, Ġetc Ġ. Ġ) Ġbefore Ġgetting Ġback Ġto Ġnormal Ġsize Ġ[ Ġsorry Ġif Ġi Ġ' m Ġjust Ġignorant Ġof Ġa Ġway Ġto Ġget Ġback Ġto Ġ1 x Ġquickly Ġ] Ġ. </s> ĠIt Ġis <mask>
06/27 07:24:12 PM input_ids: 0 25785 43010 32 3694 4832 5 619 8 18472 9 5 6063 797 32 129 98 12 2527 25606 479 24 473 295 128 90 311 5 1498 2870 2523 9 44857 246 29 19 269 251 2523 25606 479 47 531 4943 149 358 21762 2749 36 132 1178 2156 155 1178 2156 204 1178 2156 112 73 176 1178 2156 4753 479 4839 137 562 124 7 2340 1836 646 6661 114 939 128 119 95 27726 9 10 169 7 120 124 7 112 1178 1335 27779 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:24:12 PM label: ['Ġterrible']
06/27 07:24:25 PM ***** Running training *****
06/27 07:24:25 PM   Num examples = 48
06/27 07:24:25 PM   Batch size = 4
06/27 07:24:25 PM   Num steps = 120
06/27 07:24:25 PM n: embeddings.word_embeddings.weight
06/27 07:24:25 PM n: embeddings.position_embeddings.weight
06/27 07:24:25 PM n: embeddings.token_type_embeddings.weight
06/27 07:24:25 PM n: embeddings.LayerNorm.weight
06/27 07:24:25 PM n: embeddings.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.0.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.0.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.0.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.0.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.0.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.0.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.0.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.0.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.0.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.0.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.0.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.0.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.1.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.1.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.1.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.1.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.1.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.1.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.1.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.1.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.1.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.1.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.1.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.1.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.2.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.2.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.2.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.2.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.2.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.2.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.2.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.2.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.2.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.2.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.2.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.2.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.3.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.3.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.3.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.3.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.3.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.3.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.3.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.3.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.3.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.3.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.3.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.3.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.4.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.4.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.4.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.4.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.4.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.4.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.4.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.4.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.4.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.4.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.4.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.4.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.5.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.5.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.5.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.5.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.5.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.5.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.5.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.5.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.5.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.5.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.5.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.5.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.6.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.6.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.6.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.6.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.6.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.6.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.6.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.6.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.6.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.6.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.6.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.6.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.7.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.7.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.7.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.7.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.7.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.7.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.7.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.7.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.7.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.7.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.7.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.7.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.8.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.8.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.8.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.8.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.8.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.8.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.8.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.8.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.8.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.8.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.8.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.8.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.9.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.9.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.9.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.9.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.9.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.9.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.9.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.9.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.9.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.9.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.9.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.9.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.10.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.10.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.10.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.10.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.10.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.10.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.10.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.10.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.10.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.10.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.10.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.10.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.11.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.11.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.11.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.11.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.11.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.11.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.11.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.11.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.11.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.11.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.11.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.11.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.12.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.12.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.12.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.12.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.12.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.12.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.12.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.12.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.12.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.12.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.12.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.12.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.13.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.13.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.13.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.13.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.13.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.13.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.13.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.13.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.13.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.13.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.13.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.13.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.14.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.14.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.14.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.14.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.14.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.14.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.14.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.14.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.14.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.14.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.14.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.14.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.15.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.15.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.15.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.15.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.15.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.15.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.15.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.15.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.15.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.15.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.15.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.15.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.16.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.16.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.16.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.16.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.16.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.16.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.16.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.16.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.16.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.16.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.16.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.16.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.17.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.17.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.17.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.17.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.17.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.17.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.17.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.17.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.17.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.17.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.17.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.17.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.18.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.18.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.18.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.18.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.18.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.18.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.18.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.18.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.18.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.18.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.18.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.18.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.19.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.19.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.19.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.19.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.19.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.19.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.19.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.19.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.19.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.19.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.19.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.19.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.20.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.20.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.20.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.20.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.20.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.20.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.20.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.20.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.20.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.20.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.20.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.20.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.21.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.21.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.21.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.21.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.21.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.21.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.21.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.21.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.21.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.21.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.21.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.21.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.22.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.22.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.22.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.22.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.22.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.22.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.22.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.22.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.22.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.22.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.22.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.22.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.23.attention.self.query.weight
06/27 07:24:25 PM n: encoder.layer.23.attention.self.query.bias
06/27 07:24:25 PM n: encoder.layer.23.attention.self.key.weight
06/27 07:24:25 PM n: encoder.layer.23.attention.self.key.bias
06/27 07:24:25 PM n: encoder.layer.23.attention.self.value.weight
06/27 07:24:25 PM n: encoder.layer.23.attention.self.value.bias
06/27 07:24:25 PM n: encoder.layer.23.attention.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.23.attention.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: encoder.layer.23.intermediate.dense.weight
06/27 07:24:25 PM n: encoder.layer.23.intermediate.dense.bias
06/27 07:24:25 PM n: encoder.layer.23.output.dense.weight
06/27 07:24:25 PM n: encoder.layer.23.output.dense.bias
06/27 07:24:25 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 07:24:25 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 07:24:25 PM n: pooler.dense.weight
06/27 07:24:25 PM n: pooler.dense.bias
06/27 07:24:25 PM n: roberta.embeddings.word_embeddings.weight
06/27 07:24:25 PM n: roberta.embeddings.position_embeddings.weight
06/27 07:24:25 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 07:24:25 PM n: roberta.embeddings.LayerNorm.weight
06/27 07:24:25 PM n: roberta.embeddings.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 07:24:25 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 07:24:25 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 07:24:25 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 07:24:25 PM n: roberta.pooler.dense.weight
06/27 07:24:25 PM n: roberta.pooler.dense.bias
06/27 07:24:25 PM n: lm_head.bias
06/27 07:24:25 PM n: lm_head.dense.weight
06/27 07:24:25 PM n: lm_head.dense.bias
06/27 07:24:25 PM n: lm_head.layer_norm.weight
06/27 07:24:25 PM n: lm_head.layer_norm.bias
06/27 07:24:25 PM n: lm_head.decoder.weight
06/27 07:24:25 PM Total parameters: 763292761
06/27 07:24:25 PM ***** LOSS printing *****
06/27 07:24:25 PM loss
06/27 07:24:25 PM tensor(20.9349, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:24:25 PM ***** LOSS printing *****
06/27 07:24:25 PM loss
06/27 07:24:25 PM tensor(15.4166, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:24:26 PM ***** LOSS printing *****
06/27 07:24:26 PM loss
06/27 07:24:26 PM tensor(7.7595, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:24:26 PM ***** LOSS printing *****
06/27 07:24:26 PM loss
06/27 07:24:26 PM tensor(3.8985, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:24:26 PM ***** Running evaluation MLM *****
06/27 07:24:26 PM   Epoch = 0 iter 4 step
06/27 07:24:26 PM   Num examples = 16
06/27 07:24:26 PM   Batch size = 32
06/27 07:24:27 PM ***** Eval results *****
06/27 07:24:27 PM   acc = 0.875
06/27 07:24:27 PM   cls_loss = 12.002365529537201
06/27 07:24:27 PM   eval_loss = 2.698641061782837
06/27 07:24:27 PM   global_step = 4
06/27 07:24:27 PM   loss = 12.002365529537201
06/27 07:24:27 PM ***** Save model *****
06/27 07:24:27 PM ***** Test Dataset Eval Result *****
06/27 07:25:30 PM ***** Eval results *****
06/27 07:25:30 PM   acc = 0.8145
06/27 07:25:30 PM   cls_loss = 12.002365529537201
06/27 07:25:30 PM   eval_loss = 2.8470572819785467
06/27 07:25:30 PM   global_step = 4
06/27 07:25:30 PM   loss = 12.002365529537201
06/27 07:25:33 PM ***** LOSS printing *****
06/27 07:25:33 PM loss
06/27 07:25:33 PM tensor(3.7541, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:34 PM ***** LOSS printing *****
06/27 07:25:34 PM loss
06/27 07:25:34 PM tensor(3.4411, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:34 PM ***** LOSS printing *****
06/27 07:25:34 PM loss
06/27 07:25:34 PM tensor(3.5476, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:34 PM ***** LOSS printing *****
06/27 07:25:34 PM loss
06/27 07:25:34 PM tensor(3.9631, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:34 PM ***** LOSS printing *****
06/27 07:25:34 PM loss
06/27 07:25:34 PM tensor(1.2605, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:35 PM ***** Running evaluation MLM *****
06/27 07:25:35 PM   Epoch = 0 iter 9 step
06/27 07:25:35 PM   Num examples = 16
06/27 07:25:35 PM   Batch size = 32
06/27 07:25:35 PM ***** Eval results *****
06/27 07:25:35 PM   acc = 0.5625
06/27 07:25:35 PM   cls_loss = 7.108436306317647
06/27 07:25:35 PM   eval_loss = 1.9666922092437744
06/27 07:25:35 PM   global_step = 9
06/27 07:25:35 PM   loss = 7.108436306317647
06/27 07:25:35 PM ***** LOSS printing *****
06/27 07:25:35 PM loss
06/27 07:25:35 PM tensor(2.0513, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:35 PM ***** LOSS printing *****
06/27 07:25:35 PM loss
06/27 07:25:35 PM tensor(2.1872, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:36 PM ***** LOSS printing *****
06/27 07:25:36 PM loss
06/27 07:25:36 PM tensor(3.8979, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:36 PM ***** LOSS printing *****
06/27 07:25:36 PM loss
06/27 07:25:36 PM tensor(3.9847, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:36 PM ***** LOSS printing *****
06/27 07:25:36 PM loss
06/27 07:25:36 PM tensor(1.6479, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:25:36 PM ***** Running evaluation MLM *****
06/27 07:25:36 PM   Epoch = 1 iter 14 step
06/27 07:25:36 PM   Num examples = 16
06/27 07:25:36 PM   Batch size = 32
06/27 07:25:37 PM ***** Eval results *****
06/27 07:25:37 PM   acc = 0.9375
06/27 07:25:37 PM   cls_loss = 2.8163042068481445
06/27 07:25:37 PM   eval_loss = 1.0089973211288452
06/27 07:25:37 PM   global_step = 14
06/27 07:25:37 PM   loss = 2.8163042068481445
06/27 07:25:37 PM ***** Save model *****
06/27 07:25:37 PM ***** Test Dataset Eval Result *****
06/27 07:26:39 PM ***** Eval results *****
06/27 07:26:39 PM   acc = 0.9025
06/27 07:26:39 PM   cls_loss = 2.8163042068481445
06/27 07:26:39 PM   eval_loss = 1.2769888242085774
06/27 07:26:39 PM   global_step = 14
06/27 07:26:39 PM   loss = 2.8163042068481445
06/27 07:26:43 PM ***** LOSS printing *****
06/27 07:26:43 PM loss
06/27 07:26:43 PM tensor(1.4531, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:26:43 PM ***** LOSS printing *****
06/27 07:26:43 PM loss
06/27 07:26:43 PM tensor(1.7414, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:26:43 PM ***** LOSS printing *****
06/27 07:26:43 PM loss
06/27 07:26:43 PM tensor(1.4886, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:26:44 PM ***** LOSS printing *****
06/27 07:26:44 PM loss
06/27 07:26:44 PM tensor(2.2009, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:26:44 PM ***** LOSS printing *****
06/27 07:26:44 PM loss
06/27 07:26:44 PM tensor(1.5332, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:26:44 PM ***** Running evaluation MLM *****
06/27 07:26:44 PM   Epoch = 1 iter 19 step
06/27 07:26:44 PM   Num examples = 16
06/27 07:26:44 PM   Batch size = 32
06/27 07:26:44 PM ***** Eval results *****
06/27 07:26:44 PM   acc = 1.0
06/27 07:26:44 PM   cls_loss = 2.0071106127330234
06/27 07:26:44 PM   eval_loss = 1.3948984146118164
06/27 07:26:44 PM   global_step = 19
06/27 07:26:44 PM   loss = 2.0071106127330234
06/27 07:26:44 PM ***** Save model *****
06/27 07:26:44 PM ***** Test Dataset Eval Result *****
06/27 07:27:46 PM ***** Eval results *****
06/27 07:27:46 PM   acc = 0.8635
06/27 07:27:46 PM   cls_loss = 2.0071106127330234
06/27 07:27:46 PM   eval_loss = 1.864801321710859
06/27 07:27:46 PM   global_step = 19
06/27 07:27:46 PM   loss = 2.0071106127330234
06/27 07:27:51 PM ***** LOSS printing *****
06/27 07:27:51 PM loss
06/27 07:27:51 PM tensor(1.4285, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:51 PM ***** LOSS printing *****
06/27 07:27:51 PM loss
06/27 07:27:51 PM tensor(1.4420, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:51 PM ***** LOSS printing *****
06/27 07:27:51 PM loss
06/27 07:27:51 PM tensor(1.7156, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:52 PM ***** LOSS printing *****
06/27 07:27:52 PM loss
06/27 07:27:52 PM tensor(1.8319, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:52 PM ***** LOSS printing *****
06/27 07:27:52 PM loss
06/27 07:27:52 PM tensor(1.7292, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:52 PM ***** Running evaluation MLM *****
06/27 07:27:52 PM   Epoch = 1 iter 24 step
06/27 07:27:52 PM   Num examples = 16
06/27 07:27:52 PM   Batch size = 32
06/27 07:27:52 PM ***** Eval results *****
06/27 07:27:52 PM   acc = 0.9375
06/27 07:27:52 PM   cls_loss = 1.8497552573680878
06/27 07:27:52 PM   eval_loss = 1.8756614923477173
06/27 07:27:52 PM   global_step = 24
06/27 07:27:52 PM   loss = 1.8497552573680878
06/27 07:27:53 PM ***** LOSS printing *****
06/27 07:27:53 PM loss
06/27 07:27:53 PM tensor(1.6038, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:53 PM ***** LOSS printing *****
06/27 07:27:53 PM loss
06/27 07:27:53 PM tensor(2.9597, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:53 PM ***** LOSS printing *****
06/27 07:27:53 PM loss
06/27 07:27:53 PM tensor(1.5325, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:53 PM ***** LOSS printing *****
06/27 07:27:53 PM loss
06/27 07:27:53 PM tensor(1.7393, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:53 PM ***** LOSS printing *****
06/27 07:27:53 PM loss
06/27 07:27:53 PM tensor(1.1230, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:54 PM ***** Running evaluation MLM *****
06/27 07:27:54 PM   Epoch = 2 iter 29 step
06/27 07:27:54 PM   Num examples = 16
06/27 07:27:54 PM   Batch size = 32
06/27 07:27:54 PM ***** Eval results *****
06/27 07:27:54 PM   acc = 0.9375
06/27 07:27:54 PM   cls_loss = 1.7916579008102418
06/27 07:27:54 PM   eval_loss = 1.6380380392074585
06/27 07:27:54 PM   global_step = 29
06/27 07:27:54 PM   loss = 1.7916579008102418
06/27 07:27:54 PM ***** LOSS printing *****
06/27 07:27:54 PM loss
06/27 07:27:54 PM tensor(1.6444, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:54 PM ***** LOSS printing *****
06/27 07:27:54 PM loss
06/27 07:27:54 PM tensor(0.9462, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:55 PM ***** LOSS printing *****
06/27 07:27:55 PM loss
06/27 07:27:55 PM tensor(1.6061, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:55 PM ***** LOSS printing *****
06/27 07:27:55 PM loss
06/27 07:27:55 PM tensor(1.3192, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:55 PM ***** LOSS printing *****
06/27 07:27:55 PM loss
06/27 07:27:55 PM tensor(1.3291, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:55 PM ***** Running evaluation MLM *****
06/27 07:27:55 PM   Epoch = 2 iter 34 step
06/27 07:27:55 PM   Num examples = 16
06/27 07:27:55 PM   Batch size = 32
06/27 07:27:56 PM ***** Eval results *****
06/27 07:27:56 PM   acc = 0.875
06/27 07:27:56 PM   cls_loss = 1.5803268313407899
06/27 07:27:56 PM   eval_loss = 1.2206637859344482
06/27 07:27:56 PM   global_step = 34
06/27 07:27:56 PM   loss = 1.5803268313407899
06/27 07:27:56 PM ***** LOSS printing *****
06/27 07:27:56 PM loss
06/27 07:27:56 PM tensor(1.4972, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:56 PM ***** LOSS printing *****
06/27 07:27:56 PM loss
06/27 07:27:56 PM tensor(2.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:56 PM ***** LOSS printing *****
06/27 07:27:56 PM loss
06/27 07:27:56 PM tensor(1.1045, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:56 PM ***** LOSS printing *****
06/27 07:27:56 PM loss
06/27 07:27:56 PM tensor(1.2486, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:57 PM ***** LOSS printing *****
06/27 07:27:57 PM loss
06/27 07:27:57 PM tensor(1.7782, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:57 PM ***** Running evaluation MLM *****
06/27 07:27:57 PM   Epoch = 3 iter 39 step
06/27 07:27:57 PM   Num examples = 16
06/27 07:27:57 PM   Batch size = 32
06/27 07:27:57 PM ***** Eval results *****
06/27 07:27:57 PM   acc = 0.875
06/27 07:27:57 PM   cls_loss = 1.377105712890625
06/27 07:27:57 PM   eval_loss = 2.9295380115509033
06/27 07:27:57 PM   global_step = 39
06/27 07:27:57 PM   loss = 1.377105712890625
06/27 07:27:57 PM ***** LOSS printing *****
06/27 07:27:57 PM loss
06/27 07:27:57 PM tensor(1.9207, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:58 PM ***** LOSS printing *****
06/27 07:27:58 PM loss
06/27 07:27:58 PM tensor(2.1538, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:58 PM ***** LOSS printing *****
06/27 07:27:58 PM loss
06/27 07:27:58 PM tensor(1.4436, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:58 PM ***** LOSS printing *****
06/27 07:27:58 PM loss
06/27 07:27:58 PM tensor(2.6837, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:58 PM ***** LOSS printing *****
06/27 07:27:58 PM loss
06/27 07:27:58 PM tensor(1.5498, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:58 PM ***** Running evaluation MLM *****
06/27 07:27:58 PM   Epoch = 3 iter 44 step
06/27 07:27:58 PM   Num examples = 16
06/27 07:27:58 PM   Batch size = 32
06/27 07:27:59 PM ***** Eval results *****
06/27 07:27:59 PM   acc = 0.875
06/27 07:27:59 PM   cls_loss = 1.7353553920984268
06/27 07:27:59 PM   eval_loss = 2.667808771133423
06/27 07:27:59 PM   global_step = 44
06/27 07:27:59 PM   loss = 1.7353553920984268
06/27 07:27:59 PM ***** LOSS printing *****
06/27 07:27:59 PM loss
06/27 07:27:59 PM tensor(1.4948, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:59 PM ***** LOSS printing *****
06/27 07:27:59 PM loss
06/27 07:27:59 PM tensor(1.3593, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:27:59 PM ***** LOSS printing *****
06/27 07:27:59 PM loss
06/27 07:27:59 PM tensor(1.6476, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:00 PM ***** LOSS printing *****
06/27 07:28:00 PM loss
06/27 07:28:00 PM tensor(1.6074, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:00 PM ***** LOSS printing *****
06/27 07:28:00 PM loss
06/27 07:28:00 PM tensor(0.9355, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:00 PM ***** Running evaluation MLM *****
06/27 07:28:00 PM   Epoch = 4 iter 49 step
06/27 07:28:00 PM   Num examples = 16
06/27 07:28:00 PM   Batch size = 32
06/27 07:28:00 PM ***** Eval results *****
06/27 07:28:00 PM   acc = 0.8125
06/27 07:28:00 PM   cls_loss = 0.9354808926582336
06/27 07:28:00 PM   eval_loss = 2.2382407188415527
06/27 07:28:00 PM   global_step = 49
06/27 07:28:00 PM   loss = 0.9354808926582336
06/27 07:28:01 PM ***** LOSS printing *****
06/27 07:28:01 PM loss
06/27 07:28:01 PM tensor(1.3108, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:01 PM ***** LOSS printing *****
06/27 07:28:01 PM loss
06/27 07:28:01 PM tensor(1.4614, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:01 PM ***** LOSS printing *****
06/27 07:28:01 PM loss
06/27 07:28:01 PM tensor(1.4205, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:01 PM ***** LOSS printing *****
06/27 07:28:01 PM loss
06/27 07:28:01 PM tensor(1.6147, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:01 PM ***** LOSS printing *****
06/27 07:28:01 PM loss
06/27 07:28:01 PM tensor(1.1453, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:02 PM ***** Running evaluation MLM *****
06/27 07:28:02 PM   Epoch = 4 iter 54 step
06/27 07:28:02 PM   Num examples = 16
06/27 07:28:02 PM   Batch size = 32
06/27 07:28:02 PM ***** Eval results *****
06/27 07:28:02 PM   acc = 0.75
06/27 07:28:02 PM   cls_loss = 1.3146896461645763
06/27 07:28:02 PM   eval_loss = 1.6806349754333496
06/27 07:28:02 PM   global_step = 54
06/27 07:28:02 PM   loss = 1.3146896461645763
06/27 07:28:02 PM ***** LOSS printing *****
06/27 07:28:02 PM loss
06/27 07:28:02 PM tensor(0.8715, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:02 PM ***** LOSS printing *****
06/27 07:28:02 PM loss
06/27 07:28:02 PM tensor(2.0131, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:03 PM ***** LOSS printing *****
06/27 07:28:03 PM loss
06/27 07:28:03 PM tensor(2.0754, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:03 PM ***** LOSS printing *****
06/27 07:28:03 PM loss
06/27 07:28:03 PM tensor(1.7269, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:03 PM ***** LOSS printing *****
06/27 07:28:03 PM loss
06/27 07:28:03 PM tensor(1.6355, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:03 PM ***** Running evaluation MLM *****
06/27 07:28:03 PM   Epoch = 4 iter 59 step
06/27 07:28:03 PM   Num examples = 16
06/27 07:28:03 PM   Batch size = 32
06/27 07:28:04 PM ***** Eval results *****
06/27 07:28:04 PM   acc = 0.75
06/27 07:28:04 PM   cls_loss = 1.4736899516799233
06/27 07:28:04 PM   eval_loss = 1.5970622301101685
06/27 07:28:04 PM   global_step = 59
06/27 07:28:04 PM   loss = 1.4736899516799233
06/27 07:28:04 PM ***** LOSS printing *****
06/27 07:28:04 PM loss
06/27 07:28:04 PM tensor(1.3800, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:04 PM ***** LOSS printing *****
06/27 07:28:04 PM loss
06/27 07:28:04 PM tensor(0.8676, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:04 PM ***** LOSS printing *****
06/27 07:28:04 PM loss
06/27 07:28:04 PM tensor(0.9424, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:04 PM ***** LOSS printing *****
06/27 07:28:04 PM loss
06/27 07:28:04 PM tensor(1.3978, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:05 PM ***** LOSS printing *****
06/27 07:28:05 PM loss
06/27 07:28:05 PM tensor(1.6089, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:05 PM ***** Running evaluation MLM *****
06/27 07:28:05 PM   Epoch = 5 iter 64 step
06/27 07:28:05 PM   Num examples = 16
06/27 07:28:05 PM   Batch size = 32
06/27 07:28:05 PM ***** Eval results *****
06/27 07:28:05 PM   acc = 0.8125
06/27 07:28:05 PM   cls_loss = 1.2041619718074799
06/27 07:28:05 PM   eval_loss = 1.746802568435669
06/27 07:28:05 PM   global_step = 64
06/27 07:28:05 PM   loss = 1.2041619718074799
06/27 07:28:05 PM ***** LOSS printing *****
06/27 07:28:05 PM loss
06/27 07:28:05 PM tensor(1.6750, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:06 PM ***** LOSS printing *****
06/27 07:28:06 PM loss
06/27 07:28:06 PM tensor(1.0407, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:06 PM ***** LOSS printing *****
06/27 07:28:06 PM loss
06/27 07:28:06 PM tensor(1.2474, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:06 PM ***** LOSS printing *****
06/27 07:28:06 PM loss
06/27 07:28:06 PM tensor(1.2612, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:06 PM ***** LOSS printing *****
06/27 07:28:06 PM loss
06/27 07:28:06 PM tensor(1.1478, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:06 PM ***** Running evaluation MLM *****
06/27 07:28:06 PM   Epoch = 5 iter 69 step
06/27 07:28:06 PM   Num examples = 16
06/27 07:28:06 PM   Batch size = 32
06/27 07:28:07 PM ***** Eval results *****
06/27 07:28:07 PM   acc = 0.875
06/27 07:28:07 PM   cls_loss = 1.2432033485836453
06/27 07:28:07 PM   eval_loss = 2.6921181678771973
06/27 07:28:07 PM   global_step = 69
06/27 07:28:07 PM   loss = 1.2432033485836453
06/27 07:28:07 PM ***** LOSS printing *****
06/27 07:28:07 PM loss
06/27 07:28:07 PM tensor(2.2573, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:07 PM ***** LOSS printing *****
06/27 07:28:07 PM loss
06/27 07:28:07 PM tensor(1.7178, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:07 PM ***** LOSS printing *****
06/27 07:28:07 PM loss
06/27 07:28:07 PM tensor(1.3804, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:08 PM ***** LOSS printing *****
06/27 07:28:08 PM loss
06/27 07:28:08 PM tensor(1.5380, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:08 PM ***** LOSS printing *****
06/27 07:28:08 PM loss
06/27 07:28:08 PM tensor(0.9948, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:08 PM ***** Running evaluation MLM *****
06/27 07:28:08 PM   Epoch = 6 iter 74 step
06/27 07:28:08 PM   Num examples = 16
06/27 07:28:08 PM   Batch size = 32
06/27 07:28:09 PM ***** Eval results *****
06/27 07:28:09 PM   acc = 0.875
06/27 07:28:09 PM   cls_loss = 1.2663762271404266
06/27 07:28:09 PM   eval_loss = 2.4674689769744873
06/27 07:28:09 PM   global_step = 74
06/27 07:28:09 PM   loss = 1.2663762271404266
06/27 07:28:09 PM ***** LOSS printing *****
06/27 07:28:09 PM loss
06/27 07:28:09 PM tensor(1.0205, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:09 PM ***** LOSS printing *****
06/27 07:28:09 PM loss
06/27 07:28:09 PM tensor(1.4655, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:09 PM ***** LOSS printing *****
06/27 07:28:09 PM loss
06/27 07:28:09 PM tensor(2.3238, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:09 PM ***** LOSS printing *****
06/27 07:28:09 PM loss
06/27 07:28:09 PM tensor(1.9700, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:09 PM ***** LOSS printing *****
06/27 07:28:09 PM loss
06/27 07:28:09 PM tensor(1.2467, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:10 PM ***** Running evaluation MLM *****
06/27 07:28:10 PM   Epoch = 6 iter 79 step
06/27 07:28:10 PM   Num examples = 16
06/27 07:28:10 PM   Batch size = 32
06/27 07:28:10 PM ***** Eval results *****
06/27 07:28:10 PM   acc = 0.875
06/27 07:28:10 PM   cls_loss = 1.508472638470786
06/27 07:28:10 PM   eval_loss = 2.2053065299987793
06/27 07:28:10 PM   global_step = 79
06/27 07:28:10 PM   loss = 1.508472638470786
06/27 07:28:10 PM ***** LOSS printing *****
06/27 07:28:10 PM loss
06/27 07:28:10 PM tensor(1.1342, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:10 PM ***** LOSS printing *****
06/27 07:28:10 PM loss
06/27 07:28:10 PM tensor(2.1904, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:11 PM ***** LOSS printing *****
06/27 07:28:11 PM loss
06/27 07:28:11 PM tensor(1.5749, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:11 PM ***** LOSS printing *****
06/27 07:28:11 PM loss
06/27 07:28:11 PM tensor(1.5303, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:11 PM ***** LOSS printing *****
06/27 07:28:11 PM loss
06/27 07:28:11 PM tensor(1.6606, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:11 PM ***** Running evaluation MLM *****
06/27 07:28:11 PM   Epoch = 6 iter 84 step
06/27 07:28:11 PM   Num examples = 16
06/27 07:28:11 PM   Batch size = 32
06/27 07:28:12 PM ***** Eval results *****
06/27 07:28:12 PM   acc = 0.875
06/27 07:28:12 PM   cls_loss = 1.5541381885608037
06/27 07:28:12 PM   eval_loss = 1.9746923446655273
06/27 07:28:12 PM   global_step = 84
06/27 07:28:12 PM   loss = 1.5541381885608037
06/27 07:28:12 PM ***** LOSS printing *****
06/27 07:28:12 PM loss
06/27 07:28:12 PM tensor(1.4265, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:12 PM ***** LOSS printing *****
06/27 07:28:12 PM loss
06/27 07:28:12 PM tensor(1.2636, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:12 PM ***** LOSS printing *****
06/27 07:28:12 PM loss
06/27 07:28:12 PM tensor(1.0469, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:12 PM ***** LOSS printing *****
06/27 07:28:12 PM loss
06/27 07:28:12 PM tensor(1.5087, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:13 PM ***** LOSS printing *****
06/27 07:28:13 PM loss
06/27 07:28:13 PM tensor(1.4207, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:13 PM ***** Running evaluation MLM *****
06/27 07:28:13 PM   Epoch = 7 iter 89 step
06/27 07:28:13 PM   Num examples = 16
06/27 07:28:13 PM   Batch size = 32
06/27 07:28:13 PM ***** Eval results *****
06/27 07:28:13 PM   acc = 0.875
06/27 07:28:13 PM   cls_loss = 1.3332866191864015
06/27 07:28:13 PM   eval_loss = 1.2434759140014648
06/27 07:28:13 PM   global_step = 89
06/27 07:28:13 PM   loss = 1.3332866191864015
06/27 07:28:13 PM ***** LOSS printing *****
06/27 07:28:13 PM loss
06/27 07:28:13 PM tensor(1.4212, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:14 PM ***** LOSS printing *****
06/27 07:28:14 PM loss
06/27 07:28:14 PM tensor(1.3323, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:14 PM ***** LOSS printing *****
06/27 07:28:14 PM loss
06/27 07:28:14 PM tensor(0.8655, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:14 PM ***** LOSS printing *****
06/27 07:28:14 PM loss
06/27 07:28:14 PM tensor(1.1799, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:14 PM ***** LOSS printing *****
06/27 07:28:14 PM loss
06/27 07:28:14 PM tensor(1.4522, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:14 PM ***** Running evaluation MLM *****
06/27 07:28:14 PM   Epoch = 7 iter 94 step
06/27 07:28:14 PM   Num examples = 16
06/27 07:28:14 PM   Batch size = 32
06/27 07:28:15 PM ***** Eval results *****
06/27 07:28:15 PM   acc = 0.875
06/27 07:28:15 PM   cls_loss = 1.2917564988136292
06/27 07:28:15 PM   eval_loss = 1.348156452178955
06/27 07:28:15 PM   global_step = 94
06/27 07:28:15 PM   loss = 1.2917564988136292
06/27 07:28:15 PM ***** LOSS printing *****
06/27 07:28:15 PM loss
06/27 07:28:15 PM tensor(1.8898, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:15 PM ***** LOSS printing *****
06/27 07:28:15 PM loss
06/27 07:28:15 PM tensor(2.1750, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:15 PM ***** LOSS printing *****
06/27 07:28:15 PM loss
06/27 07:28:15 PM tensor(1.3609, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:16 PM ***** LOSS printing *****
06/27 07:28:16 PM loss
06/27 07:28:16 PM tensor(1.1647, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:16 PM ***** LOSS printing *****
06/27 07:28:16 PM loss
06/27 07:28:16 PM tensor(1.0946, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:16 PM ***** Running evaluation MLM *****
06/27 07:28:16 PM   Epoch = 8 iter 99 step
06/27 07:28:16 PM   Num examples = 16
06/27 07:28:16 PM   Batch size = 32
06/27 07:28:17 PM ***** Eval results *****
06/27 07:28:17 PM   acc = 0.75
06/27 07:28:17 PM   cls_loss = 1.2067404588063557
06/27 07:28:17 PM   eval_loss = 2.847349166870117
06/27 07:28:17 PM   global_step = 99
06/27 07:28:17 PM   loss = 1.2067404588063557
06/27 07:28:17 PM ***** LOSS printing *****
06/27 07:28:17 PM loss
06/27 07:28:17 PM tensor(1.9270, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:17 PM ***** LOSS printing *****
06/27 07:28:17 PM loss
06/27 07:28:17 PM tensor(1.1172, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:17 PM ***** LOSS printing *****
06/27 07:28:17 PM loss
06/27 07:28:17 PM tensor(2.0908, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:17 PM ***** LOSS printing *****
06/27 07:28:17 PM loss
06/27 07:28:17 PM tensor(0.9837, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:18 PM ***** LOSS printing *****
06/27 07:28:18 PM loss
06/27 07:28:18 PM tensor(1.5916, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:18 PM ***** Running evaluation MLM *****
06/27 07:28:18 PM   Epoch = 8 iter 104 step
06/27 07:28:18 PM   Num examples = 16
06/27 07:28:18 PM   Batch size = 32
06/27 07:28:18 PM ***** Eval results *****
06/27 07:28:18 PM   acc = 0.75
06/27 07:28:18 PM   cls_loss = 1.416311576962471
06/27 07:28:18 PM   eval_loss = 2.9367029666900635
06/27 07:28:18 PM   global_step = 104
06/27 07:28:18 PM   loss = 1.416311576962471
06/27 07:28:18 PM ***** LOSS printing *****
06/27 07:28:18 PM loss
06/27 07:28:18 PM tensor(1.0539, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:19 PM ***** LOSS printing *****
06/27 07:28:19 PM loss
06/27 07:28:19 PM tensor(1.7596, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:19 PM ***** LOSS printing *****
06/27 07:28:19 PM loss
06/27 07:28:19 PM tensor(1.6704, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:19 PM ***** LOSS printing *****
06/27 07:28:19 PM loss
06/27 07:28:19 PM tensor(1.3444, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:19 PM ***** LOSS printing *****
06/27 07:28:19 PM loss
06/27 07:28:19 PM tensor(0.9555, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:19 PM ***** Running evaluation MLM *****
06/27 07:28:19 PM   Epoch = 9 iter 109 step
06/27 07:28:19 PM   Num examples = 16
06/27 07:28:19 PM   Batch size = 32
06/27 07:28:20 PM ***** Eval results *****
06/27 07:28:20 PM   acc = 0.8125
06/27 07:28:20 PM   cls_loss = 0.9554975032806396
06/27 07:28:20 PM   eval_loss = 2.242440700531006
06/27 07:28:20 PM   global_step = 109
06/27 07:28:20 PM   loss = 0.9554975032806396
06/27 07:28:20 PM ***** LOSS printing *****
06/27 07:28:20 PM loss
06/27 07:28:20 PM tensor(1.2615, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:20 PM ***** LOSS printing *****
06/27 07:28:20 PM loss
06/27 07:28:20 PM tensor(1.2133, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:20 PM ***** LOSS printing *****
06/27 07:28:20 PM loss
06/27 07:28:20 PM tensor(1.2859, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:21 PM ***** LOSS printing *****
06/27 07:28:21 PM loss
06/27 07:28:21 PM tensor(1.2877, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:21 PM ***** LOSS printing *****
06/27 07:28:21 PM loss
06/27 07:28:21 PM tensor(1.1252, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:21 PM ***** Running evaluation MLM *****
06/27 07:28:21 PM   Epoch = 9 iter 114 step
06/27 07:28:21 PM   Num examples = 16
06/27 07:28:21 PM   Batch size = 32
06/27 07:28:21 PM ***** Eval results *****
06/27 07:28:21 PM   acc = 0.875
06/27 07:28:21 PM   cls_loss = 1.1881733536720276
06/27 07:28:21 PM   eval_loss = 1.5737173557281494
06/27 07:28:21 PM   global_step = 114
06/27 07:28:21 PM   loss = 1.1881733536720276
06/27 07:28:22 PM ***** LOSS printing *****
06/27 07:28:22 PM loss
06/27 07:28:22 PM tensor(1.8533, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:22 PM ***** LOSS printing *****
06/27 07:28:22 PM loss
06/27 07:28:22 PM tensor(1.2391, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:22 PM ***** LOSS printing *****
06/27 07:28:22 PM loss
06/27 07:28:22 PM tensor(1.1915, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:22 PM ***** LOSS printing *****
06/27 07:28:22 PM loss
06/27 07:28:22 PM tensor(1.2200, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:22 PM ***** LOSS printing *****
06/27 07:28:22 PM loss
06/27 07:28:22 PM tensor(1.8825, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:28:23 PM ***** Running evaluation MLM *****
06/27 07:28:23 PM   Epoch = 9 iter 119 step
06/27 07:28:23 PM   Num examples = 16
06/27 07:28:23 PM   Batch size = 32
06/27 07:28:23 PM ***** Eval results *****
06/27 07:28:23 PM   acc = 0.875
06/27 07:28:23 PM   cls_loss = 1.3195924975655295
06/27 07:28:23 PM   eval_loss = 1.5271449089050293
06/27 07:28:23 PM   global_step = 119
06/27 07:28:23 PM   loss = 1.3195924975655295
06/27 07:28:23 PM ***** LOSS printing *****
06/27 07:28:23 PM loss
06/27 07:28:23 PM tensor(1.4902, device='cuda:0', grad_fn=<NllLossBackward0>)

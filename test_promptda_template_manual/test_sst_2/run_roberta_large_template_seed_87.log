06/27 07:13:17 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/SST-2/8-87/', data_seed=87, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='sst-2', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 07:13:17 PM device: cuda n_gpu: 1
06/27 07:13:17 PM Writing example 0 of 48
06/27 07:13:17 PM *** Example ***
06/27 07:13:17 PM guid: train-1
06/27 07:13:17 PM tokens: <s> the Ġsatire Ġis Ġunfocused Ġ, Ġwhile Ġthe Ġstory Ġgoes Ġnowhere Ġ. </s> ĠIt Ġis <mask>
06/27 07:13:17 PM input_ids: 0 627 31368 16 47306 2156 150 5 527 1411 9261 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM label: ['Ġterrible']
06/27 07:13:17 PM Writing example 0 of 16
06/27 07:13:17 PM *** Example ***
06/27 07:13:17 PM guid: dev-1
06/27 07:13:17 PM tokens: <s> norm ally Ġ, Ġro h mer Ġ' s Ġtalk y Ġfilms Ġfasc inate Ġme Ġ, Ġbut Ġwhen Ġhe Ġmoves Ġhis Ġsetting Ġto Ġthe Ġpast Ġ, Ġand Ġrelies Ġon Ġa Ġhistorical Ġtext Ġ, Ġhe Ġloses Ġthe Ġrichness Ġof Ġcharacterization Ġthat Ġmakes Ġhis Ġfilms Ġso Ġmemorable Ġ. </s> ĠIt Ġis <mask>
06/27 07:13:17 PM input_ids: 0 42258 2368 2156 4533 298 2089 128 29 1067 219 3541 35439 13014 162 2156 53 77 37 3136 39 2749 7 5 375 2156 8 12438 15 10 4566 2788 2156 37 13585 5 38857 9 34934 14 817 39 3541 98 10132 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM label: ['Ġterrible']
06/27 07:13:17 PM Writing example 0 of 872
06/27 07:13:17 PM *** Example ***
06/27 07:13:17 PM guid: dev-1
06/27 07:13:17 PM tokens: <s> one Ġlong Ġstring Ġof Ġcl ic hes Ġ. </s> ĠIt Ġis <mask>
06/27 07:13:17 PM input_ids: 0 1264 251 6755 9 3741 636 5065 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:13:17 PM label: ['Ġterrible']
06/27 07:13:30 PM ***** Running training *****
06/27 07:13:30 PM   Num examples = 48
06/27 07:13:30 PM   Batch size = 4
06/27 07:13:30 PM   Num steps = 120
06/27 07:13:30 PM n: embeddings.word_embeddings.weight
06/27 07:13:30 PM n: embeddings.position_embeddings.weight
06/27 07:13:30 PM n: embeddings.token_type_embeddings.weight
06/27 07:13:30 PM n: embeddings.LayerNorm.weight
06/27 07:13:30 PM n: embeddings.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.0.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.0.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.0.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.0.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.0.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.0.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.0.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.0.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.0.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.0.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.0.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.0.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.1.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.1.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.1.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.1.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.1.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.1.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.1.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.1.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.1.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.1.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.1.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.1.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.2.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.2.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.2.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.2.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.2.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.2.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.2.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.2.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.2.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.2.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.2.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.2.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.3.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.3.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.3.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.3.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.3.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.3.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.3.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.3.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.3.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.3.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.3.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.3.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.4.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.4.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.4.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.4.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.4.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.4.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.4.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.4.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.4.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.4.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.4.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.4.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.5.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.5.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.5.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.5.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.5.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.5.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.5.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.5.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.5.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.5.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.5.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.5.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.6.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.6.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.6.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.6.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.6.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.6.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.6.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.6.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.6.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.6.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.6.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.6.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.7.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.7.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.7.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.7.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.7.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.7.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.7.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.7.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.7.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.7.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.7.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.7.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.8.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.8.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.8.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.8.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.8.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.8.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.8.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.8.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.8.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.8.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.8.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.8.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.9.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.9.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.9.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.9.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.9.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.9.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.9.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.9.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.9.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.9.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.9.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.9.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.10.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.10.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.10.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.10.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.10.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.10.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.10.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.10.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.10.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.10.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.10.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.10.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.11.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.11.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.11.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.11.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.11.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.11.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.11.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.11.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.11.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.11.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.11.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.11.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.12.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.12.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.12.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.12.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.12.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.12.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.12.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.12.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.12.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.12.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.12.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.12.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.13.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.13.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.13.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.13.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.13.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.13.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.13.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.13.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.13.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.13.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.13.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.13.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.14.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.14.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.14.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.14.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.14.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.14.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.14.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.14.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.14.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.14.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.14.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.14.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.15.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.15.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.15.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.15.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.15.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.15.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.15.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.15.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.15.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.15.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.15.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.15.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.16.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.16.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.16.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.16.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.16.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.16.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.16.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.16.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.16.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.16.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.16.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.16.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.17.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.17.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.17.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.17.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.17.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.17.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.17.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.17.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.17.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.17.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.17.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.17.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.18.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.18.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.18.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.18.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.18.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.18.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.18.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.18.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.18.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.18.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.18.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.18.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.19.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.19.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.19.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.19.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.19.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.19.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.19.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.19.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.19.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.19.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.19.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.19.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.20.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.20.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.20.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.20.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.20.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.20.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.20.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.20.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.20.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.20.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.20.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.20.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.21.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.21.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.21.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.21.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.21.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.21.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.21.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.21.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.21.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.21.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.21.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.21.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.22.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.22.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.22.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.22.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.22.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.22.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.22.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.22.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.22.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.22.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.22.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.22.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.23.attention.self.query.weight
06/27 07:13:30 PM n: encoder.layer.23.attention.self.query.bias
06/27 07:13:30 PM n: encoder.layer.23.attention.self.key.weight
06/27 07:13:30 PM n: encoder.layer.23.attention.self.key.bias
06/27 07:13:30 PM n: encoder.layer.23.attention.self.value.weight
06/27 07:13:30 PM n: encoder.layer.23.attention.self.value.bias
06/27 07:13:30 PM n: encoder.layer.23.attention.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.23.attention.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: encoder.layer.23.intermediate.dense.weight
06/27 07:13:30 PM n: encoder.layer.23.intermediate.dense.bias
06/27 07:13:30 PM n: encoder.layer.23.output.dense.weight
06/27 07:13:30 PM n: encoder.layer.23.output.dense.bias
06/27 07:13:30 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 07:13:30 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 07:13:30 PM n: pooler.dense.weight
06/27 07:13:30 PM n: pooler.dense.bias
06/27 07:13:30 PM n: roberta.embeddings.word_embeddings.weight
06/27 07:13:30 PM n: roberta.embeddings.position_embeddings.weight
06/27 07:13:30 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 07:13:30 PM n: roberta.embeddings.LayerNorm.weight
06/27 07:13:30 PM n: roberta.embeddings.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 07:13:30 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 07:13:30 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 07:13:30 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 07:13:30 PM n: roberta.pooler.dense.weight
06/27 07:13:30 PM n: roberta.pooler.dense.bias
06/27 07:13:30 PM n: lm_head.bias
06/27 07:13:30 PM n: lm_head.dense.weight
06/27 07:13:30 PM n: lm_head.dense.bias
06/27 07:13:30 PM n: lm_head.layer_norm.weight
06/27 07:13:30 PM n: lm_head.layer_norm.bias
06/27 07:13:30 PM n: lm_head.decoder.weight
06/27 07:13:30 PM Total parameters: 763292761
06/27 07:13:30 PM ***** LOSS printing *****
06/27 07:13:30 PM loss
06/27 07:13:30 PM tensor(20.3841, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:13:31 PM ***** LOSS printing *****
06/27 07:13:31 PM loss
06/27 07:13:31 PM tensor(13.7189, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:13:31 PM ***** LOSS printing *****
06/27 07:13:31 PM loss
06/27 07:13:31 PM tensor(9.2185, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:13:31 PM ***** LOSS printing *****
06/27 07:13:31 PM loss
06/27 07:13:31 PM tensor(5.3826, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:13:31 PM ***** Running evaluation MLM *****
06/27 07:13:31 PM   Epoch = 0 iter 4 step
06/27 07:13:31 PM   Num examples = 16
06/27 07:13:31 PM   Batch size = 32
06/27 07:13:32 PM ***** Eval results *****
06/27 07:13:32 PM   acc = 0.75
06/27 07:13:32 PM   cls_loss = 12.17603349685669
06/27 07:13:32 PM   eval_loss = 2.902390241622925
06/27 07:13:32 PM   global_step = 4
06/27 07:13:32 PM   loss = 12.17603349685669
06/27 07:13:32 PM ***** Save model *****
06/27 07:13:32 PM ***** Test Dataset Eval Result *****
06/27 07:13:59 PM ***** Eval results *****
06/27 07:13:59 PM   acc = 0.7431192660550459
06/27 07:13:59 PM   cls_loss = 12.17603349685669
06/27 07:13:59 PM   eval_loss = 2.9200670889445712
06/27 07:13:59 PM   global_step = 4
06/27 07:13:59 PM   loss = 12.17603349685669
06/27 07:14:03 PM ***** LOSS printing *****
06/27 07:14:03 PM loss
06/27 07:14:03 PM tensor(4.6257, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:04 PM ***** LOSS printing *****
06/27 07:14:04 PM loss
06/27 07:14:04 PM tensor(2.7893, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:04 PM ***** LOSS printing *****
06/27 07:14:04 PM loss
06/27 07:14:04 PM tensor(3.1611, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:04 PM ***** LOSS printing *****
06/27 07:14:04 PM loss
06/27 07:14:04 PM tensor(2.2292, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:04 PM ***** LOSS printing *****
06/27 07:14:04 PM loss
06/27 07:14:04 PM tensor(5.2913, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:05 PM ***** Running evaluation MLM *****
06/27 07:14:05 PM   Epoch = 0 iter 9 step
06/27 07:14:05 PM   Num examples = 16
06/27 07:14:05 PM   Batch size = 32
06/27 07:14:05 PM ***** Eval results *****
06/27 07:14:05 PM   acc = 0.875
06/27 07:14:05 PM   cls_loss = 7.422296418084039
06/27 07:14:05 PM   eval_loss = 3.0083940029144287
06/27 07:14:05 PM   global_step = 9
06/27 07:14:05 PM   loss = 7.422296418084039
06/27 07:14:05 PM ***** Save model *****
06/27 07:14:05 PM ***** Test Dataset Eval Result *****
06/27 07:14:32 PM ***** Eval results *****
06/27 07:14:32 PM   acc = 0.8658256880733946
06/27 07:14:32 PM   cls_loss = 7.422296418084039
06/27 07:14:32 PM   eval_loss = 3.0683787635394504
06/27 07:14:32 PM   global_step = 9
06/27 07:14:32 PM   loss = 7.422296418084039
06/27 07:14:37 PM ***** LOSS printing *****
06/27 07:14:37 PM loss
06/27 07:14:37 PM tensor(2.5626, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:37 PM ***** LOSS printing *****
06/27 07:14:37 PM loss
06/27 07:14:37 PM tensor(3.3558, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:37 PM ***** LOSS printing *****
06/27 07:14:37 PM loss
06/27 07:14:37 PM tensor(2.6554, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:37 PM ***** LOSS printing *****
06/27 07:14:37 PM loss
06/27 07:14:37 PM tensor(2.5984, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:37 PM ***** LOSS printing *****
06/27 07:14:37 PM loss
06/27 07:14:37 PM tensor(2.2660, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:38 PM ***** Running evaluation MLM *****
06/27 07:14:38 PM   Epoch = 1 iter 14 step
06/27 07:14:38 PM   Num examples = 16
06/27 07:14:38 PM   Batch size = 32
06/27 07:14:38 PM ***** Eval results *****
06/27 07:14:38 PM   acc = 0.75
06/27 07:14:38 PM   cls_loss = 2.432236671447754
06/27 07:14:38 PM   eval_loss = 1.9845385551452637
06/27 07:14:38 PM   global_step = 14
06/27 07:14:38 PM   loss = 2.432236671447754
06/27 07:14:38 PM ***** LOSS printing *****
06/27 07:14:38 PM loss
06/27 07:14:38 PM tensor(1.9078, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:38 PM ***** LOSS printing *****
06/27 07:14:38 PM loss
06/27 07:14:38 PM tensor(2.0333, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:39 PM ***** LOSS printing *****
06/27 07:14:39 PM loss
06/27 07:14:39 PM tensor(2.7036, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:39 PM ***** LOSS printing *****
06/27 07:14:39 PM loss
06/27 07:14:39 PM tensor(1.9999, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:39 PM ***** LOSS printing *****
06/27 07:14:39 PM loss
06/27 07:14:39 PM tensor(2.3359, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:39 PM ***** Running evaluation MLM *****
06/27 07:14:39 PM   Epoch = 1 iter 19 step
06/27 07:14:39 PM   Num examples = 16
06/27 07:14:39 PM   Batch size = 32
06/27 07:14:40 PM ***** Eval results *****
06/27 07:14:40 PM   acc = 0.8125
06/27 07:14:40 PM   cls_loss = 2.263573374067034
06/27 07:14:40 PM   eval_loss = 1.7211352586746216
06/27 07:14:40 PM   global_step = 19
06/27 07:14:40 PM   loss = 2.263573374067034
06/27 07:14:40 PM ***** LOSS printing *****
06/27 07:14:40 PM loss
06/27 07:14:40 PM tensor(1.6036, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:40 PM ***** LOSS printing *****
06/27 07:14:40 PM loss
06/27 07:14:40 PM tensor(1.2511, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:40 PM ***** LOSS printing *****
06/27 07:14:40 PM loss
06/27 07:14:40 PM tensor(2.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:40 PM ***** LOSS printing *****
06/27 07:14:40 PM loss
06/27 07:14:40 PM tensor(2.4653, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:41 PM ***** LOSS printing *****
06/27 07:14:41 PM loss
06/27 07:14:41 PM tensor(3.2676, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:41 PM ***** Running evaluation MLM *****
06/27 07:14:41 PM   Epoch = 1 iter 24 step
06/27 07:14:41 PM   Num examples = 16
06/27 07:14:41 PM   Batch size = 32
06/27 07:14:41 PM ***** Eval results *****
06/27 07:14:41 PM   acc = 0.8125
06/27 07:14:41 PM   cls_loss = 2.256043871243795
06/27 07:14:41 PM   eval_loss = 1.9642831087112427
06/27 07:14:41 PM   global_step = 24
06/27 07:14:41 PM   loss = 2.256043871243795
06/27 07:14:41 PM ***** LOSS printing *****
06/27 07:14:41 PM loss
06/27 07:14:41 PM tensor(2.4733, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:42 PM ***** LOSS printing *****
06/27 07:14:42 PM loss
06/27 07:14:42 PM tensor(1.5233, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:42 PM ***** LOSS printing *****
06/27 07:14:42 PM loss
06/27 07:14:42 PM tensor(1.2044, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:42 PM ***** LOSS printing *****
06/27 07:14:42 PM loss
06/27 07:14:42 PM tensor(2.8623, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:42 PM ***** LOSS printing *****
06/27 07:14:42 PM loss
06/27 07:14:42 PM tensor(2.8337, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:42 PM ***** Running evaluation MLM *****
06/27 07:14:42 PM   Epoch = 2 iter 29 step
06/27 07:14:42 PM   Num examples = 16
06/27 07:14:42 PM   Batch size = 32
06/27 07:14:43 PM ***** Eval results *****
06/27 07:14:43 PM   acc = 0.6875
06/27 07:14:43 PM   cls_loss = 2.1793997526168822
06/27 07:14:43 PM   eval_loss = 2.1481521129608154
06/27 07:14:43 PM   global_step = 29
06/27 07:14:43 PM   loss = 2.1793997526168822
06/27 07:14:43 PM ***** LOSS printing *****
06/27 07:14:43 PM loss
06/27 07:14:43 PM tensor(1.9184, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:43 PM ***** LOSS printing *****
06/27 07:14:43 PM loss
06/27 07:14:43 PM tensor(2.1886, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:43 PM ***** LOSS printing *****
06/27 07:14:43 PM loss
06/27 07:14:43 PM tensor(1.5915, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:44 PM ***** LOSS printing *****
06/27 07:14:44 PM loss
06/27 07:14:44 PM tensor(1.6206, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:44 PM ***** LOSS printing *****
06/27 07:14:44 PM loss
06/27 07:14:44 PM tensor(1.3604, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:44 PM ***** Running evaluation MLM *****
06/27 07:14:44 PM   Epoch = 2 iter 34 step
06/27 07:14:44 PM   Num examples = 16
06/27 07:14:44 PM   Batch size = 32
06/27 07:14:45 PM ***** Eval results *****
06/27 07:14:45 PM   acc = 0.75
06/27 07:14:45 PM   cls_loss = 1.9576495885849
06/27 07:14:45 PM   eval_loss = 1.7658538818359375
06/27 07:14:45 PM   global_step = 34
06/27 07:14:45 PM   loss = 1.9576495885849
06/27 07:14:45 PM ***** LOSS printing *****
06/27 07:14:45 PM loss
06/27 07:14:45 PM tensor(2.7030, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:45 PM ***** LOSS printing *****
06/27 07:14:45 PM loss
06/27 07:14:45 PM tensor(2.3685, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:45 PM ***** LOSS printing *****
06/27 07:14:45 PM loss
06/27 07:14:45 PM tensor(2.2785, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:45 PM ***** LOSS printing *****
06/27 07:14:45 PM loss
06/27 07:14:45 PM tensor(1.1483, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:45 PM ***** LOSS printing *****
06/27 07:14:45 PM loss
06/27 07:14:45 PM tensor(2.1307, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:46 PM ***** Running evaluation MLM *****
06/27 07:14:46 PM   Epoch = 3 iter 39 step
06/27 07:14:46 PM   Num examples = 16
06/27 07:14:46 PM   Batch size = 32
06/27 07:14:46 PM ***** Eval results *****
06/27 07:14:46 PM   acc = 0.6875
06/27 07:14:46 PM   cls_loss = 1.8524879614512126
06/27 07:14:46 PM   eval_loss = 1.7904874086380005
06/27 07:14:46 PM   global_step = 39
06/27 07:14:46 PM   loss = 1.8524879614512126
06/27 07:14:46 PM ***** LOSS printing *****
06/27 07:14:46 PM loss
06/27 07:14:46 PM tensor(3.8913, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:46 PM ***** LOSS printing *****
06/27 07:14:46 PM loss
06/27 07:14:46 PM tensor(3.6150, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:47 PM ***** LOSS printing *****
06/27 07:14:47 PM loss
06/27 07:14:47 PM tensor(1.4874, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:47 PM ***** LOSS printing *****
06/27 07:14:47 PM loss
06/27 07:14:47 PM tensor(3.0014, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:47 PM ***** LOSS printing *****
06/27 07:14:47 PM loss
06/27 07:14:47 PM tensor(2.9471, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:47 PM ***** Running evaluation MLM *****
06/27 07:14:47 PM   Epoch = 3 iter 44 step
06/27 07:14:47 PM   Num examples = 16
06/27 07:14:47 PM   Batch size = 32
06/27 07:14:48 PM ***** Eval results *****
06/27 07:14:48 PM   acc = 0.8125
06/27 07:14:48 PM   cls_loss = 2.5624583959579468
06/27 07:14:48 PM   eval_loss = 1.2592202425003052
06/27 07:14:48 PM   global_step = 44
06/27 07:14:48 PM   loss = 2.5624583959579468
06/27 07:14:48 PM ***** LOSS printing *****
06/27 07:14:48 PM loss
06/27 07:14:48 PM tensor(0.8654, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:48 PM ***** LOSS printing *****
06/27 07:14:48 PM loss
06/27 07:14:48 PM tensor(1.1015, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:48 PM ***** LOSS printing *****
06/27 07:14:48 PM loss
06/27 07:14:48 PM tensor(1.4759, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:48 PM ***** LOSS printing *****
06/27 07:14:48 PM loss
06/27 07:14:48 PM tensor(1.9299, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:49 PM ***** LOSS printing *****
06/27 07:14:49 PM loss
06/27 07:14:49 PM tensor(1.4047, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:49 PM ***** Running evaluation MLM *****
06/27 07:14:49 PM   Epoch = 4 iter 49 step
06/27 07:14:49 PM   Num examples = 16
06/27 07:14:49 PM   Batch size = 32
06/27 07:14:49 PM ***** Eval results *****
06/27 07:14:49 PM   acc = 0.75
06/27 07:14:49 PM   cls_loss = 1.4047315120697021
06/27 07:14:49 PM   eval_loss = 1.797761082649231
06/27 07:14:49 PM   global_step = 49
06/27 07:14:49 PM   loss = 1.4047315120697021
06/27 07:14:49 PM ***** LOSS printing *****
06/27 07:14:49 PM loss
06/27 07:14:49 PM tensor(1.3429, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:50 PM ***** LOSS printing *****
06/27 07:14:50 PM loss
06/27 07:14:50 PM tensor(1.7557, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:50 PM ***** LOSS printing *****
06/27 07:14:50 PM loss
06/27 07:14:50 PM tensor(1.4640, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:50 PM ***** LOSS printing *****
06/27 07:14:50 PM loss
06/27 07:14:50 PM tensor(1.1442, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:50 PM ***** LOSS printing *****
06/27 07:14:50 PM loss
06/27 07:14:50 PM tensor(1.3826, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:51 PM ***** Running evaluation MLM *****
06/27 07:14:51 PM   Epoch = 4 iter 54 step
06/27 07:14:51 PM   Num examples = 16
06/27 07:14:51 PM   Batch size = 32
06/27 07:14:51 PM ***** Eval results *****
06/27 07:14:51 PM   acc = 0.875
06/27 07:14:51 PM   cls_loss = 1.4156939188639324
06/27 07:14:51 PM   eval_loss = 2.1862435340881348
06/27 07:14:51 PM   global_step = 54
06/27 07:14:51 PM   loss = 1.4156939188639324
06/27 07:14:51 PM ***** LOSS printing *****
06/27 07:14:51 PM loss
06/27 07:14:51 PM tensor(1.5440, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:51 PM ***** LOSS printing *****
06/27 07:14:51 PM loss
06/27 07:14:51 PM tensor(1.4321, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:52 PM ***** LOSS printing *****
06/27 07:14:52 PM loss
06/27 07:14:52 PM tensor(1.5966, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:52 PM ***** LOSS printing *****
06/27 07:14:52 PM loss
06/27 07:14:52 PM tensor(1.4510, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:52 PM ***** LOSS printing *****
06/27 07:14:52 PM loss
06/27 07:14:52 PM tensor(1.2757, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:52 PM ***** Running evaluation MLM *****
06/27 07:14:52 PM   Epoch = 4 iter 59 step
06/27 07:14:52 PM   Num examples = 16
06/27 07:14:52 PM   Batch size = 32
06/27 07:14:53 PM ***** Eval results *****
06/27 07:14:53 PM   acc = 0.875
06/27 07:14:53 PM   cls_loss = 1.4357855103232644
06/27 07:14:53 PM   eval_loss = 1.5071123838424683
06/27 07:14:53 PM   global_step = 59
06/27 07:14:53 PM   loss = 1.4357855103232644
06/27 07:14:53 PM ***** LOSS printing *****
06/27 07:14:53 PM loss
06/27 07:14:53 PM tensor(1.3660, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:53 PM ***** LOSS printing *****
06/27 07:14:53 PM loss
06/27 07:14:53 PM tensor(0.8313, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:53 PM ***** LOSS printing *****
06/27 07:14:53 PM loss
06/27 07:14:53 PM tensor(1.3023, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:53 PM ***** LOSS printing *****
06/27 07:14:53 PM loss
06/27 07:14:53 PM tensor(0.9910, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:54 PM ***** LOSS printing *****
06/27 07:14:54 PM loss
06/27 07:14:54 PM tensor(1.1983, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:14:54 PM ***** Running evaluation MLM *****
06/27 07:14:54 PM   Epoch = 5 iter 64 step
06/27 07:14:54 PM   Num examples = 16
06/27 07:14:54 PM   Batch size = 32
06/27 07:14:54 PM ***** Eval results *****
06/27 07:14:54 PM   acc = 0.9375
06/27 07:14:54 PM   cls_loss = 1.0807079672813416
06/27 07:14:54 PM   eval_loss = 1.1704723834991455
06/27 07:14:54 PM   global_step = 64
06/27 07:14:54 PM   loss = 1.0807079672813416
06/27 07:14:54 PM ***** Save model *****
06/27 07:14:54 PM ***** Test Dataset Eval Result *****
06/27 07:15:22 PM ***** Eval results *****
06/27 07:15:22 PM   acc = 0.9197247706422018
06/27 07:15:22 PM   cls_loss = 1.0807079672813416
06/27 07:15:22 PM   eval_loss = 1.3728919114385332
06/27 07:15:22 PM   global_step = 64
06/27 07:15:22 PM   loss = 1.0807079672813416
06/27 07:15:26 PM ***** LOSS printing *****
06/27 07:15:26 PM loss
06/27 07:15:26 PM tensor(1.6137, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:26 PM ***** LOSS printing *****
06/27 07:15:26 PM loss
06/27 07:15:26 PM tensor(1.3481, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:26 PM ***** LOSS printing *****
06/27 07:15:26 PM loss
06/27 07:15:26 PM tensor(1.3156, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:26 PM ***** LOSS printing *****
06/27 07:15:26 PM loss
06/27 07:15:26 PM tensor(1.4605, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:27 PM ***** LOSS printing *****
06/27 07:15:27 PM loss
06/27 07:15:27 PM tensor(1.2477, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:27 PM ***** Running evaluation MLM *****
06/27 07:15:27 PM   Epoch = 5 iter 69 step
06/27 07:15:27 PM   Num examples = 16
06/27 07:15:27 PM   Batch size = 32
06/27 07:15:27 PM ***** Eval results *****
06/27 07:15:27 PM   acc = 0.875
06/27 07:15:27 PM   cls_loss = 1.2565025620990329
06/27 07:15:27 PM   eval_loss = 1.9467964172363281
06/27 07:15:27 PM   global_step = 69
06/27 07:15:27 PM   loss = 1.2565025620990329
06/27 07:15:27 PM ***** LOSS printing *****
06/27 07:15:27 PM loss
06/27 07:15:27 PM tensor(1.1022, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:28 PM ***** LOSS printing *****
06/27 07:15:28 PM loss
06/27 07:15:28 PM tensor(1.3737, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:28 PM ***** LOSS printing *****
06/27 07:15:28 PM loss
06/27 07:15:28 PM tensor(1.4335, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:28 PM ***** LOSS printing *****
06/27 07:15:28 PM loss
06/27 07:15:28 PM tensor(1.1448, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:28 PM ***** LOSS printing *****
06/27 07:15:28 PM loss
06/27 07:15:28 PM tensor(0.9039, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:28 PM ***** Running evaluation MLM *****
06/27 07:15:28 PM   Epoch = 6 iter 74 step
06/27 07:15:28 PM   Num examples = 16
06/27 07:15:28 PM   Batch size = 32
06/27 07:15:29 PM ***** Eval results *****
06/27 07:15:29 PM   acc = 0.75
06/27 07:15:29 PM   cls_loss = 1.0243903696537018
06/27 07:15:29 PM   eval_loss = 2.634655237197876
06/27 07:15:29 PM   global_step = 74
06/27 07:15:29 PM   loss = 1.0243903696537018
06/27 07:15:29 PM ***** LOSS printing *****
06/27 07:15:29 PM loss
06/27 07:15:29 PM tensor(0.9527, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:29 PM ***** LOSS printing *****
06/27 07:15:29 PM loss
06/27 07:15:29 PM tensor(2.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:29 PM ***** LOSS printing *****
06/27 07:15:29 PM loss
06/27 07:15:29 PM tensor(1.8788, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:30 PM ***** LOSS printing *****
06/27 07:15:30 PM loss
06/27 07:15:30 PM tensor(2.6394, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:30 PM ***** LOSS printing *****
06/27 07:15:30 PM loss
06/27 07:15:30 PM tensor(1.4315, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:30 PM ***** Running evaluation MLM *****
06/27 07:15:30 PM   Epoch = 6 iter 79 step
06/27 07:15:30 PM   Num examples = 16
06/27 07:15:30 PM   Batch size = 32
06/27 07:15:31 PM ***** Eval results *****
06/27 07:15:31 PM   acc = 0.8125
06/27 07:15:31 PM   cls_loss = 1.57846633877073
06/27 07:15:31 PM   eval_loss = 2.492804765701294
06/27 07:15:31 PM   global_step = 79
06/27 07:15:31 PM   loss = 1.57846633877073
06/27 07:15:31 PM ***** LOSS printing *****
06/27 07:15:31 PM loss
06/27 07:15:31 PM tensor(1.2793, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:31 PM ***** LOSS printing *****
06/27 07:15:31 PM loss
06/27 07:15:31 PM tensor(1.7646, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:31 PM ***** LOSS printing *****
06/27 07:15:31 PM loss
06/27 07:15:31 PM tensor(0.8524, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:31 PM ***** LOSS printing *****
06/27 07:15:31 PM loss
06/27 07:15:31 PM tensor(1.1812, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:31 PM ***** LOSS printing *****
06/27 07:15:31 PM loss
06/27 07:15:31 PM tensor(1.3473, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:32 PM ***** Running evaluation MLM *****
06/27 07:15:32 PM   Epoch = 6 iter 84 step
06/27 07:15:32 PM   Num examples = 16
06/27 07:15:32 PM   Batch size = 32
06/27 07:15:32 PM ***** Eval results *****
06/27 07:15:32 PM   acc = 0.8125
06/27 07:15:32 PM   cls_loss = 1.4561634709437687
06/27 07:15:32 PM   eval_loss = 1.483454704284668
06/27 07:15:32 PM   global_step = 84
06/27 07:15:32 PM   loss = 1.4561634709437687
06/27 07:15:32 PM ***** LOSS printing *****
06/27 07:15:32 PM loss
06/27 07:15:32 PM tensor(1.1652, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:32 PM ***** LOSS printing *****
06/27 07:15:32 PM loss
06/27 07:15:32 PM tensor(1.0501, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:33 PM ***** LOSS printing *****
06/27 07:15:33 PM loss
06/27 07:15:33 PM tensor(1.3394, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:33 PM ***** LOSS printing *****
06/27 07:15:33 PM loss
06/27 07:15:33 PM tensor(1.5010, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:33 PM ***** LOSS printing *****
06/27 07:15:33 PM loss
06/27 07:15:33 PM tensor(1.9146, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:33 PM ***** Running evaluation MLM *****
06/27 07:15:33 PM   Epoch = 7 iter 89 step
06/27 07:15:33 PM   Num examples = 16
06/27 07:15:33 PM   Batch size = 32
06/27 07:15:34 PM ***** Eval results *****
06/27 07:15:34 PM   acc = 0.9375
06/27 07:15:34 PM   cls_loss = 1.3940489530563354
06/27 07:15:34 PM   eval_loss = 0.8194984793663025
06/27 07:15:34 PM   global_step = 89
06/27 07:15:34 PM   loss = 1.3940489530563354
06/27 07:15:34 PM ***** LOSS printing *****
06/27 07:15:34 PM loss
06/27 07:15:34 PM tensor(1.7196, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:34 PM ***** LOSS printing *****
06/27 07:15:34 PM loss
06/27 07:15:34 PM tensor(1.4399, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:34 PM ***** LOSS printing *****
06/27 07:15:34 PM loss
06/27 07:15:34 PM tensor(0.9125, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:34 PM ***** LOSS printing *****
06/27 07:15:34 PM loss
06/27 07:15:34 PM tensor(1.0216, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:35 PM ***** LOSS printing *****
06/27 07:15:35 PM loss
06/27 07:15:35 PM tensor(1.4568, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:35 PM ***** Running evaluation MLM *****
06/27 07:15:35 PM   Epoch = 7 iter 94 step
06/27 07:15:35 PM   Num examples = 16
06/27 07:15:35 PM   Batch size = 32
06/27 07:15:35 PM ***** Eval results *****
06/27 07:15:35 PM   acc = 0.8125
06/27 07:15:35 PM   cls_loss = 1.3520705938339233
06/27 07:15:35 PM   eval_loss = 1.6813340187072754
06/27 07:15:35 PM   global_step = 94
06/27 07:15:35 PM   loss = 1.3520705938339233
06/27 07:15:35 PM ***** LOSS printing *****
06/27 07:15:35 PM loss
06/27 07:15:35 PM tensor(2.3353, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:36 PM ***** LOSS printing *****
06/27 07:15:36 PM loss
06/27 07:15:36 PM tensor(1.5315, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:36 PM ***** LOSS printing *****
06/27 07:15:36 PM loss
06/27 07:15:36 PM tensor(1.5155, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:36 PM ***** LOSS printing *****
06/27 07:15:36 PM loss
06/27 07:15:36 PM tensor(1.5168, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:36 PM ***** LOSS printing *****
06/27 07:15:36 PM loss
06/27 07:15:36 PM tensor(1.3946, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:36 PM ***** Running evaluation MLM *****
06/27 07:15:36 PM   Epoch = 8 iter 99 step
06/27 07:15:36 PM   Num examples = 16
06/27 07:15:36 PM   Batch size = 32
06/27 07:15:37 PM ***** Eval results *****
06/27 07:15:37 PM   acc = 0.8125
06/27 07:15:37 PM   cls_loss = 1.4756478468577068
06/27 07:15:37 PM   eval_loss = 2.027892827987671
06/27 07:15:37 PM   global_step = 99
06/27 07:15:37 PM   loss = 1.4756478468577068
06/27 07:15:37 PM ***** LOSS printing *****
06/27 07:15:37 PM loss
06/27 07:15:37 PM tensor(1.0796, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:37 PM ***** LOSS printing *****
06/27 07:15:37 PM loss
06/27 07:15:37 PM tensor(1.2656, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:37 PM ***** LOSS printing *****
06/27 07:15:37 PM loss
06/27 07:15:37 PM tensor(1.5160, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:38 PM ***** LOSS printing *****
06/27 07:15:38 PM loss
06/27 07:15:38 PM tensor(1.2397, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:38 PM ***** LOSS printing *****
06/27 07:15:38 PM loss
06/27 07:15:38 PM tensor(1.1497, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:38 PM ***** Running evaluation MLM *****
06/27 07:15:38 PM   Epoch = 8 iter 104 step
06/27 07:15:38 PM   Num examples = 16
06/27 07:15:38 PM   Batch size = 32
06/27 07:15:39 PM ***** Eval results *****
06/27 07:15:39 PM   acc = 0.875
06/27 07:15:39 PM   cls_loss = 1.3346936106681824
06/27 07:15:39 PM   eval_loss = 1.6236683130264282
06/27 07:15:39 PM   global_step = 104
06/27 07:15:39 PM   loss = 1.3346936106681824
06/27 07:15:39 PM ***** LOSS printing *****
06/27 07:15:39 PM loss
06/27 07:15:39 PM tensor(1.0555, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:39 PM ***** LOSS printing *****
06/27 07:15:39 PM loss
06/27 07:15:39 PM tensor(1.5204, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:39 PM ***** LOSS printing *****
06/27 07:15:39 PM loss
06/27 07:15:39 PM tensor(1.4167, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:39 PM ***** LOSS printing *****
06/27 07:15:39 PM loss
06/27 07:15:39 PM tensor(1.4424, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:40 PM ***** LOSS printing *****
06/27 07:15:40 PM loss
06/27 07:15:40 PM tensor(1.0724, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:40 PM ***** Running evaluation MLM *****
06/27 07:15:40 PM   Epoch = 9 iter 109 step
06/27 07:15:40 PM   Num examples = 16
06/27 07:15:40 PM   Batch size = 32
06/27 07:15:40 PM ***** Eval results *****
06/27 07:15:40 PM   acc = 0.75
06/27 07:15:40 PM   cls_loss = 1.0723743438720703
06/27 07:15:40 PM   eval_loss = 2.117450714111328
06/27 07:15:40 PM   global_step = 109
06/27 07:15:40 PM   loss = 1.0723743438720703
06/27 07:15:40 PM ***** LOSS printing *****
06/27 07:15:40 PM loss
06/27 07:15:40 PM tensor(1.4908, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:40 PM ***** LOSS printing *****
06/27 07:15:40 PM loss
06/27 07:15:40 PM tensor(1.2445, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:41 PM ***** LOSS printing *****
06/27 07:15:41 PM loss
06/27 07:15:41 PM tensor(1.2012, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:41 PM ***** LOSS printing *****
06/27 07:15:41 PM loss
06/27 07:15:41 PM tensor(1.1706, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:41 PM ***** LOSS printing *****
06/27 07:15:41 PM loss
06/27 07:15:41 PM tensor(2.1165, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:41 PM ***** Running evaluation MLM *****
06/27 07:15:41 PM   Epoch = 9 iter 114 step
06/27 07:15:41 PM   Num examples = 16
06/27 07:15:41 PM   Batch size = 32
06/27 07:15:42 PM ***** Eval results *****
06/27 07:15:42 PM   acc = 0.8125
06/27 07:15:42 PM   cls_loss = 1.3826664288838704
06/27 07:15:42 PM   eval_loss = 1.9233726263046265
06/27 07:15:42 PM   global_step = 114
06/27 07:15:42 PM   loss = 1.3826664288838704
06/27 07:15:42 PM ***** LOSS printing *****
06/27 07:15:42 PM loss
06/27 07:15:42 PM tensor(1.5847, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:42 PM ***** LOSS printing *****
06/27 07:15:42 PM loss
06/27 07:15:42 PM tensor(1.0695, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:42 PM ***** LOSS printing *****
06/27 07:15:42 PM loss
06/27 07:15:42 PM tensor(1.3540, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:43 PM ***** LOSS printing *****
06/27 07:15:43 PM loss
06/27 07:15:43 PM tensor(1.3336, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:43 PM ***** LOSS printing *****
06/27 07:15:43 PM loss
06/27 07:15:43 PM tensor(1.1823, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:15:43 PM ***** Running evaluation MLM *****
06/27 07:15:43 PM   Epoch = 9 iter 119 step
06/27 07:15:43 PM   Num examples = 16
06/27 07:15:43 PM   Batch size = 32
06/27 07:15:43 PM ***** Eval results *****
06/27 07:15:43 PM   acc = 0.75
06/27 07:15:43 PM   cls_loss = 1.3472743792967363
06/27 07:15:43 PM   eval_loss = 2.0577597618103027
06/27 07:15:43 PM   global_step = 119
06/27 07:15:43 PM   loss = 1.3472743792967363
06/27 07:15:43 PM ***** LOSS printing *****
06/27 07:15:43 PM loss
06/27 07:15:43 PM tensor(1.2150, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 07:45:24 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/mr/8-100/', data_seed=100, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='mr', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 07:45:24 PM device: cuda n_gpu: 1
06/27 07:45:24 PM Writing example 0 of 48
06/27 07:45:24 PM *** Example ***
06/27 07:45:24 PM guid: train-1
06/27 07:45:24 PM tokens: <s> supp osed ly Ġbased Ġupon Ġreal Ġ, Ġor Ġat Ġleast Ġsober ly Ġreported Ġincidents Ġ, Ġthe Ġfilm Ġends Ġwith Ġa Ġlarge Ġhuman Ġtragedy Ġ. Ġalas Ġ, Ġgetting Ġthere Ġis Ġnot Ġeven Ġhalf Ġthe Ġinterest Ġ. </s> ĠIt Ġis <mask>
06/27 07:45:24 PM input_ids: 0 16714 7878 352 716 2115 588 2156 50 23 513 17333 352 431 4495 2156 5 822 3587 19 10 739 1050 6906 479 40463 2156 562 89 16 45 190 457 5 773 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM label: ['Ġnegative']
06/27 07:45:24 PM Writing example 0 of 16
06/27 07:45:24 PM *** Example ***
06/27 07:45:24 PM guid: dev-1
06/27 07:45:24 PM tokens: <s> although Ġbased Ġon Ġa Ġreal - life Ġperson Ġ, Ġjohn Ġ, Ġin Ġthe Ġmovie Ġ, Ġis Ġa Ġrather Ġdull Ġperson Ġto Ġbe Ġstuck Ġwith Ġfor Ġtwo Ġhours Ġ. </s> ĠIt Ġis <mask>
06/27 07:45:24 PM input_ids: 0 24648 716 15 10 588 12 5367 621 2156 41906 2156 11 5 1569 2156 16 10 1195 22018 621 7 28 4889 19 13 80 722 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM label: ['Ġnegative']
06/27 07:45:24 PM Writing example 0 of 2000
06/27 07:45:24 PM *** Example ***
06/27 07:45:24 PM guid: dev-1
06/27 07:45:24 PM tokens: <s> sim pl istic Ġ, Ġsilly Ġand Ġtedious Ġ. </s> ĠIt Ġis <mask>
06/27 07:45:24 PM input_ids: 0 13092 2911 5580 2156 15470 8 35138 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:45:24 PM label: ['Ġnegative']
06/27 07:45:38 PM ***** Running training *****
06/27 07:45:38 PM   Num examples = 48
06/27 07:45:38 PM   Batch size = 4
06/27 07:45:38 PM   Num steps = 120
06/27 07:45:38 PM n: embeddings.word_embeddings.weight
06/27 07:45:38 PM n: embeddings.position_embeddings.weight
06/27 07:45:38 PM n: embeddings.token_type_embeddings.weight
06/27 07:45:38 PM n: embeddings.LayerNorm.weight
06/27 07:45:38 PM n: embeddings.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.0.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.0.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.0.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.0.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.0.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.0.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.0.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.0.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.0.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.0.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.0.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.0.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.1.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.1.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.1.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.1.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.1.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.1.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.1.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.1.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.1.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.1.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.1.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.1.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.2.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.2.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.2.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.2.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.2.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.2.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.2.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.2.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.2.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.2.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.2.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.2.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.3.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.3.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.3.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.3.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.3.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.3.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.3.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.3.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.3.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.3.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.3.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.3.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.4.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.4.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.4.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.4.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.4.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.4.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.4.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.4.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.4.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.4.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.4.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.4.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.5.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.5.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.5.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.5.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.5.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.5.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.5.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.5.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.5.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.5.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.5.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.5.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.6.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.6.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.6.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.6.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.6.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.6.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.6.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.6.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.6.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.6.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.6.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.6.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.7.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.7.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.7.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.7.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.7.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.7.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.7.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.7.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.7.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.7.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.7.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.7.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.8.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.8.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.8.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.8.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.8.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.8.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.8.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.8.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.8.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.8.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.8.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.8.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.9.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.9.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.9.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.9.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.9.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.9.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.9.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.9.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.9.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.9.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.9.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.9.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.10.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.10.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.10.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.10.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.10.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.10.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.10.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.10.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.10.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.10.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.10.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.10.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.11.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.11.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.11.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.11.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.11.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.11.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.11.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.11.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.11.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.11.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.11.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.11.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.12.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.12.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.12.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.12.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.12.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.12.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.12.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.12.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.12.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.12.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.12.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.12.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.13.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.13.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.13.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.13.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.13.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.13.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.13.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.13.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.13.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.13.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.13.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.13.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.14.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.14.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.14.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.14.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.14.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.14.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.14.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.14.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.14.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.14.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.14.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.14.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.15.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.15.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.15.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.15.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.15.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.15.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.15.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.15.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.15.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.15.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.15.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.15.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.16.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.16.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.16.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.16.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.16.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.16.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.16.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.16.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.16.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.16.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.16.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.16.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.17.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.17.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.17.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.17.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.17.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.17.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.17.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.17.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.17.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.17.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.17.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.17.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.18.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.18.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.18.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.18.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.18.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.18.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.18.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.18.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.18.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.18.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.18.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.18.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.19.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.19.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.19.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.19.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.19.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.19.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.19.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.19.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.19.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.19.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.19.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.19.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.20.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.20.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.20.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.20.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.20.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.20.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.20.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.20.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.20.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.20.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.20.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.20.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.21.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.21.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.21.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.21.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.21.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.21.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.21.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.21.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.21.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.21.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.21.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.21.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.22.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.22.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.22.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.22.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.22.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.22.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.22.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.22.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.22.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.22.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.22.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.22.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.23.attention.self.query.weight
06/27 07:45:38 PM n: encoder.layer.23.attention.self.query.bias
06/27 07:45:38 PM n: encoder.layer.23.attention.self.key.weight
06/27 07:45:38 PM n: encoder.layer.23.attention.self.key.bias
06/27 07:45:38 PM n: encoder.layer.23.attention.self.value.weight
06/27 07:45:38 PM n: encoder.layer.23.attention.self.value.bias
06/27 07:45:38 PM n: encoder.layer.23.attention.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.23.attention.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: encoder.layer.23.intermediate.dense.weight
06/27 07:45:38 PM n: encoder.layer.23.intermediate.dense.bias
06/27 07:45:38 PM n: encoder.layer.23.output.dense.weight
06/27 07:45:38 PM n: encoder.layer.23.output.dense.bias
06/27 07:45:38 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 07:45:38 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 07:45:38 PM n: pooler.dense.weight
06/27 07:45:38 PM n: pooler.dense.bias
06/27 07:45:38 PM n: roberta.embeddings.word_embeddings.weight
06/27 07:45:38 PM n: roberta.embeddings.position_embeddings.weight
06/27 07:45:38 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 07:45:38 PM n: roberta.embeddings.LayerNorm.weight
06/27 07:45:38 PM n: roberta.embeddings.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 07:45:38 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 07:45:38 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 07:45:38 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 07:45:38 PM n: roberta.pooler.dense.weight
06/27 07:45:38 PM n: roberta.pooler.dense.bias
06/27 07:45:38 PM n: lm_head.bias
06/27 07:45:38 PM n: lm_head.dense.weight
06/27 07:45:38 PM n: lm_head.dense.bias
06/27 07:45:38 PM n: lm_head.layer_norm.weight
06/27 07:45:38 PM n: lm_head.layer_norm.bias
06/27 07:45:38 PM n: lm_head.decoder.weight
06/27 07:45:38 PM Total parameters: 763292761
06/27 07:45:38 PM ***** LOSS printing *****
06/27 07:45:38 PM loss
06/27 07:45:38 PM tensor(18.0512, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:45:38 PM ***** LOSS printing *****
06/27 07:45:38 PM loss
06/27 07:45:38 PM tensor(10.8218, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:45:38 PM ***** LOSS printing *****
06/27 07:45:38 PM loss
06/27 07:45:38 PM tensor(6.9309, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:45:39 PM ***** LOSS printing *****
06/27 07:45:39 PM loss
06/27 07:45:39 PM tensor(4.4201, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:45:39 PM ***** Running evaluation MLM *****
06/27 07:45:39 PM   Epoch = 0 iter 4 step
06/27 07:45:39 PM   Num examples = 16
06/27 07:45:39 PM   Batch size = 32
06/27 07:45:39 PM ***** Eval results *****
06/27 07:45:39 PM   acc = 0.8125
06/27 07:45:39 PM   cls_loss = 10.05599558353424
06/27 07:45:39 PM   eval_loss = 5.473066329956055
06/27 07:45:39 PM   global_step = 4
06/27 07:45:39 PM   loss = 10.05599558353424
06/27 07:45:39 PM ***** Save model *****
06/27 07:45:39 PM ***** Test Dataset Eval Result *****
06/27 07:46:42 PM ***** Eval results *****
06/27 07:46:42 PM   acc = 0.7225
06/27 07:46:42 PM   cls_loss = 10.05599558353424
06/27 07:46:42 PM   eval_loss = 5.452789200676812
06/27 07:46:42 PM   global_step = 4
06/27 07:46:42 PM   loss = 10.05599558353424
06/27 07:46:46 PM ***** LOSS printing *****
06/27 07:46:46 PM loss
06/27 07:46:46 PM tensor(2.2626, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:47 PM ***** LOSS printing *****
06/27 07:46:47 PM loss
06/27 07:46:47 PM tensor(2.6218, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:47 PM ***** LOSS printing *****
06/27 07:46:47 PM loss
06/27 07:46:47 PM tensor(2.5457, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:47 PM ***** LOSS printing *****
06/27 07:46:47 PM loss
06/27 07:46:47 PM tensor(2.2584, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:47 PM ***** LOSS printing *****
06/27 07:46:47 PM loss
06/27 07:46:47 PM tensor(3.1732, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:47 PM ***** Running evaluation MLM *****
06/27 07:46:47 PM   Epoch = 0 iter 9 step
06/27 07:46:47 PM   Num examples = 16
06/27 07:46:47 PM   Batch size = 32
06/27 07:46:48 PM ***** Eval results *****
06/27 07:46:48 PM   acc = 0.5
06/27 07:46:48 PM   cls_loss = 5.898414876725939
06/27 07:46:48 PM   eval_loss = 0.8051953911781311
06/27 07:46:48 PM   global_step = 9
06/27 07:46:48 PM   loss = 5.898414876725939
06/27 07:46:48 PM ***** LOSS printing *****
06/27 07:46:48 PM loss
06/27 07:46:48 PM tensor(2.7771, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:48 PM ***** LOSS printing *****
06/27 07:46:48 PM loss
06/27 07:46:48 PM tensor(4.6945, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:48 PM ***** LOSS printing *****
06/27 07:46:48 PM loss
06/27 07:46:48 PM tensor(5.7200, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:49 PM ***** LOSS printing *****
06/27 07:46:49 PM loss
06/27 07:46:49 PM tensor(2.0749, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:49 PM ***** LOSS printing *****
06/27 07:46:49 PM loss
06/27 07:46:49 PM tensor(3.8381, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:49 PM ***** Running evaluation MLM *****
06/27 07:46:49 PM   Epoch = 1 iter 14 step
06/27 07:46:49 PM   Num examples = 16
06/27 07:46:49 PM   Batch size = 32
06/27 07:46:50 PM ***** Eval results *****
06/27 07:46:50 PM   acc = 0.5
06/27 07:46:50 PM   cls_loss = 2.956498146057129
06/27 07:46:50 PM   eval_loss = 3.7393009662628174
06/27 07:46:50 PM   global_step = 14
06/27 07:46:50 PM   loss = 2.956498146057129
06/27 07:46:50 PM ***** LOSS printing *****
06/27 07:46:50 PM loss
06/27 07:46:50 PM tensor(3.4636, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:50 PM ***** LOSS printing *****
06/27 07:46:50 PM loss
06/27 07:46:50 PM tensor(2.4468, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:50 PM ***** LOSS printing *****
06/27 07:46:50 PM loss
06/27 07:46:50 PM tensor(2.1242, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:50 PM ***** LOSS printing *****
06/27 07:46:50 PM loss
06/27 07:46:50 PM tensor(1.7089, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:50 PM ***** LOSS printing *****
06/27 07:46:50 PM loss
06/27 07:46:50 PM tensor(2.5584, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:51 PM ***** Running evaluation MLM *****
06/27 07:46:51 PM   Epoch = 1 iter 19 step
06/27 07:46:51 PM   Num examples = 16
06/27 07:46:51 PM   Batch size = 32
06/27 07:46:51 PM ***** Eval results *****
06/27 07:46:51 PM   acc = 0.5
06/27 07:46:51 PM   cls_loss = 2.6021252359662737
06/27 07:46:51 PM   eval_loss = 1.8448216915130615
06/27 07:46:51 PM   global_step = 19
06/27 07:46:51 PM   loss = 2.6021252359662737
06/27 07:46:51 PM ***** LOSS printing *****
06/27 07:46:51 PM loss
06/27 07:46:51 PM tensor(2.4190, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:51 PM ***** LOSS printing *****
06/27 07:46:51 PM loss
06/27 07:46:51 PM tensor(2.1117, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:52 PM ***** LOSS printing *****
06/27 07:46:52 PM loss
06/27 07:46:52 PM tensor(2.3056, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:52 PM ***** LOSS printing *****
06/27 07:46:52 PM loss
06/27 07:46:52 PM tensor(1.9597, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:52 PM ***** LOSS printing *****
06/27 07:46:52 PM loss
06/27 07:46:52 PM tensor(2.6376, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:46:52 PM ***** Running evaluation MLM *****
06/27 07:46:52 PM   Epoch = 1 iter 24 step
06/27 07:46:52 PM   Num examples = 16
06/27 07:46:52 PM   Batch size = 32
06/27 07:46:53 PM ***** Eval results *****
06/27 07:46:53 PM   acc = 0.9375
06/27 07:46:53 PM   cls_loss = 2.4707112510999045
06/27 07:46:53 PM   eval_loss = 1.16206693649292
06/27 07:46:53 PM   global_step = 24
06/27 07:46:53 PM   loss = 2.4707112510999045
06/27 07:46:53 PM ***** Save model *****
06/27 07:46:53 PM ***** Test Dataset Eval Result *****
06/27 07:47:55 PM ***** Eval results *****
06/27 07:47:55 PM   acc = 0.809
06/27 07:47:55 PM   cls_loss = 2.4707112510999045
06/27 07:47:55 PM   eval_loss = 1.3506386544969347
06/27 07:47:55 PM   global_step = 24
06/27 07:47:55 PM   loss = 2.4707112510999045
06/27 07:48:00 PM ***** LOSS printing *****
06/27 07:48:00 PM loss
06/27 07:48:00 PM tensor(1.9721, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:00 PM ***** LOSS printing *****
06/27 07:48:00 PM loss
06/27 07:48:00 PM tensor(0.9019, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:00 PM ***** LOSS printing *****
06/27 07:48:00 PM loss
06/27 07:48:00 PM tensor(0.8825, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:00 PM ***** LOSS printing *****
06/27 07:48:00 PM loss
06/27 07:48:00 PM tensor(1.9294, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:01 PM ***** LOSS printing *****
06/27 07:48:01 PM loss
06/27 07:48:01 PM tensor(1.6887, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:01 PM ***** Running evaluation MLM *****
06/27 07:48:01 PM   Epoch = 2 iter 29 step
06/27 07:48:01 PM   Num examples = 16
06/27 07:48:01 PM   Batch size = 32
06/27 07:48:01 PM ***** Eval results *****
06/27 07:48:01 PM   acc = 0.9375
06/27 07:48:01 PM   cls_loss = 1.4749387145042419
06/27 07:48:01 PM   eval_loss = 2.408982515335083
06/27 07:48:01 PM   global_step = 29
06/27 07:48:01 PM   loss = 1.4749387145042419
06/27 07:48:01 PM ***** LOSS printing *****
06/27 07:48:01 PM loss
06/27 07:48:01 PM tensor(1.3695, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:02 PM ***** LOSS printing *****
06/27 07:48:02 PM loss
06/27 07:48:02 PM tensor(1.6437, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:02 PM ***** LOSS printing *****
06/27 07:48:02 PM loss
06/27 07:48:02 PM tensor(1.6079, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:02 PM ***** LOSS printing *****
06/27 07:48:02 PM loss
06/27 07:48:02 PM tensor(1.3388, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:02 PM ***** LOSS printing *****
06/27 07:48:02 PM loss
06/27 07:48:02 PM tensor(1.3608, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:48:02 PM ***** Running evaluation MLM *****
06/27 07:48:02 PM   Epoch = 2 iter 34 step
06/27 07:48:02 PM   Num examples = 16
06/27 07:48:02 PM   Batch size = 32
06/27 07:48:03 PM ***** Eval results *****
06/27 07:48:03 PM   acc = 1.0
06/27 07:48:03 PM   cls_loss = 1.469543570280075
06/27 07:48:03 PM   eval_loss = 0.4109269082546234
06/27 07:48:03 PM   global_step = 34
06/27 07:48:03 PM   loss = 1.469543570280075
06/27 07:48:03 PM ***** Save model *****
06/27 07:48:03 PM ***** Test Dataset Eval Result *****
06/27 07:49:06 PM ***** Eval results *****
06/27 07:49:06 PM   acc = 0.8255
06/27 07:49:06 PM   cls_loss = 1.469543570280075
06/27 07:49:06 PM   eval_loss = 1.026038598446619
06/27 07:49:06 PM   global_step = 34
06/27 07:49:06 PM   loss = 1.469543570280075
06/27 07:49:10 PM ***** LOSS printing *****
06/27 07:49:10 PM loss
06/27 07:49:10 PM tensor(2.2759, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:10 PM ***** LOSS printing *****
06/27 07:49:10 PM loss
06/27 07:49:10 PM tensor(1.7641, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:10 PM ***** LOSS printing *****
06/27 07:49:10 PM loss
06/27 07:49:10 PM tensor(1.6423, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:10 PM ***** LOSS printing *****
06/27 07:49:10 PM loss
06/27 07:49:10 PM tensor(1.3076, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:11 PM ***** LOSS printing *****
06/27 07:49:11 PM loss
06/27 07:49:11 PM tensor(0.7124, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:11 PM ***** Running evaluation MLM *****
06/27 07:49:11 PM   Epoch = 3 iter 39 step
06/27 07:49:11 PM   Num examples = 16
06/27 07:49:11 PM   Batch size = 32
06/27 07:49:11 PM ***** Eval results *****
06/27 07:49:11 PM   acc = 1.0
06/27 07:49:11 PM   cls_loss = 1.2207655906677246
06/27 07:49:11 PM   eval_loss = 1.108333706855774
06/27 07:49:11 PM   global_step = 39
06/27 07:49:11 PM   loss = 1.2207655906677246
06/27 07:49:11 PM ***** LOSS printing *****
06/27 07:49:11 PM loss
06/27 07:49:11 PM tensor(1.8947, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:12 PM ***** LOSS printing *****
06/27 07:49:12 PM loss
06/27 07:49:12 PM tensor(1.2745, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:12 PM ***** LOSS printing *****
06/27 07:49:12 PM loss
06/27 07:49:12 PM tensor(0.9515, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:12 PM ***** LOSS printing *****
06/27 07:49:12 PM loss
06/27 07:49:12 PM tensor(2.0769, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:12 PM ***** LOSS printing *****
06/27 07:49:12 PM loss
06/27 07:49:12 PM tensor(1.1145, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:12 PM ***** Running evaluation MLM *****
06/27 07:49:12 PM   Epoch = 3 iter 44 step
06/27 07:49:12 PM   Num examples = 16
06/27 07:49:12 PM   Batch size = 32
06/27 07:49:13 PM ***** Eval results *****
06/27 07:49:13 PM   acc = 1.0
06/27 07:49:13 PM   cls_loss = 1.3718147277832031
06/27 07:49:13 PM   eval_loss = 1.6343557834625244
06/27 07:49:13 PM   global_step = 44
06/27 07:49:13 PM   loss = 1.3718147277832031
06/27 07:49:13 PM ***** LOSS printing *****
06/27 07:49:13 PM loss
06/27 07:49:13 PM tensor(1.0328, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:13 PM ***** LOSS printing *****
06/27 07:49:13 PM loss
06/27 07:49:13 PM tensor(1.5950, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:13 PM ***** LOSS printing *****
06/27 07:49:13 PM loss
06/27 07:49:13 PM tensor(1.7935, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:14 PM ***** LOSS printing *****
06/27 07:49:14 PM loss
06/27 07:49:14 PM tensor(2.0610, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:14 PM ***** LOSS printing *****
06/27 07:49:14 PM loss
06/27 07:49:14 PM tensor(1.0581, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:14 PM ***** Running evaluation MLM *****
06/27 07:49:14 PM   Epoch = 4 iter 49 step
06/27 07:49:14 PM   Num examples = 16
06/27 07:49:14 PM   Batch size = 32
06/27 07:49:15 PM ***** Eval results *****
06/27 07:49:15 PM   acc = 1.0
06/27 07:49:15 PM   cls_loss = 1.0580990314483643
06/27 07:49:15 PM   eval_loss = 1.6214042901992798
06/27 07:49:15 PM   global_step = 49
06/27 07:49:15 PM   loss = 1.0580990314483643
06/27 07:49:15 PM ***** LOSS printing *****
06/27 07:49:15 PM loss
06/27 07:49:15 PM tensor(1.3495, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:15 PM ***** LOSS printing *****
06/27 07:49:15 PM loss
06/27 07:49:15 PM tensor(1.2486, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:15 PM ***** LOSS printing *****
06/27 07:49:15 PM loss
06/27 07:49:15 PM tensor(1.3784, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:15 PM ***** LOSS printing *****
06/27 07:49:15 PM loss
06/27 07:49:15 PM tensor(1.1855, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:16 PM ***** LOSS printing *****
06/27 07:49:16 PM loss
06/27 07:49:16 PM tensor(1.2295, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:16 PM ***** Running evaluation MLM *****
06/27 07:49:16 PM   Epoch = 4 iter 54 step
06/27 07:49:16 PM   Num examples = 16
06/27 07:49:16 PM   Batch size = 32
06/27 07:49:16 PM ***** Eval results *****
06/27 07:49:16 PM   acc = 1.0
06/27 07:49:16 PM   cls_loss = 1.2415860096613567
06/27 07:49:16 PM   eval_loss = 0.5703105330467224
06/27 07:49:16 PM   global_step = 54
06/27 07:49:16 PM   loss = 1.2415860096613567
06/27 07:49:16 PM ***** LOSS printing *****
06/27 07:49:16 PM loss
06/27 07:49:16 PM tensor(0.6753, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:16 PM ***** LOSS printing *****
06/27 07:49:16 PM loss
06/27 07:49:16 PM tensor(2.0892, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:17 PM ***** LOSS printing *****
06/27 07:49:17 PM loss
06/27 07:49:17 PM tensor(1.8278, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:17 PM ***** LOSS printing *****
06/27 07:49:17 PM loss
06/27 07:49:17 PM tensor(1.3389, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:17 PM ***** LOSS printing *****
06/27 07:49:17 PM loss
06/27 07:49:17 PM tensor(1.5891, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:17 PM ***** Running evaluation MLM *****
06/27 07:49:17 PM   Epoch = 4 iter 59 step
06/27 07:49:17 PM   Num examples = 16
06/27 07:49:17 PM   Batch size = 32
06/27 07:49:18 PM ***** Eval results *****
06/27 07:49:18 PM   acc = 0.9375
06/27 07:49:18 PM   cls_loss = 1.3608947775580666
06/27 07:49:18 PM   eval_loss = 1.03035569190979
06/27 07:49:18 PM   global_step = 59
06/27 07:49:18 PM   loss = 1.3608947775580666
06/27 07:49:18 PM ***** LOSS printing *****
06/27 07:49:18 PM loss
06/27 07:49:18 PM tensor(1.4587, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:18 PM ***** LOSS printing *****
06/27 07:49:18 PM loss
06/27 07:49:18 PM tensor(1.2181, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:18 PM ***** LOSS printing *****
06/27 07:49:18 PM loss
06/27 07:49:18 PM tensor(1.4269, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:19 PM ***** LOSS printing *****
06/27 07:49:19 PM loss
06/27 07:49:19 PM tensor(1.2636, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:19 PM ***** LOSS printing *****
06/27 07:49:19 PM loss
06/27 07:49:19 PM tensor(1.6675, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:19 PM ***** Running evaluation MLM *****
06/27 07:49:19 PM   Epoch = 5 iter 64 step
06/27 07:49:19 PM   Num examples = 16
06/27 07:49:19 PM   Batch size = 32
06/27 07:49:19 PM ***** Eval results *****
06/27 07:49:19 PM   acc = 1.0
06/27 07:49:19 PM   cls_loss = 1.3940187692642212
06/27 07:49:19 PM   eval_loss = 1.8355669975280762
06/27 07:49:19 PM   global_step = 64
06/27 07:49:19 PM   loss = 1.3940187692642212
06/27 07:49:19 PM ***** LOSS printing *****
06/27 07:49:19 PM loss
06/27 07:49:19 PM tensor(1.1895, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:20 PM ***** LOSS printing *****
06/27 07:49:20 PM loss
06/27 07:49:20 PM tensor(1.1547, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:20 PM ***** LOSS printing *****
06/27 07:49:20 PM loss
06/27 07:49:20 PM tensor(1.0778, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:20 PM ***** LOSS printing *****
06/27 07:49:20 PM loss
06/27 07:49:20 PM tensor(0.8534, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:20 PM ***** LOSS printing *****
06/27 07:49:20 PM loss
06/27 07:49:20 PM tensor(1.0423, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:21 PM ***** Running evaluation MLM *****
06/27 07:49:21 PM   Epoch = 5 iter 69 step
06/27 07:49:21 PM   Num examples = 16
06/27 07:49:21 PM   Batch size = 32
06/27 07:49:21 PM ***** Eval results *****
06/27 07:49:21 PM   acc = 1.0
06/27 07:49:21 PM   cls_loss = 1.210408714082506
06/27 07:49:21 PM   eval_loss = 2.126864194869995
06/27 07:49:21 PM   global_step = 69
06/27 07:49:21 PM   loss = 1.210408714082506
06/27 07:49:21 PM ***** LOSS printing *****
06/27 07:49:21 PM loss
06/27 07:49:21 PM tensor(2.0979, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:21 PM ***** LOSS printing *****
06/27 07:49:21 PM loss
06/27 07:49:21 PM tensor(1.4290, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:22 PM ***** LOSS printing *****
06/27 07:49:22 PM loss
06/27 07:49:22 PM tensor(1.0147, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:22 PM ***** LOSS printing *****
06/27 07:49:22 PM loss
06/27 07:49:22 PM tensor(1.0495, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:22 PM ***** LOSS printing *****
06/27 07:49:22 PM loss
06/27 07:49:22 PM tensor(0.8638, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:22 PM ***** Running evaluation MLM *****
06/27 07:49:22 PM   Epoch = 6 iter 74 step
06/27 07:49:22 PM   Num examples = 16
06/27 07:49:22 PM   Batch size = 32
06/27 07:49:23 PM ***** Eval results *****
06/27 07:49:23 PM   acc = 1.0
06/27 07:49:23 PM   cls_loss = 0.9566605687141418
06/27 07:49:23 PM   eval_loss = 1.2326358556747437
06/27 07:49:23 PM   global_step = 74
06/27 07:49:23 PM   loss = 0.9566605687141418
06/27 07:49:23 PM ***** LOSS printing *****
06/27 07:49:23 PM loss
06/27 07:49:23 PM tensor(1.3216, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:23 PM ***** LOSS printing *****
06/27 07:49:23 PM loss
06/27 07:49:23 PM tensor(1.2725, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:23 PM ***** LOSS printing *****
06/27 07:49:23 PM loss
06/27 07:49:23 PM tensor(1.5640, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:23 PM ***** LOSS printing *****
06/27 07:49:23 PM loss
06/27 07:49:23 PM tensor(1.5132, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:24 PM ***** LOSS printing *****
06/27 07:49:24 PM loss
06/27 07:49:24 PM tensor(1.3080, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:24 PM ***** Running evaluation MLM *****
06/27 07:49:24 PM   Epoch = 6 iter 79 step
06/27 07:49:24 PM   Num examples = 16
06/27 07:49:24 PM   Batch size = 32
06/27 07:49:24 PM ***** Eval results *****
06/27 07:49:24 PM   acc = 0.9375
06/27 07:49:24 PM   cls_loss = 1.270380973815918
06/27 07:49:24 PM   eval_loss = 0.8795364499092102
06/27 07:49:24 PM   global_step = 79
06/27 07:49:24 PM   loss = 1.270380973815918
06/27 07:49:24 PM ***** LOSS printing *****
06/27 07:49:24 PM loss
06/27 07:49:24 PM tensor(2.0293, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:25 PM ***** LOSS printing *****
06/27 07:49:25 PM loss
06/27 07:49:25 PM tensor(1.2382, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:25 PM ***** LOSS printing *****
06/27 07:49:25 PM loss
06/27 07:49:25 PM tensor(1.3264, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:25 PM ***** LOSS printing *****
06/27 07:49:25 PM loss
06/27 07:49:25 PM tensor(0.9516, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:25 PM ***** LOSS printing *****
06/27 07:49:25 PM loss
06/27 07:49:25 PM tensor(1.6713, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:25 PM ***** Running evaluation MLM *****
06/27 07:49:25 PM   Epoch = 6 iter 84 step
06/27 07:49:25 PM   Num examples = 16
06/27 07:49:25 PM   Batch size = 32
06/27 07:49:26 PM ***** Eval results *****
06/27 07:49:26 PM   acc = 1.0
06/27 07:49:26 PM   cls_loss = 1.3424538572629292
06/27 07:49:26 PM   eval_loss = 0.7332603931427002
06/27 07:49:26 PM   global_step = 84
06/27 07:49:26 PM   loss = 1.3424538572629292
06/27 07:49:26 PM ***** LOSS printing *****
06/27 07:49:26 PM loss
06/27 07:49:26 PM tensor(0.9906, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:26 PM ***** LOSS printing *****
06/27 07:49:26 PM loss
06/27 07:49:26 PM tensor(1.0413, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:26 PM ***** LOSS printing *****
06/27 07:49:26 PM loss
06/27 07:49:26 PM tensor(1.1394, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:27 PM ***** LOSS printing *****
06/27 07:49:27 PM loss
06/27 07:49:27 PM tensor(1.5461, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:27 PM ***** LOSS printing *****
06/27 07:49:27 PM loss
06/27 07:49:27 PM tensor(1.4817, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:27 PM ***** Running evaluation MLM *****
06/27 07:49:27 PM   Epoch = 7 iter 89 step
06/27 07:49:27 PM   Num examples = 16
06/27 07:49:27 PM   Batch size = 32
06/27 07:49:28 PM ***** Eval results *****
06/27 07:49:28 PM   acc = 1.0
06/27 07:49:28 PM   cls_loss = 1.2398211717605592
06/27 07:49:28 PM   eval_loss = 0.6267677545547485
06/27 07:49:28 PM   global_step = 89
06/27 07:49:28 PM   loss = 1.2398211717605592
06/27 07:49:28 PM ***** LOSS printing *****
06/27 07:49:28 PM loss
06/27 07:49:28 PM tensor(1.3940, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:28 PM ***** LOSS printing *****
06/27 07:49:28 PM loss
06/27 07:49:28 PM tensor(1.2963, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:28 PM ***** LOSS printing *****
06/27 07:49:28 PM loss
06/27 07:49:28 PM tensor(1.3793, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:28 PM ***** LOSS printing *****
06/27 07:49:28 PM loss
06/27 07:49:28 PM tensor(1.1063, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:28 PM ***** LOSS printing *****
06/27 07:49:28 PM loss
06/27 07:49:28 PM tensor(1.2990, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:29 PM ***** Running evaluation MLM *****
06/27 07:49:29 PM   Epoch = 7 iter 94 step
06/27 07:49:29 PM   Num examples = 16
06/27 07:49:29 PM   Batch size = 32
06/27 07:49:29 PM ***** Eval results *****
06/27 07:49:29 PM   acc = 0.9375
06/27 07:49:29 PM   cls_loss = 1.2674101710319519
06/27 07:49:29 PM   eval_loss = 1.3695122003555298
06/27 07:49:29 PM   global_step = 94
06/27 07:49:29 PM   loss = 1.2674101710319519
06/27 07:49:29 PM ***** LOSS printing *****
06/27 07:49:29 PM loss
06/27 07:49:29 PM tensor(1.4675, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:29 PM ***** LOSS printing *****
06/27 07:49:29 PM loss
06/27 07:49:29 PM tensor(1.1647, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:30 PM ***** LOSS printing *****
06/27 07:49:30 PM loss
06/27 07:49:30 PM tensor(1.3024, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:30 PM ***** LOSS printing *****
06/27 07:49:30 PM loss
06/27 07:49:30 PM tensor(1.2987, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:30 PM ***** LOSS printing *****
06/27 07:49:30 PM loss
06/27 07:49:30 PM tensor(1.4264, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:30 PM ***** Running evaluation MLM *****
06/27 07:49:30 PM   Epoch = 8 iter 99 step
06/27 07:49:30 PM   Num examples = 16
06/27 07:49:30 PM   Batch size = 32
06/27 07:49:31 PM ***** Eval results *****
06/27 07:49:31 PM   acc = 0.8125
06/27 07:49:31 PM   cls_loss = 1.3424692551294963
06/27 07:49:31 PM   eval_loss = 2.5962538719177246
06/27 07:49:31 PM   global_step = 99
06/27 07:49:31 PM   loss = 1.3424692551294963
06/27 07:49:31 PM ***** LOSS printing *****
06/27 07:49:31 PM loss
06/27 07:49:31 PM tensor(2.0073, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:31 PM ***** LOSS printing *****
06/27 07:49:31 PM loss
06/27 07:49:31 PM tensor(1.7376, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:31 PM ***** LOSS printing *****
06/27 07:49:31 PM loss
06/27 07:49:31 PM tensor(2.5892, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:31 PM ***** LOSS printing *****
06/27 07:49:31 PM loss
06/27 07:49:31 PM tensor(1.3793, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:32 PM ***** LOSS printing *****
06/27 07:49:32 PM loss
06/27 07:49:32 PM tensor(1.5870, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:32 PM ***** Running evaluation MLM *****
06/27 07:49:32 PM   Epoch = 8 iter 104 step
06/27 07:49:32 PM   Num examples = 16
06/27 07:49:32 PM   Batch size = 32
06/27 07:49:32 PM ***** Eval results *****
06/27 07:49:32 PM   acc = 0.9375
06/27 07:49:32 PM   cls_loss = 1.665970966219902
06/27 07:49:32 PM   eval_loss = 1.5559571981430054
06/27 07:49:32 PM   global_step = 104
06/27 07:49:32 PM   loss = 1.665970966219902
06/27 07:49:32 PM ***** LOSS printing *****
06/27 07:49:32 PM loss
06/27 07:49:32 PM tensor(1.1599, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:33 PM ***** LOSS printing *****
06/27 07:49:33 PM loss
06/27 07:49:33 PM tensor(1.6580, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:33 PM ***** LOSS printing *****
06/27 07:49:33 PM loss
06/27 07:49:33 PM tensor(1.7318, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:33 PM ***** LOSS printing *****
06/27 07:49:33 PM loss
06/27 07:49:33 PM tensor(1.1639, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:33 PM ***** LOSS printing *****
06/27 07:49:33 PM loss
06/27 07:49:33 PM tensor(1.1367, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:33 PM ***** Running evaluation MLM *****
06/27 07:49:33 PM   Epoch = 9 iter 109 step
06/27 07:49:33 PM   Num examples = 16
06/27 07:49:33 PM   Batch size = 32
06/27 07:49:34 PM ***** Eval results *****
06/27 07:49:34 PM   acc = 0.875
06/27 07:49:34 PM   cls_loss = 1.1367040872573853
06/27 07:49:34 PM   eval_loss = 1.3883687257766724
06/27 07:49:34 PM   global_step = 109
06/27 07:49:34 PM   loss = 1.1367040872573853
06/27 07:49:34 PM ***** LOSS printing *****
06/27 07:49:34 PM loss
06/27 07:49:34 PM tensor(1.2968, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:34 PM ***** LOSS printing *****
06/27 07:49:34 PM loss
06/27 07:49:34 PM tensor(1.3036, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:34 PM ***** LOSS printing *****
06/27 07:49:34 PM loss
06/27 07:49:34 PM tensor(1.2186, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:35 PM ***** LOSS printing *****
06/27 07:49:35 PM loss
06/27 07:49:35 PM tensor(1.3604, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:35 PM ***** LOSS printing *****
06/27 07:49:35 PM loss
06/27 07:49:35 PM tensor(1.0628, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:35 PM ***** Running evaluation MLM *****
06/27 07:49:35 PM   Epoch = 9 iter 114 step
06/27 07:49:35 PM   Num examples = 16
06/27 07:49:35 PM   Batch size = 32
06/27 07:49:36 PM ***** Eval results *****
06/27 07:49:36 PM   acc = 0.9375
06/27 07:49:36 PM   cls_loss = 1.229828159014384
06/27 07:49:36 PM   eval_loss = 0.7282278537750244
06/27 07:49:36 PM   global_step = 114
06/27 07:49:36 PM   loss = 1.229828159014384
06/27 07:49:36 PM ***** LOSS printing *****
06/27 07:49:36 PM loss
06/27 07:49:36 PM tensor(1.2631, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:36 PM ***** LOSS printing *****
06/27 07:49:36 PM loss
06/27 07:49:36 PM tensor(1.0981, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:36 PM ***** LOSS printing *****
06/27 07:49:36 PM loss
06/27 07:49:36 PM tensor(1.4212, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:36 PM ***** LOSS printing *****
06/27 07:49:36 PM loss
06/27 07:49:36 PM tensor(1.0606, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:37 PM ***** LOSS printing *****
06/27 07:49:37 PM loss
06/27 07:49:37 PM tensor(1.4071, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:49:37 PM ***** Running evaluation MLM *****
06/27 07:49:37 PM   Epoch = 9 iter 119 step
06/27 07:49:37 PM   Num examples = 16
06/27 07:49:37 PM   Batch size = 32
06/27 07:49:37 PM ***** Eval results *****
06/27 07:49:37 PM   acc = 1.0
06/27 07:49:37 PM   cls_loss = 1.2390076138756492
06/27 07:49:37 PM   eval_loss = 1.1787539720535278
06/27 07:49:37 PM   global_step = 119
06/27 07:49:37 PM   loss = 1.2390076138756492
06/27 07:49:37 PM ***** LOSS printing *****
06/27 07:49:37 PM loss
06/27 07:49:37 PM tensor(1.3078, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 06:41:51 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-100/', data_seed=100, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 06:41:51 PM device: cuda n_gpu: 1
06/27 06:41:51 PM Writing example 0 of 48
06/27 06:41:51 PM *** Example ***
06/27 06:41:51 PM guid: train-1
06/27 06:41:51 PM tokens: <s> a Ġfew Ġyears Ġlater Ġ, Ġtragedy Ġstruck Ġher Ġ, Ġfirst Ġa Ġfire Ġin Ġher Ġhouse Ġwhich Ġcaused Ġher Ġto Ġnot Ġbe Ġable Ġto Ġgo Ġinto Ġany Ġtype Ġof Ġlight Ġ, Ġand Ġthen Ġshe Ġwas Ġhanged Ġ. </s> ĠIt Ġis <mask>
06/27 06:41:51 PM input_ids: 0 102 367 107 423 2156 6906 2322 69 2156 78 10 668 11 69 790 61 1726 69 7 45 28 441 7 213 88 143 1907 9 1109 2156 8 172 79 21 32466 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:51 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:51 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:51 PM label: ['Ġgood']
06/27 06:41:51 PM Writing example 0 of 16
06/27 06:41:51 PM *** Example ***
06/27 06:41:51 PM guid: dev-1
06/27 06:41:51 PM tokens: <s> ch arl ie Ġis Ġman Ġwho Ġwakes Ġup Ġto Ġfind Ġthat Ġno Ġone Ġcan Ġsee Ġhim Ġ, Ġby Ġchance Ġhe Ġmeets Ġcar ol Ġon Ġa Ġlonely Ġhighway Ġ. </s> ĠIt Ġis <mask>
06/27 06:41:51 PM input_ids: 0 611 11278 324 16 313 54 34142 62 7 465 14 117 65 64 192 123 2156 30 778 37 6616 512 1168 15 10 20100 6418 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:51 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:51 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:51 PM label: ['Ġgood']
06/27 06:41:52 PM Writing example 0 of 2000
06/27 06:41:52 PM *** Example ***
06/27 06:41:52 PM guid: dev-1
06/27 06:41:52 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 06:41:52 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:52 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:52 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:41:52 PM label: ['Ġbad']
06/27 06:42:05 PM ***** Running training *****
06/27 06:42:05 PM   Num examples = 48
06/27 06:42:05 PM   Batch size = 4
06/27 06:42:05 PM   Num steps = 120
06/27 06:42:05 PM n: embeddings.word_embeddings.weight
06/27 06:42:05 PM n: embeddings.position_embeddings.weight
06/27 06:42:05 PM n: embeddings.token_type_embeddings.weight
06/27 06:42:05 PM n: embeddings.LayerNorm.weight
06/27 06:42:05 PM n: embeddings.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.0.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.0.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.0.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.0.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.0.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.0.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.0.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.0.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.0.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.0.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.0.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.0.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.1.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.1.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.1.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.1.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.1.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.1.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.1.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.1.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.1.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.1.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.1.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.1.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.2.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.2.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.2.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.2.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.2.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.2.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.2.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.2.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.2.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.2.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.2.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.2.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.3.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.3.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.3.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.3.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.3.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.3.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.3.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.3.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.3.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.3.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.3.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.3.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.4.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.4.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.4.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.4.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.4.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.4.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.4.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.4.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.4.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.4.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.4.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.4.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.5.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.5.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.5.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.5.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.5.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.5.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.5.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.5.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.5.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.5.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.5.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.5.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.6.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.6.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.6.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.6.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.6.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.6.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.6.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.6.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.6.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.6.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.6.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.6.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.7.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.7.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.7.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.7.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.7.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.7.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.7.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.7.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.7.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.7.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.7.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.7.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.8.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.8.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.8.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.8.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.8.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.8.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.8.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.8.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.8.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.8.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.8.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.8.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.9.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.9.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.9.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.9.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.9.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.9.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.9.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.9.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.9.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.9.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.9.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.9.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.10.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.10.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.10.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.10.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.10.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.10.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.10.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.10.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.10.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.10.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.10.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.10.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.11.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.11.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.11.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.11.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.11.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.11.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.11.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.11.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.11.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.11.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.11.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.11.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.12.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.12.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.12.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.12.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.12.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.12.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.12.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.12.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.12.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.12.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.12.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.12.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.13.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.13.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.13.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.13.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.13.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.13.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.13.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.13.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.13.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.13.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.13.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.13.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.14.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.14.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.14.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.14.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.14.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.14.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.14.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.14.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.14.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.14.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.14.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.14.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.15.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.15.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.15.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.15.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.15.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.15.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.15.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.15.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.15.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.15.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.15.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.15.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.16.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.16.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.16.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.16.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.16.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.16.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.16.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.16.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.16.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.16.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.16.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.16.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.17.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.17.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.17.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.17.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.17.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.17.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.17.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.17.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.17.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.17.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.17.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.17.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.18.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.18.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.18.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.18.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.18.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.18.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.18.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.18.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.18.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.18.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.18.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.18.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.19.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.19.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.19.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.19.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.19.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.19.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.19.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.19.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.19.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.19.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.19.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.19.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.20.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.20.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.20.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.20.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.20.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.20.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.20.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.20.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.20.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.20.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.20.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.20.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.21.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.21.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.21.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.21.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.21.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.21.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.21.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.21.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.21.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.21.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.21.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.21.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.22.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.22.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.22.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.22.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.22.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.22.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.22.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.22.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.22.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.22.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.22.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.22.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.23.attention.self.query.weight
06/27 06:42:05 PM n: encoder.layer.23.attention.self.query.bias
06/27 06:42:05 PM n: encoder.layer.23.attention.self.key.weight
06/27 06:42:05 PM n: encoder.layer.23.attention.self.key.bias
06/27 06:42:05 PM n: encoder.layer.23.attention.self.value.weight
06/27 06:42:05 PM n: encoder.layer.23.attention.self.value.bias
06/27 06:42:05 PM n: encoder.layer.23.attention.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.23.attention.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: encoder.layer.23.intermediate.dense.weight
06/27 06:42:05 PM n: encoder.layer.23.intermediate.dense.bias
06/27 06:42:05 PM n: encoder.layer.23.output.dense.weight
06/27 06:42:05 PM n: encoder.layer.23.output.dense.bias
06/27 06:42:05 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 06:42:05 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 06:42:05 PM n: pooler.dense.weight
06/27 06:42:05 PM n: pooler.dense.bias
06/27 06:42:05 PM n: roberta.embeddings.word_embeddings.weight
06/27 06:42:05 PM n: roberta.embeddings.position_embeddings.weight
06/27 06:42:05 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 06:42:05 PM n: roberta.embeddings.LayerNorm.weight
06/27 06:42:05 PM n: roberta.embeddings.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 06:42:05 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 06:42:05 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 06:42:05 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 06:42:05 PM n: roberta.pooler.dense.weight
06/27 06:42:05 PM n: roberta.pooler.dense.bias
06/27 06:42:05 PM n: lm_head.bias
06/27 06:42:05 PM n: lm_head.dense.weight
06/27 06:42:05 PM n: lm_head.dense.bias
06/27 06:42:05 PM n: lm_head.layer_norm.weight
06/27 06:42:05 PM n: lm_head.layer_norm.bias
06/27 06:42:05 PM n: lm_head.decoder.weight
06/27 06:42:05 PM Total parameters: 763292761
06/27 06:42:05 PM ***** LOSS printing *****
06/27 06:42:05 PM loss
06/27 06:42:05 PM tensor(23.7980, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:42:05 PM ***** LOSS printing *****
06/27 06:42:05 PM loss
06/27 06:42:05 PM tensor(14.8901, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:42:06 PM ***** LOSS printing *****
06/27 06:42:06 PM loss
06/27 06:42:06 PM tensor(10.8361, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:42:06 PM ***** LOSS printing *****
06/27 06:42:06 PM loss
06/27 06:42:06 PM tensor(7.2408, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:42:06 PM ***** Running evaluation MLM *****
06/27 06:42:06 PM   Epoch = 0 iter 4 step
06/27 06:42:06 PM   Num examples = 16
06/27 06:42:06 PM   Batch size = 32
06/27 06:42:07 PM ***** Eval results *****
06/27 06:42:07 PM   acc = 0.5
06/27 06:42:07 PM   cls_loss = 14.191258192062378
06/27 06:42:07 PM   eval_loss = 4.618947982788086
06/27 06:42:07 PM   global_step = 4
06/27 06:42:07 PM   loss = 14.191258192062378
06/27 06:42:07 PM ***** Save model *****
06/27 06:42:07 PM ***** Test Dataset Eval Result *****
06/27 06:43:09 PM ***** Eval results *****
06/27 06:43:09 PM   acc = 0.4965
06/27 06:43:09 PM   cls_loss = 14.191258192062378
06/27 06:43:09 PM   eval_loss = 4.623658263494098
06/27 06:43:09 PM   global_step = 4
06/27 06:43:09 PM   loss = 14.191258192062378
06/27 06:43:14 PM ***** LOSS printing *****
06/27 06:43:14 PM loss
06/27 06:43:14 PM tensor(5.3931, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:14 PM ***** LOSS printing *****
06/27 06:43:14 PM loss
06/27 06:43:14 PM tensor(5.1426, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:14 PM ***** LOSS printing *****
06/27 06:43:14 PM loss
06/27 06:43:14 PM tensor(3.0194, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:14 PM ***** LOSS printing *****
06/27 06:43:14 PM loss
06/27 06:43:14 PM tensor(4.2754, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:15 PM ***** LOSS printing *****
06/27 06:43:15 PM loss
06/27 06:43:15 PM tensor(4.1664, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:15 PM ***** Running evaluation MLM *****
06/27 06:43:15 PM   Epoch = 0 iter 9 step
06/27 06:43:15 PM   Num examples = 16
06/27 06:43:15 PM   Batch size = 32
06/27 06:43:15 PM ***** Eval results *****
06/27 06:43:15 PM   acc = 0.5
06/27 06:43:15 PM   cls_loss = 8.751335197024876
06/27 06:43:15 PM   eval_loss = 1.51393723487854
06/27 06:43:15 PM   global_step = 9
06/27 06:43:15 PM   loss = 8.751335197024876
06/27 06:43:15 PM ***** LOSS printing *****
06/27 06:43:15 PM loss
06/27 06:43:15 PM tensor(2.5115, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:16 PM ***** LOSS printing *****
06/27 06:43:16 PM loss
06/27 06:43:16 PM tensor(3.3691, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:16 PM ***** LOSS printing *****
06/27 06:43:16 PM loss
06/27 06:43:16 PM tensor(4.9498, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:16 PM ***** LOSS printing *****
06/27 06:43:16 PM loss
06/27 06:43:16 PM tensor(1.9128, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:16 PM ***** LOSS printing *****
06/27 06:43:16 PM loss
06/27 06:43:16 PM tensor(3.1532, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:43:16 PM ***** Running evaluation MLM *****
06/27 06:43:16 PM   Epoch = 1 iter 14 step
06/27 06:43:16 PM   Num examples = 16
06/27 06:43:16 PM   Batch size = 32
06/27 06:43:17 PM ***** Eval results *****
06/27 06:43:17 PM   acc = 0.625
06/27 06:43:17 PM   cls_loss = 2.53302401304245
06/27 06:43:17 PM   eval_loss = 2.143141031265259
06/27 06:43:17 PM   global_step = 14
06/27 06:43:17 PM   loss = 2.53302401304245
06/27 06:43:17 PM ***** Save model *****
06/27 06:43:17 PM ***** Test Dataset Eval Result *****
06/27 06:44:19 PM ***** Eval results *****
06/27 06:44:19 PM   acc = 0.571
06/27 06:44:19 PM   cls_loss = 2.53302401304245
06/27 06:44:19 PM   eval_loss = 2.161860693068731
06/27 06:44:19 PM   global_step = 14
06/27 06:44:19 PM   loss = 2.53302401304245
06/27 06:44:24 PM ***** LOSS printing *****
06/27 06:44:24 PM loss
06/27 06:44:24 PM tensor(1.9013, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:24 PM ***** LOSS printing *****
06/27 06:44:24 PM loss
06/27 06:44:24 PM tensor(2.7309, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:24 PM ***** LOSS printing *****
06/27 06:44:24 PM loss
06/27 06:44:24 PM tensor(2.3018, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:24 PM ***** LOSS printing *****
06/27 06:44:24 PM loss
06/27 06:44:24 PM tensor(1.6956, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:25 PM ***** LOSS printing *****
06/27 06:44:25 PM loss
06/27 06:44:25 PM tensor(2.3531, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:25 PM ***** Running evaluation MLM *****
06/27 06:44:25 PM   Epoch = 1 iter 19 step
06/27 06:44:25 PM   Num examples = 16
06/27 06:44:25 PM   Batch size = 32
06/27 06:44:25 PM ***** Eval results *****
06/27 06:44:25 PM   acc = 0.4375
06/27 06:44:25 PM   cls_loss = 2.292685423578535
06/27 06:44:25 PM   eval_loss = 1.9116806983947754
06/27 06:44:25 PM   global_step = 19
06/27 06:44:25 PM   loss = 2.292685423578535
06/27 06:44:25 PM ***** LOSS printing *****
06/27 06:44:25 PM loss
06/27 06:44:25 PM tensor(1.9606, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:26 PM ***** LOSS printing *****
06/27 06:44:26 PM loss
06/27 06:44:26 PM tensor(2.2519, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:26 PM ***** LOSS printing *****
06/27 06:44:26 PM loss
06/27 06:44:26 PM tensor(2.4074, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:26 PM ***** LOSS printing *****
06/27 06:44:26 PM loss
06/27 06:44:26 PM tensor(2.3689, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:26 PM ***** LOSS printing *****
06/27 06:44:26 PM loss
06/27 06:44:26 PM tensor(3.2620, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:44:26 PM ***** Running evaluation MLM *****
06/27 06:44:26 PM   Epoch = 1 iter 24 step
06/27 06:44:26 PM   Num examples = 16
06/27 06:44:26 PM   Batch size = 32
06/27 06:44:27 PM ***** Eval results *****
06/27 06:44:27 PM   acc = 0.8125
06/27 06:44:27 PM   cls_loss = 2.3583071331183114
06/27 06:44:27 PM   eval_loss = 1.5385184288024902
06/27 06:44:27 PM   global_step = 24
06/27 06:44:27 PM   loss = 2.3583071331183114
06/27 06:44:27 PM ***** Save model *****
06/27 06:44:27 PM ***** Test Dataset Eval Result *****
06/27 06:45:29 PM ***** Eval results *****
06/27 06:45:29 PM   acc = 0.664
06/27 06:45:29 PM   cls_loss = 2.3583071331183114
06/27 06:45:29 PM   eval_loss = 1.6266769378904313
06/27 06:45:29 PM   global_step = 24
06/27 06:45:29 PM   loss = 2.3583071331183114
06/27 06:45:33 PM ***** LOSS printing *****
06/27 06:45:33 PM loss
06/27 06:45:33 PM tensor(1.3928, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:34 PM ***** LOSS printing *****
06/27 06:45:34 PM loss
06/27 06:45:34 PM tensor(2.6615, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:34 PM ***** LOSS printing *****
06/27 06:45:34 PM loss
06/27 06:45:34 PM tensor(1.7322, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:34 PM ***** LOSS printing *****
06/27 06:45:34 PM loss
06/27 06:45:34 PM tensor(1.4603, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:34 PM ***** LOSS printing *****
06/27 06:45:34 PM loss
06/27 06:45:34 PM tensor(2.7923, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:34 PM ***** Running evaluation MLM *****
06/27 06:45:34 PM   Epoch = 2 iter 29 step
06/27 06:45:34 PM   Num examples = 16
06/27 06:45:34 PM   Batch size = 32
06/27 06:45:35 PM ***** Eval results *****
06/27 06:45:35 PM   acc = 0.5
06/27 06:45:35 PM   cls_loss = 2.007821226119995
06/27 06:45:35 PM   eval_loss = 1.991567850112915
06/27 06:45:35 PM   global_step = 29
06/27 06:45:35 PM   loss = 2.007821226119995
06/27 06:45:35 PM ***** LOSS printing *****
06/27 06:45:35 PM loss
06/27 06:45:35 PM tensor(2.8567, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:35 PM ***** LOSS printing *****
06/27 06:45:35 PM loss
06/27 06:45:35 PM tensor(1.5959, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:35 PM ***** LOSS printing *****
06/27 06:45:35 PM loss
06/27 06:45:35 PM tensor(1.9808, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:36 PM ***** LOSS printing *****
06/27 06:45:36 PM loss
06/27 06:45:36 PM tensor(2.4075, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:36 PM ***** LOSS printing *****
06/27 06:45:36 PM loss
06/27 06:45:36 PM tensor(2.1218, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:36 PM ***** Running evaluation MLM *****
06/27 06:45:36 PM   Epoch = 2 iter 34 step
06/27 06:45:36 PM   Num examples = 16
06/27 06:45:36 PM   Batch size = 32
06/27 06:45:37 PM ***** Eval results *****
06/27 06:45:37 PM   acc = 0.5625
06/27 06:45:37 PM   cls_loss = 2.1001715540885924
06/27 06:45:37 PM   eval_loss = 1.699385404586792
06/27 06:45:37 PM   global_step = 34
06/27 06:45:37 PM   loss = 2.1001715540885924
06/27 06:45:37 PM ***** LOSS printing *****
06/27 06:45:37 PM loss
06/27 06:45:37 PM tensor(1.9579, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:37 PM ***** LOSS printing *****
06/27 06:45:37 PM loss
06/27 06:45:37 PM tensor(2.4364, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:37 PM ***** LOSS printing *****
06/27 06:45:37 PM loss
06/27 06:45:37 PM tensor(1.2823, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:37 PM ***** LOSS printing *****
06/27 06:45:37 PM loss
06/27 06:45:37 PM tensor(2.3335, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:37 PM ***** LOSS printing *****
06/27 06:45:37 PM loss
06/27 06:45:37 PM tensor(2.2232, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:38 PM ***** Running evaluation MLM *****
06/27 06:45:38 PM   Epoch = 3 iter 39 step
06/27 06:45:38 PM   Num examples = 16
06/27 06:45:38 PM   Batch size = 32
06/27 06:45:38 PM ***** Eval results *****
06/27 06:45:38 PM   acc = 0.5
06/27 06:45:38 PM   cls_loss = 1.9463478326797485
06/27 06:45:38 PM   eval_loss = 2.6834962368011475
06/27 06:45:38 PM   global_step = 39
06/27 06:45:38 PM   loss = 1.9463478326797485
06/27 06:45:38 PM ***** LOSS printing *****
06/27 06:45:38 PM loss
06/27 06:45:38 PM tensor(2.7021, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:38 PM ***** LOSS printing *****
06/27 06:45:38 PM loss
06/27 06:45:38 PM tensor(2.2160, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:39 PM ***** LOSS printing *****
06/27 06:45:39 PM loss
06/27 06:45:39 PM tensor(2.0414, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:39 PM ***** LOSS printing *****
06/27 06:45:39 PM loss
06/27 06:45:39 PM tensor(1.8732, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:39 PM ***** LOSS printing *****
06/27 06:45:39 PM loss
06/27 06:45:39 PM tensor(1.9651, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:39 PM ***** Running evaluation MLM *****
06/27 06:45:39 PM   Epoch = 3 iter 44 step
06/27 06:45:39 PM   Num examples = 16
06/27 06:45:39 PM   Batch size = 32
06/27 06:45:40 PM ***** Eval results *****
06/27 06:45:40 PM   acc = 0.5
06/27 06:45:40 PM   cls_loss = 2.0796018242836
06/27 06:45:40 PM   eval_loss = 1.4184114933013916
06/27 06:45:40 PM   global_step = 44
06/27 06:45:40 PM   loss = 2.0796018242836
06/27 06:45:40 PM ***** LOSS printing *****
06/27 06:45:40 PM loss
06/27 06:45:40 PM tensor(1.4262, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:40 PM ***** LOSS printing *****
06/27 06:45:40 PM loss
06/27 06:45:40 PM tensor(2.2830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:40 PM ***** LOSS printing *****
06/27 06:45:40 PM loss
06/27 06:45:40 PM tensor(2.2606, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:40 PM ***** LOSS printing *****
06/27 06:45:40 PM loss
06/27 06:45:40 PM tensor(1.9305, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:41 PM ***** LOSS printing *****
06/27 06:45:41 PM loss
06/27 06:45:41 PM tensor(1.4110, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:45:41 PM ***** Running evaluation MLM *****
06/27 06:45:41 PM   Epoch = 4 iter 49 step
06/27 06:45:41 PM   Num examples = 16
06/27 06:45:41 PM   Batch size = 32
06/27 06:45:41 PM ***** Eval results *****
06/27 06:45:41 PM   acc = 0.9375
06/27 06:45:41 PM   cls_loss = 1.4109828472137451
06/27 06:45:41 PM   eval_loss = 1.426842212677002
06/27 06:45:41 PM   global_step = 49
06/27 06:45:41 PM   loss = 1.4109828472137451
06/27 06:45:41 PM ***** Save model *****
06/27 06:45:41 PM ***** Test Dataset Eval Result *****
06/27 06:46:44 PM ***** Eval results *****
06/27 06:46:44 PM   acc = 0.817
06/27 06:46:44 PM   cls_loss = 1.4109828472137451
06/27 06:46:44 PM   eval_loss = 1.549673699197315
06/27 06:46:44 PM   global_step = 49
06/27 06:46:44 PM   loss = 1.4109828472137451
06/27 06:46:48 PM ***** LOSS printing *****
06/27 06:46:48 PM loss
06/27 06:46:48 PM tensor(1.1564, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:48 PM ***** LOSS printing *****
06/27 06:46:48 PM loss
06/27 06:46:48 PM tensor(1.7639, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:49 PM ***** LOSS printing *****
06/27 06:46:49 PM loss
06/27 06:46:49 PM tensor(1.9773, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:49 PM ***** LOSS printing *****
06/27 06:46:49 PM loss
06/27 06:46:49 PM tensor(1.3711, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:49 PM ***** LOSS printing *****
06/27 06:46:49 PM loss
06/27 06:46:49 PM tensor(1.5093, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:49 PM ***** Running evaluation MLM *****
06/27 06:46:49 PM   Epoch = 4 iter 54 step
06/27 06:46:49 PM   Num examples = 16
06/27 06:46:49 PM   Batch size = 32
06/27 06:46:50 PM ***** Eval results *****
06/27 06:46:50 PM   acc = 0.875
06/27 06:46:50 PM   cls_loss = 1.531486411889394
06/27 06:46:50 PM   eval_loss = 0.9513104557991028
06/27 06:46:50 PM   global_step = 54
06/27 06:46:50 PM   loss = 1.531486411889394
06/27 06:46:50 PM ***** LOSS printing *****
06/27 06:46:50 PM loss
06/27 06:46:50 PM tensor(0.9756, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:50 PM ***** LOSS printing *****
06/27 06:46:50 PM loss
06/27 06:46:50 PM tensor(2.1942, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:50 PM ***** LOSS printing *****
06/27 06:46:50 PM loss
06/27 06:46:50 PM tensor(1.7724, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:50 PM ***** LOSS printing *****
06/27 06:46:50 PM loss
06/27 06:46:50 PM tensor(2.2499, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:51 PM ***** LOSS printing *****
06/27 06:46:51 PM loss
06/27 06:46:51 PM tensor(1.5475, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:51 PM ***** Running evaluation MLM *****
06/27 06:46:51 PM   Epoch = 4 iter 59 step
06/27 06:46:51 PM   Num examples = 16
06/27 06:46:51 PM   Batch size = 32
06/27 06:46:51 PM ***** Eval results *****
06/27 06:46:51 PM   acc = 0.875
06/27 06:46:51 PM   cls_loss = 1.6298663941296665
06/27 06:46:51 PM   eval_loss = 1.2253988981246948
06/27 06:46:51 PM   global_step = 59
06/27 06:46:51 PM   loss = 1.6298663941296665
06/27 06:46:51 PM ***** LOSS printing *****
06/27 06:46:51 PM loss
06/27 06:46:51 PM tensor(1.4331, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:52 PM ***** LOSS printing *****
06/27 06:46:52 PM loss
06/27 06:46:52 PM tensor(1.0765, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:52 PM ***** LOSS printing *****
06/27 06:46:52 PM loss
06/27 06:46:52 PM tensor(1.5157, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:52 PM ***** LOSS printing *****
06/27 06:46:52 PM loss
06/27 06:46:52 PM tensor(1.5993, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:52 PM ***** LOSS printing *****
06/27 06:46:52 PM loss
06/27 06:46:52 PM tensor(1.3182, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:52 PM ***** Running evaluation MLM *****
06/27 06:46:52 PM   Epoch = 5 iter 64 step
06/27 06:46:52 PM   Num examples = 16
06/27 06:46:52 PM   Batch size = 32
06/27 06:46:53 PM ***** Eval results *****
06/27 06:46:53 PM   acc = 0.6875
06/27 06:46:53 PM   cls_loss = 1.377388745546341
06/27 06:46:53 PM   eval_loss = 2.359647035598755
06/27 06:46:53 PM   global_step = 64
06/27 06:46:53 PM   loss = 1.377388745546341
06/27 06:46:53 PM ***** LOSS printing *****
06/27 06:46:53 PM loss
06/27 06:46:53 PM tensor(1.2016, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:53 PM ***** LOSS printing *****
06/27 06:46:53 PM loss
06/27 06:46:53 PM tensor(1.4969, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:53 PM ***** LOSS printing *****
06/27 06:46:53 PM loss
06/27 06:46:53 PM tensor(1.4767, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:54 PM ***** LOSS printing *****
06/27 06:46:54 PM loss
06/27 06:46:54 PM tensor(1.9050, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:54 PM ***** LOSS printing *****
06/27 06:46:54 PM loss
06/27 06:46:54 PM tensor(2.1416, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:54 PM ***** Running evaluation MLM *****
06/27 06:46:54 PM   Epoch = 5 iter 69 step
06/27 06:46:54 PM   Num examples = 16
06/27 06:46:54 PM   Batch size = 32
06/27 06:46:54 PM ***** Eval results *****
06/27 06:46:54 PM   acc = 0.8125
06/27 06:46:54 PM   cls_loss = 1.5257028473748102
06/27 06:46:54 PM   eval_loss = 1.7473399639129639
06/27 06:46:54 PM   global_step = 69
06/27 06:46:54 PM   loss = 1.5257028473748102
06/27 06:46:55 PM ***** LOSS printing *****
06/27 06:46:55 PM loss
06/27 06:46:55 PM tensor(1.9968, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:55 PM ***** LOSS printing *****
06/27 06:46:55 PM loss
06/27 06:46:55 PM tensor(1.4754, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:55 PM ***** LOSS printing *****
06/27 06:46:55 PM loss
06/27 06:46:55 PM tensor(1.2803, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:55 PM ***** LOSS printing *****
06/27 06:46:55 PM loss
06/27 06:46:55 PM tensor(1.0792, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:55 PM ***** LOSS printing *****
06/27 06:46:55 PM loss
06/27 06:46:55 PM tensor(1.0777, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:56 PM ***** Running evaluation MLM *****
06/27 06:46:56 PM   Epoch = 6 iter 74 step
06/27 06:46:56 PM   Num examples = 16
06/27 06:46:56 PM   Batch size = 32
06/27 06:46:56 PM ***** Eval results *****
06/27 06:46:56 PM   acc = 0.875
06/27 06:46:56 PM   cls_loss = 1.0784159898757935
06/27 06:46:56 PM   eval_loss = 1.4873206615447998
06/27 06:46:56 PM   global_step = 74
06/27 06:46:56 PM   loss = 1.0784159898757935
06/27 06:46:56 PM ***** LOSS printing *****
06/27 06:46:56 PM loss
06/27 06:46:56 PM tensor(0.9562, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:56 PM ***** LOSS printing *****
06/27 06:46:56 PM loss
06/27 06:46:56 PM tensor(1.4234, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:57 PM ***** LOSS printing *****
06/27 06:46:57 PM loss
06/27 06:46:57 PM tensor(1.6399, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:57 PM ***** LOSS printing *****
06/27 06:46:57 PM loss
06/27 06:46:57 PM tensor(1.6078, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:57 PM ***** LOSS printing *****
06/27 06:46:57 PM loss
06/27 06:46:57 PM tensor(1.2001, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:57 PM ***** Running evaluation MLM *****
06/27 06:46:57 PM   Epoch = 6 iter 79 step
06/27 06:46:57 PM   Num examples = 16
06/27 06:46:57 PM   Batch size = 32
06/27 06:46:58 PM ***** Eval results *****
06/27 06:46:58 PM   acc = 0.875
06/27 06:46:58 PM   cls_loss = 1.283457636833191
06/27 06:46:58 PM   eval_loss = 1.4990538358688354
06/27 06:46:58 PM   global_step = 79
06/27 06:46:58 PM   loss = 1.283457636833191
06/27 06:46:58 PM ***** LOSS printing *****
06/27 06:46:58 PM loss
06/27 06:46:58 PM tensor(1.4646, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:58 PM ***** LOSS printing *****
06/27 06:46:58 PM loss
06/27 06:46:58 PM tensor(1.4103, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:58 PM ***** LOSS printing *****
06/27 06:46:58 PM loss
06/27 06:46:58 PM tensor(1.4999, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:58 PM ***** LOSS printing *****
06/27 06:46:58 PM loss
06/27 06:46:58 PM tensor(1.2789, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:59 PM ***** LOSS printing *****
06/27 06:46:59 PM loss
06/27 06:46:59 PM tensor(1.7732, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:46:59 PM ***** Running evaluation MLM *****
06/27 06:46:59 PM   Epoch = 6 iter 84 step
06/27 06:46:59 PM   Num examples = 16
06/27 06:46:59 PM   Batch size = 32
06/27 06:46:59 PM ***** Eval results *****
06/27 06:46:59 PM   acc = 0.8125
06/27 06:46:59 PM   cls_loss = 1.3675917188326518
06/27 06:46:59 PM   eval_loss = 1.0767582654953003
06/27 06:46:59 PM   global_step = 84
06/27 06:46:59 PM   loss = 1.3675917188326518
06/27 06:46:59 PM ***** LOSS printing *****
06/27 06:46:59 PM loss
06/27 06:46:59 PM tensor(1.2120, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:00 PM ***** LOSS printing *****
06/27 06:47:00 PM loss
06/27 06:47:00 PM tensor(1.3124, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:00 PM ***** LOSS printing *****
06/27 06:47:00 PM loss
06/27 06:47:00 PM tensor(1.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:00 PM ***** LOSS printing *****
06/27 06:47:00 PM loss
06/27 06:47:00 PM tensor(1.4022, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:00 PM ***** LOSS printing *****
06/27 06:47:00 PM loss
06/27 06:47:00 PM tensor(1.7139, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:00 PM ***** Running evaluation MLM *****
06/27 06:47:00 PM   Epoch = 7 iter 89 step
06/27 06:47:00 PM   Num examples = 16
06/27 06:47:00 PM   Batch size = 32
06/27 06:47:01 PM ***** Eval results *****
06/27 06:47:01 PM   acc = 0.5625
06/27 06:47:01 PM   cls_loss = 1.3491328001022338
06/27 06:47:01 PM   eval_loss = 2.3696017265319824
06/27 06:47:01 PM   global_step = 89
06/27 06:47:01 PM   loss = 1.3491328001022338
06/27 06:47:01 PM ***** LOSS printing *****
06/27 06:47:01 PM loss
06/27 06:47:01 PM tensor(2.3181, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:01 PM ***** LOSS printing *****
06/27 06:47:01 PM loss
06/27 06:47:01 PM tensor(1.8004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:01 PM ***** LOSS printing *****
06/27 06:47:01 PM loss
06/27 06:47:01 PM tensor(2.2525, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:02 PM ***** LOSS printing *****
06/27 06:47:02 PM loss
06/27 06:47:02 PM tensor(1.2858, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:02 PM ***** LOSS printing *****
06/27 06:47:02 PM loss
06/27 06:47:02 PM tensor(2.1109, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:02 PM ***** Running evaluation MLM *****
06/27 06:47:02 PM   Epoch = 7 iter 94 step
06/27 06:47:02 PM   Num examples = 16
06/27 06:47:02 PM   Batch size = 32
06/27 06:47:02 PM ***** Eval results *****
06/27 06:47:02 PM   acc = 0.875
06/27 06:47:02 PM   cls_loss = 1.6513306736946105
06/27 06:47:02 PM   eval_loss = 1.617185354232788
06/27 06:47:02 PM   global_step = 94
06/27 06:47:02 PM   loss = 1.6513306736946105
06/27 06:47:03 PM ***** LOSS printing *****
06/27 06:47:03 PM loss
06/27 06:47:03 PM tensor(1.4120, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:03 PM ***** LOSS printing *****
06/27 06:47:03 PM loss
06/27 06:47:03 PM tensor(1.4139, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:03 PM ***** LOSS printing *****
06/27 06:47:03 PM loss
06/27 06:47:03 PM tensor(1.4723, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:03 PM ***** LOSS printing *****
06/27 06:47:03 PM loss
06/27 06:47:03 PM tensor(1.0626, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:03 PM ***** LOSS printing *****
06/27 06:47:03 PM loss
06/27 06:47:03 PM tensor(1.2719, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:04 PM ***** Running evaluation MLM *****
06/27 06:47:04 PM   Epoch = 8 iter 99 step
06/27 06:47:04 PM   Num examples = 16
06/27 06:47:04 PM   Batch size = 32
06/27 06:47:04 PM ***** Eval results *****
06/27 06:47:04 PM   acc = 0.9375
06/27 06:47:04 PM   cls_loss = 1.2689382235209148
06/27 06:47:04 PM   eval_loss = 1.563251256942749
06/27 06:47:04 PM   global_step = 99
06/27 06:47:04 PM   loss = 1.2689382235209148
06/27 06:47:04 PM ***** LOSS printing *****
06/27 06:47:04 PM loss
06/27 06:47:04 PM tensor(0.9734, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:04 PM ***** LOSS printing *****
06/27 06:47:04 PM loss
06/27 06:47:04 PM tensor(0.7551, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:05 PM ***** LOSS printing *****
06/27 06:47:05 PM loss
06/27 06:47:05 PM tensor(2.4241, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:05 PM ***** LOSS printing *****
06/27 06:47:05 PM loss
06/27 06:47:05 PM tensor(1.2036, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:05 PM ***** LOSS printing *****
06/27 06:47:05 PM loss
06/27 06:47:05 PM tensor(1.3796, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:05 PM ***** Running evaluation MLM *****
06/27 06:47:05 PM   Epoch = 8 iter 104 step
06/27 06:47:05 PM   Num examples = 16
06/27 06:47:05 PM   Batch size = 32
06/27 06:47:06 PM ***** Eval results *****
06/27 06:47:06 PM   acc = 0.9375
06/27 06:47:06 PM   cls_loss = 1.317811205983162
06/27 06:47:06 PM   eval_loss = 1.614394187927246
06/27 06:47:06 PM   global_step = 104
06/27 06:47:06 PM   loss = 1.317811205983162
06/27 06:47:06 PM ***** LOSS printing *****
06/27 06:47:06 PM loss
06/27 06:47:06 PM tensor(3.0190, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:06 PM ***** LOSS printing *****
06/27 06:47:06 PM loss
06/27 06:47:06 PM tensor(2.6745, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:06 PM ***** LOSS printing *****
06/27 06:47:06 PM loss
06/27 06:47:06 PM tensor(1.1014, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:06 PM ***** LOSS printing *****
06/27 06:47:06 PM loss
06/27 06:47:06 PM tensor(1.0020, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:07 PM ***** LOSS printing *****
06/27 06:47:07 PM loss
06/27 06:47:07 PM tensor(1.1167, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:07 PM ***** Running evaluation MLM *****
06/27 06:47:07 PM   Epoch = 9 iter 109 step
06/27 06:47:07 PM   Num examples = 16
06/27 06:47:07 PM   Batch size = 32
06/27 06:47:07 PM ***** Eval results *****
06/27 06:47:07 PM   acc = 0.9375
06/27 06:47:07 PM   cls_loss = 1.11673903465271
06/27 06:47:07 PM   eval_loss = 0.9474624395370483
06/27 06:47:07 PM   global_step = 109
06/27 06:47:07 PM   loss = 1.11673903465271
06/27 06:47:07 PM ***** LOSS printing *****
06/27 06:47:07 PM loss
06/27 06:47:07 PM tensor(1.2289, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:08 PM ***** LOSS printing *****
06/27 06:47:08 PM loss
06/27 06:47:08 PM tensor(1.4398, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:08 PM ***** LOSS printing *****
06/27 06:47:08 PM loss
06/27 06:47:08 PM tensor(1.7258, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:08 PM ***** LOSS printing *****
06/27 06:47:08 PM loss
06/27 06:47:08 PM tensor(1.0173, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:08 PM ***** LOSS printing *****
06/27 06:47:08 PM loss
06/27 06:47:08 PM tensor(1.4336, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:08 PM ***** Running evaluation MLM *****
06/27 06:47:08 PM   Epoch = 9 iter 114 step
06/27 06:47:08 PM   Num examples = 16
06/27 06:47:08 PM   Batch size = 32
06/27 06:47:09 PM ***** Eval results *****
06/27 06:47:09 PM   acc = 0.9375
06/27 06:47:09 PM   cls_loss = 1.3270171880722046
06/27 06:47:09 PM   eval_loss = 0.9143082499504089
06/27 06:47:09 PM   global_step = 114
06/27 06:47:09 PM   loss = 1.3270171880722046
06/27 06:47:09 PM ***** LOSS printing *****
06/27 06:47:09 PM loss
06/27 06:47:09 PM tensor(1.9132, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:09 PM ***** LOSS printing *****
06/27 06:47:09 PM loss
06/27 06:47:09 PM tensor(1.5992, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:09 PM ***** LOSS printing *****
06/27 06:47:09 PM loss
06/27 06:47:09 PM tensor(1.2147, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:10 PM ***** LOSS printing *****
06/27 06:47:10 PM loss
06/27 06:47:10 PM tensor(1.5180, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:10 PM ***** LOSS printing *****
06/27 06:47:10 PM loss
06/27 06:47:10 PM tensor(1.1972, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:47:10 PM ***** Running evaluation MLM *****
06/27 06:47:10 PM   Epoch = 9 iter 119 step
06/27 06:47:10 PM   Num examples = 16
06/27 06:47:10 PM   Batch size = 32
06/27 06:47:10 PM ***** Eval results *****
06/27 06:47:10 PM   acc = 1.0
06/27 06:47:10 PM   cls_loss = 1.400398156859658
06/27 06:47:10 PM   eval_loss = 1.0821797847747803
06/27 06:47:10 PM   global_step = 119
06/27 06:47:10 PM   loss = 1.400398156859658
06/27 06:47:10 PM ***** Save model *****
06/27 06:47:10 PM ***** Test Dataset Eval Result *****
06/27 06:48:13 PM ***** Eval results *****
06/27 06:48:13 PM   acc = 0.8755
06/27 06:48:13 PM   cls_loss = 1.400398156859658
06/27 06:48:13 PM   eval_loss = 1.2440849258786155
06/27 06:48:13 PM   global_step = 119
06/27 06:48:13 PM   loss = 1.400398156859658
06/27 06:48:16 PM ***** LOSS printing *****
06/27 06:48:16 PM loss
06/27 06:48:16 PM tensor(1.3255, device='cuda:0', grad_fn=<NllLossBackward0>)

06/27 06:48:18 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-87/', data_seed=87, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 06:48:18 PM device: cuda n_gpu: 1
06/27 06:48:18 PM Writing example 0 of 48
06/27 06:48:18 PM *** Example ***
06/27 06:48:18 PM guid: train-1
06/27 06:48:18 PM tokens: <s> the Ġpace Ġof Ġthe Ġfilm Ġis Ġvery Ġslow Ġ( Ġfor Ġobvious Ġreasons Ġ) Ġand Ġthat Ġtoo Ġbecomes Ġoff - put ting Ġ. </s> ĠIt Ġis <mask>
06/27 06:48:18 PM input_ids: 0 627 2877 9 5 822 16 182 2635 36 13 4678 2188 4839 8 14 350 3374 160 12 9179 2577 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM label: ['Ġbad']
06/27 06:48:18 PM Writing example 0 of 16
06/27 06:48:18 PM *** Example ***
06/27 06:48:18 PM guid: dev-1
06/27 06:48:18 PM tokens: <s> what Ġcould Ġhave Ġbeen Ġa Ġpointed Ġlittle Ġch iller Ġabout Ġthe Ġfrightening Ġsed uct iveness Ġof Ġnew Ġtechnology Ġloses Ġfaith Ġin Ġits Ġown Ġviability Ġand Ġsucc umbs Ġto Ġjoy less Ġspecial - effects Ġexcess Ġ. </s> ĠIt Ġis <mask>
06/27 06:48:18 PM input_ids: 0 12196 115 33 57 10 3273 410 1855 8690 59 5 21111 10195 21491 12367 9 92 806 13585 3975 11 63 308 23990 8 25047 29123 7 5823 1672 780 12 38375 7400 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM label: ['Ġbad']
06/27 06:48:18 PM Writing example 0 of 2000
06/27 06:48:18 PM *** Example ***
06/27 06:48:18 PM guid: dev-1
06/27 06:48:18 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 06:48:18 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 06:48:18 PM label: ['Ġbad']
06/27 06:48:32 PM ***** Running training *****
06/27 06:48:32 PM   Num examples = 48
06/27 06:48:32 PM   Batch size = 4
06/27 06:48:32 PM   Num steps = 120
06/27 06:48:32 PM n: embeddings.word_embeddings.weight
06/27 06:48:32 PM n: embeddings.position_embeddings.weight
06/27 06:48:32 PM n: embeddings.token_type_embeddings.weight
06/27 06:48:32 PM n: embeddings.LayerNorm.weight
06/27 06:48:32 PM n: embeddings.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.0.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.0.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.0.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.0.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.0.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.0.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.0.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.0.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.0.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.0.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.0.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.0.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.1.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.1.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.1.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.1.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.1.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.1.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.1.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.1.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.1.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.1.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.1.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.1.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.2.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.2.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.2.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.2.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.2.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.2.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.2.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.2.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.2.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.2.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.2.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.2.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.3.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.3.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.3.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.3.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.3.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.3.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.3.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.3.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.3.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.3.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.3.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.3.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.4.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.4.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.4.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.4.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.4.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.4.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.4.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.4.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.4.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.4.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.4.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.4.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.5.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.5.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.5.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.5.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.5.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.5.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.5.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.5.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.5.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.5.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.5.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.5.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.6.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.6.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.6.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.6.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.6.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.6.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.6.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.6.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.6.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.6.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.6.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.6.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.7.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.7.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.7.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.7.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.7.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.7.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.7.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.7.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.7.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.7.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.7.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.7.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.8.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.8.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.8.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.8.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.8.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.8.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.8.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.8.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.8.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.8.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.8.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.8.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.9.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.9.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.9.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.9.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.9.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.9.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.9.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.9.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.9.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.9.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.9.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.9.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.10.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.10.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.10.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.10.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.10.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.10.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.10.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.10.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.10.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.10.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.10.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.10.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.11.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.11.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.11.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.11.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.11.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.11.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.11.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.11.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.11.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.11.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.11.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.11.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.12.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.12.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.12.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.12.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.12.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.12.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.12.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.12.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.12.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.12.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.12.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.12.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.13.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.13.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.13.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.13.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.13.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.13.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.13.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.13.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.13.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.13.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.13.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.13.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.14.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.14.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.14.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.14.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.14.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.14.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.14.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.14.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.14.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.14.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.14.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.14.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.15.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.15.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.15.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.15.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.15.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.15.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.15.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.15.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.15.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.15.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.15.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.15.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.16.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.16.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.16.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.16.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.16.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.16.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.16.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.16.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.16.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.16.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.16.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.16.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.17.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.17.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.17.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.17.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.17.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.17.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.17.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.17.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.17.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.17.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.17.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.17.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.18.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.18.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.18.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.18.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.18.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.18.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.18.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.18.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.18.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.18.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.18.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.18.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.19.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.19.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.19.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.19.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.19.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.19.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.19.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.19.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.19.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.19.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.19.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.19.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.20.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.20.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.20.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.20.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.20.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.20.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.20.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.20.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.20.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.20.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.20.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.20.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.21.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.21.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.21.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.21.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.21.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.21.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.21.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.21.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.21.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.21.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.21.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.21.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.22.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.22.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.22.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.22.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.22.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.22.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.22.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.22.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.22.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.22.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.22.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.22.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.23.attention.self.query.weight
06/27 06:48:32 PM n: encoder.layer.23.attention.self.query.bias
06/27 06:48:32 PM n: encoder.layer.23.attention.self.key.weight
06/27 06:48:32 PM n: encoder.layer.23.attention.self.key.bias
06/27 06:48:32 PM n: encoder.layer.23.attention.self.value.weight
06/27 06:48:32 PM n: encoder.layer.23.attention.self.value.bias
06/27 06:48:32 PM n: encoder.layer.23.attention.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.23.attention.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: encoder.layer.23.intermediate.dense.weight
06/27 06:48:32 PM n: encoder.layer.23.intermediate.dense.bias
06/27 06:48:32 PM n: encoder.layer.23.output.dense.weight
06/27 06:48:32 PM n: encoder.layer.23.output.dense.bias
06/27 06:48:32 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 06:48:32 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 06:48:32 PM n: pooler.dense.weight
06/27 06:48:32 PM n: pooler.dense.bias
06/27 06:48:32 PM n: roberta.embeddings.word_embeddings.weight
06/27 06:48:32 PM n: roberta.embeddings.position_embeddings.weight
06/27 06:48:32 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 06:48:32 PM n: roberta.embeddings.LayerNorm.weight
06/27 06:48:32 PM n: roberta.embeddings.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 06:48:32 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 06:48:32 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 06:48:32 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 06:48:32 PM n: roberta.pooler.dense.weight
06/27 06:48:32 PM n: roberta.pooler.dense.bias
06/27 06:48:32 PM n: lm_head.bias
06/27 06:48:32 PM n: lm_head.dense.weight
06/27 06:48:32 PM n: lm_head.dense.bias
06/27 06:48:32 PM n: lm_head.layer_norm.weight
06/27 06:48:32 PM n: lm_head.layer_norm.bias
06/27 06:48:32 PM n: lm_head.decoder.weight
06/27 06:48:32 PM Total parameters: 763292761
06/27 06:48:33 PM ***** LOSS printing *****
06/27 06:48:33 PM loss
06/27 06:48:33 PM tensor(19.8255, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:48:33 PM ***** LOSS printing *****
06/27 06:48:33 PM loss
06/27 06:48:33 PM tensor(14.9663, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:48:33 PM ***** LOSS printing *****
06/27 06:48:33 PM loss
06/27 06:48:33 PM tensor(9.9286, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:48:33 PM ***** LOSS printing *****
06/27 06:48:33 PM loss
06/27 06:48:33 PM tensor(7.1228, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:48:33 PM ***** Running evaluation MLM *****
06/27 06:48:33 PM   Epoch = 0 iter 4 step
06/27 06:48:33 PM   Num examples = 16
06/27 06:48:33 PM   Batch size = 32
06/27 06:48:34 PM ***** Eval results *****
06/27 06:48:34 PM   acc = 0.5
06/27 06:48:34 PM   cls_loss = 12.960798501968384
06/27 06:48:34 PM   eval_loss = 3.5859375
06/27 06:48:34 PM   global_step = 4
06/27 06:48:34 PM   loss = 12.960798501968384
06/27 06:48:34 PM ***** Save model *****
06/27 06:48:34 PM ***** Test Dataset Eval Result *****
06/27 06:49:37 PM ***** Eval results *****
06/27 06:49:37 PM   acc = 0.498
06/27 06:49:37 PM   cls_loss = 12.960798501968384
06/27 06:49:37 PM   eval_loss = 3.8669449299100846
06/27 06:49:37 PM   global_step = 4
06/27 06:49:37 PM   loss = 12.960798501968384
06/27 06:49:42 PM ***** LOSS printing *****
06/27 06:49:42 PM loss
06/27 06:49:42 PM tensor(5.9625, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:49:42 PM ***** LOSS printing *****
06/27 06:49:42 PM loss
06/27 06:49:42 PM tensor(3.2619, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:49:42 PM ***** LOSS printing *****
06/27 06:49:42 PM loss
06/27 06:49:42 PM tensor(2.8849, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:49:43 PM ***** LOSS printing *****
06/27 06:49:43 PM loss
06/27 06:49:43 PM tensor(5.2838, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:49:43 PM ***** LOSS printing *****
06/27 06:49:43 PM loss
06/27 06:49:43 PM tensor(3.9533, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:49:43 PM ***** Running evaluation MLM *****
06/27 06:49:43 PM   Epoch = 0 iter 9 step
06/27 06:49:43 PM   Num examples = 16
06/27 06:49:43 PM   Batch size = 32
06/27 06:49:43 PM ***** Eval results *****
06/27 06:49:43 PM   acc = 0.75
06/27 06:49:43 PM   cls_loss = 8.132170041402182
06/27 06:49:43 PM   eval_loss = 1.9863535165786743
06/27 06:49:43 PM   global_step = 9
06/27 06:49:43 PM   loss = 8.132170041402182
06/27 06:49:43 PM ***** Save model *****
06/27 06:49:43 PM ***** Test Dataset Eval Result *****
06/27 06:50:47 PM ***** Eval results *****
06/27 06:50:47 PM   acc = 0.7085
06/27 06:50:47 PM   cls_loss = 8.132170041402182
06/27 06:50:47 PM   eval_loss = 2.102479915770273
06/27 06:50:47 PM   global_step = 9
06/27 06:50:47 PM   loss = 8.132170041402182
06/27 06:50:50 PM ***** LOSS printing *****
06/27 06:50:50 PM loss
06/27 06:50:50 PM tensor(3.0853, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:51 PM ***** LOSS printing *****
06/27 06:50:51 PM loss
06/27 06:50:51 PM tensor(2.6623, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:51 PM ***** LOSS printing *****
06/27 06:50:51 PM loss
06/27 06:50:51 PM tensor(4.7817, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:51 PM ***** LOSS printing *****
06/27 06:50:51 PM loss
06/27 06:50:51 PM tensor(1.4554, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:51 PM ***** LOSS printing *****
06/27 06:50:51 PM loss
06/27 06:50:51 PM tensor(1.9362, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:51 PM ***** Running evaluation MLM *****
06/27 06:50:51 PM   Epoch = 1 iter 14 step
06/27 06:50:51 PM   Num examples = 16
06/27 06:50:51 PM   Batch size = 32
06/27 06:50:52 PM ***** Eval results *****
06/27 06:50:52 PM   acc = 0.5625
06/27 06:50:52 PM   cls_loss = 1.695805311203003
06/27 06:50:52 PM   eval_loss = 2.465200185775757
06/27 06:50:52 PM   global_step = 14
06/27 06:50:52 PM   loss = 1.695805311203003
06/27 06:50:52 PM ***** LOSS printing *****
06/27 06:50:52 PM loss
06/27 06:50:52 PM tensor(1.2919, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:52 PM ***** LOSS printing *****
06/27 06:50:52 PM loss
06/27 06:50:52 PM tensor(2.4395, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:52 PM ***** LOSS printing *****
06/27 06:50:52 PM loss
06/27 06:50:52 PM tensor(3.6330, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:53 PM ***** LOSS printing *****
06/27 06:50:53 PM loss
06/27 06:50:53 PM tensor(2.9570, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:53 PM ***** LOSS printing *****
06/27 06:50:53 PM loss
06/27 06:50:53 PM tensor(2.9785, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:50:53 PM ***** Running evaluation MLM *****
06/27 06:50:53 PM   Epoch = 1 iter 19 step
06/27 06:50:53 PM   Num examples = 16
06/27 06:50:53 PM   Batch size = 32
06/27 06:50:53 PM ***** Eval results *****
06/27 06:50:53 PM   acc = 0.8125
06/27 06:50:53 PM   cls_loss = 2.3845155579703197
06/27 06:50:53 PM   eval_loss = 2.5154924392700195
06/27 06:50:53 PM   global_step = 19
06/27 06:50:53 PM   loss = 2.3845155579703197
06/27 06:50:53 PM ***** Save model *****
06/27 06:50:53 PM ***** Test Dataset Eval Result *****
06/27 06:51:56 PM ***** Eval results *****
06/27 06:51:56 PM   acc = 0.653
06/27 06:51:56 PM   cls_loss = 2.3845155579703197
06/27 06:51:56 PM   eval_loss = 2.514149588251871
06/27 06:51:56 PM   global_step = 19
06/27 06:51:56 PM   loss = 2.3845155579703197
06/27 06:52:00 PM ***** LOSS printing *****
06/27 06:52:00 PM loss
06/27 06:52:00 PM tensor(1.7983, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:52:00 PM ***** LOSS printing *****
06/27 06:52:00 PM loss
06/27 06:52:00 PM tensor(2.7916, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:52:01 PM ***** LOSS printing *****
06/27 06:52:01 PM loss
06/27 06:52:01 PM tensor(2.6147, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:52:01 PM ***** LOSS printing *****
06/27 06:52:01 PM loss
06/27 06:52:01 PM tensor(1.8654, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:52:01 PM ***** LOSS printing *****
06/27 06:52:01 PM loss
06/27 06:52:01 PM tensor(1.8270, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:52:01 PM ***** Running evaluation MLM *****
06/27 06:52:01 PM   Epoch = 1 iter 24 step
06/27 06:52:01 PM   Num examples = 16
06/27 06:52:01 PM   Batch size = 32
06/27 06:52:02 PM ***** Eval results *****
06/27 06:52:02 PM   acc = 0.9375
06/27 06:52:02 PM   cls_loss = 2.2990527550379434
06/27 06:52:02 PM   eval_loss = 1.2404255867004395
06/27 06:52:02 PM   global_step = 24
06/27 06:52:02 PM   loss = 2.2990527550379434
06/27 06:52:02 PM ***** Save model *****
06/27 06:52:02 PM ***** Test Dataset Eval Result *****
06/27 06:53:05 PM ***** Eval results *****
06/27 06:53:05 PM   acc = 0.79
06/27 06:53:05 PM   cls_loss = 2.2990527550379434
06/27 06:53:05 PM   eval_loss = 1.2898302532377697
06/27 06:53:05 PM   global_step = 24
06/27 06:53:05 PM   loss = 2.2990527550379434
06/27 06:53:09 PM ***** LOSS printing *****
06/27 06:53:09 PM loss
06/27 06:53:09 PM tensor(1.5426, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:53:09 PM ***** LOSS printing *****
06/27 06:53:09 PM loss
06/27 06:53:09 PM tensor(1.7680, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:53:09 PM ***** LOSS printing *****
06/27 06:53:09 PM loss
06/27 06:53:09 PM tensor(1.7554, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:53:09 PM ***** LOSS printing *****
06/27 06:53:09 PM loss
06/27 06:53:09 PM tensor(2.6348, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:53:10 PM ***** LOSS printing *****
06/27 06:53:10 PM loss
06/27 06:53:10 PM tensor(2.2226, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:53:10 PM ***** Running evaluation MLM *****
06/27 06:53:10 PM   Epoch = 2 iter 29 step
06/27 06:53:10 PM   Num examples = 16
06/27 06:53:10 PM   Batch size = 32
06/27 06:53:10 PM ***** Eval results *****
06/27 06:53:10 PM   acc = 1.0
06/27 06:53:10 PM   cls_loss = 1.984689426422119
06/27 06:53:10 PM   eval_loss = 1.596038818359375
06/27 06:53:10 PM   global_step = 29
06/27 06:53:10 PM   loss = 1.984689426422119
06/27 06:53:10 PM ***** Save model *****
06/27 06:53:10 PM ***** Test Dataset Eval Result *****
06/27 06:54:14 PM ***** Eval results *****
06/27 06:54:14 PM   acc = 0.8725
06/27 06:54:14 PM   cls_loss = 1.984689426422119
06/27 06:54:14 PM   eval_loss = 1.6100494710225908
06/27 06:54:14 PM   global_step = 29
06/27 06:54:14 PM   loss = 1.984689426422119
06/27 06:54:18 PM ***** LOSS printing *****
06/27 06:54:18 PM loss
06/27 06:54:18 PM tensor(0.9976, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:18 PM ***** LOSS printing *****
06/27 06:54:18 PM loss
06/27 06:54:18 PM tensor(1.7065, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:19 PM ***** LOSS printing *****
06/27 06:54:19 PM loss
06/27 06:54:19 PM tensor(2.5251, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:19 PM ***** LOSS printing *****
06/27 06:54:19 PM loss
06/27 06:54:19 PM tensor(1.5460, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:19 PM ***** LOSS printing *****
06/27 06:54:19 PM loss
06/27 06:54:19 PM tensor(2.3820, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:19 PM ***** Running evaluation MLM *****
06/27 06:54:19 PM   Epoch = 2 iter 34 step
06/27 06:54:19 PM   Num examples = 16
06/27 06:54:19 PM   Batch size = 32
06/27 06:54:20 PM ***** Eval results *****
06/27 06:54:20 PM   acc = 1.0
06/27 06:54:20 PM   cls_loss = 1.9080623269081116
06/27 06:54:20 PM   eval_loss = 1.7893657684326172
06/27 06:54:20 PM   global_step = 34
06/27 06:54:20 PM   loss = 1.9080623269081116
06/27 06:54:20 PM ***** LOSS printing *****
06/27 06:54:20 PM loss
06/27 06:54:20 PM tensor(1.6140, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:20 PM ***** LOSS printing *****
06/27 06:54:20 PM loss
06/27 06:54:20 PM tensor(1.7296, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:20 PM ***** LOSS printing *****
06/27 06:54:20 PM loss
06/27 06:54:20 PM tensor(0.9371, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:20 PM ***** LOSS printing *****
06/27 06:54:20 PM loss
06/27 06:54:20 PM tensor(1.6866, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:21 PM ***** LOSS printing *****
06/27 06:54:21 PM loss
06/27 06:54:21 PM tensor(1.2329, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:21 PM ***** Running evaluation MLM *****
06/27 06:54:21 PM   Epoch = 3 iter 39 step
06/27 06:54:21 PM   Num examples = 16
06/27 06:54:21 PM   Batch size = 32
06/27 06:54:21 PM ***** Eval results *****
06/27 06:54:21 PM   acc = 1.0
06/27 06:54:21 PM   cls_loss = 1.2855320572853088
06/27 06:54:21 PM   eval_loss = 1.886191725730896
06/27 06:54:21 PM   global_step = 39
06/27 06:54:21 PM   loss = 1.2855320572853088
06/27 06:54:21 PM ***** LOSS printing *****
06/27 06:54:21 PM loss
06/27 06:54:21 PM tensor(1.5779, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:22 PM ***** LOSS printing *****
06/27 06:54:22 PM loss
06/27 06:54:22 PM tensor(1.9091, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:22 PM ***** LOSS printing *****
06/27 06:54:22 PM loss
06/27 06:54:22 PM tensor(1.4105, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:22 PM ***** LOSS printing *****
06/27 06:54:22 PM loss
06/27 06:54:22 PM tensor(1.3099, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:22 PM ***** LOSS printing *****
06/27 06:54:22 PM loss
06/27 06:54:22 PM tensor(1.3296, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:22 PM ***** Running evaluation MLM *****
06/27 06:54:22 PM   Epoch = 3 iter 44 step
06/27 06:54:22 PM   Num examples = 16
06/27 06:54:22 PM   Batch size = 32
06/27 06:54:23 PM ***** Eval results *****
06/27 06:54:23 PM   acc = 0.9375
06/27 06:54:23 PM   cls_loss = 1.4241813942790031
06/27 06:54:23 PM   eval_loss = 0.8749520182609558
06/27 06:54:23 PM   global_step = 44
06/27 06:54:23 PM   loss = 1.4241813942790031
06/27 06:54:23 PM ***** LOSS printing *****
06/27 06:54:23 PM loss
06/27 06:54:23 PM tensor(1.3854, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:23 PM ***** LOSS printing *****
06/27 06:54:23 PM loss
06/27 06:54:23 PM tensor(1.2384, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:23 PM ***** LOSS printing *****
06/27 06:54:23 PM loss
06/27 06:54:23 PM tensor(2.0834, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:24 PM ***** LOSS printing *****
06/27 06:54:24 PM loss
06/27 06:54:24 PM tensor(1.3616, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:24 PM ***** LOSS printing *****
06/27 06:54:24 PM loss
06/27 06:54:24 PM tensor(1.1037, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:24 PM ***** Running evaluation MLM *****
06/27 06:54:24 PM   Epoch = 4 iter 49 step
06/27 06:54:24 PM   Num examples = 16
06/27 06:54:24 PM   Batch size = 32
06/27 06:54:24 PM ***** Eval results *****
06/27 06:54:24 PM   acc = 0.9375
06/27 06:54:24 PM   cls_loss = 1.1037483215332031
06/27 06:54:24 PM   eval_loss = 2.077449321746826
06/27 06:54:24 PM   global_step = 49
06/27 06:54:24 PM   loss = 1.1037483215332031
06/27 06:54:25 PM ***** LOSS printing *****
06/27 06:54:25 PM loss
06/27 06:54:25 PM tensor(1.3238, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:25 PM ***** LOSS printing *****
06/27 06:54:25 PM loss
06/27 06:54:25 PM tensor(1.1945, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:25 PM ***** LOSS printing *****
06/27 06:54:25 PM loss
06/27 06:54:25 PM tensor(1.6493, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:25 PM ***** LOSS printing *****
06/27 06:54:25 PM loss
06/27 06:54:25 PM tensor(1.7754, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:25 PM ***** LOSS printing *****
06/27 06:54:25 PM loss
06/27 06:54:25 PM tensor(1.4071, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:26 PM ***** Running evaluation MLM *****
06/27 06:54:26 PM   Epoch = 4 iter 54 step
06/27 06:54:26 PM   Num examples = 16
06/27 06:54:26 PM   Batch size = 32
06/27 06:54:26 PM ***** Eval results *****
06/27 06:54:26 PM   acc = 1.0
06/27 06:54:26 PM   cls_loss = 1.408962110678355
06/27 06:54:26 PM   eval_loss = 1.3679745197296143
06/27 06:54:26 PM   global_step = 54
06/27 06:54:26 PM   loss = 1.408962110678355
06/27 06:54:26 PM ***** LOSS printing *****
06/27 06:54:26 PM loss
06/27 06:54:26 PM tensor(1.2953, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:26 PM ***** LOSS printing *****
06/27 06:54:26 PM loss
06/27 06:54:26 PM tensor(1.1785, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:27 PM ***** LOSS printing *****
06/27 06:54:27 PM loss
06/27 06:54:27 PM tensor(1.8012, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:27 PM ***** LOSS printing *****
06/27 06:54:27 PM loss
06/27 06:54:27 PM tensor(1.6962, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:27 PM ***** LOSS printing *****
06/27 06:54:27 PM loss
06/27 06:54:27 PM tensor(1.7888, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:27 PM ***** Running evaluation MLM *****
06/27 06:54:27 PM   Epoch = 4 iter 59 step
06/27 06:54:27 PM   Num examples = 16
06/27 06:54:27 PM   Batch size = 32
06/27 06:54:28 PM ***** Eval results *****
06/27 06:54:28 PM   acc = 1.0
06/27 06:54:28 PM   cls_loss = 1.4739789420908147
06/27 06:54:28 PM   eval_loss = 0.9383952617645264
06/27 06:54:28 PM   global_step = 59
06/27 06:54:28 PM   loss = 1.4739789420908147
06/27 06:54:28 PM ***** LOSS printing *****
06/27 06:54:28 PM loss
06/27 06:54:28 PM tensor(1.9792, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:28 PM ***** LOSS printing *****
06/27 06:54:28 PM loss
06/27 06:54:28 PM tensor(1.0846, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:28 PM ***** LOSS printing *****
06/27 06:54:28 PM loss
06/27 06:54:28 PM tensor(1.0004, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:28 PM ***** LOSS printing *****
06/27 06:54:28 PM loss
06/27 06:54:28 PM tensor(1.9229, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:29 PM ***** LOSS printing *****
06/27 06:54:29 PM loss
06/27 06:54:29 PM tensor(2.1507, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:29 PM ***** Running evaluation MLM *****
06/27 06:54:29 PM   Epoch = 5 iter 64 step
06/27 06:54:29 PM   Num examples = 16
06/27 06:54:29 PM   Batch size = 32
06/27 06:54:29 PM ***** Eval results *****
06/27 06:54:29 PM   acc = 1.0
06/27 06:54:29 PM   cls_loss = 1.539634883403778
06/27 06:54:29 PM   eval_loss = 0.9063302278518677
06/27 06:54:29 PM   global_step = 64
06/27 06:54:29 PM   loss = 1.539634883403778
06/27 06:54:29 PM ***** LOSS printing *****
06/27 06:54:29 PM loss
06/27 06:54:29 PM tensor(1.0143, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:30 PM ***** LOSS printing *****
06/27 06:54:30 PM loss
06/27 06:54:30 PM tensor(2.2082, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:30 PM ***** LOSS printing *****
06/27 06:54:30 PM loss
06/27 06:54:30 PM tensor(1.4973, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:30 PM ***** LOSS printing *****
06/27 06:54:30 PM loss
06/27 06:54:30 PM tensor(1.7174, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:30 PM ***** LOSS printing *****
06/27 06:54:30 PM loss
06/27 06:54:30 PM tensor(1.4592, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:30 PM ***** Running evaluation MLM *****
06/27 06:54:30 PM   Epoch = 5 iter 69 step
06/27 06:54:30 PM   Num examples = 16
06/27 06:54:30 PM   Batch size = 32
06/27 06:54:31 PM ***** Eval results *****
06/27 06:54:31 PM   acc = 0.9375
06/27 06:54:31 PM   cls_loss = 1.561656700240241
06/27 06:54:31 PM   eval_loss = 0.9878700971603394
06/27 06:54:31 PM   global_step = 69
06/27 06:54:31 PM   loss = 1.561656700240241
06/27 06:54:31 PM ***** LOSS printing *****
06/27 06:54:31 PM loss
06/27 06:54:31 PM tensor(1.5740, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:31 PM ***** LOSS printing *****
06/27 06:54:31 PM loss
06/27 06:54:31 PM tensor(1.2231, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:31 PM ***** LOSS printing *****
06/27 06:54:31 PM loss
06/27 06:54:31 PM tensor(1.6546, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:32 PM ***** LOSS printing *****
06/27 06:54:32 PM loss
06/27 06:54:32 PM tensor(1.2140, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:32 PM ***** LOSS printing *****
06/27 06:54:32 PM loss
06/27 06:54:32 PM tensor(2.1593, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:32 PM ***** Running evaluation MLM *****
06/27 06:54:32 PM   Epoch = 6 iter 74 step
06/27 06:54:32 PM   Num examples = 16
06/27 06:54:32 PM   Batch size = 32
06/27 06:54:33 PM ***** Eval results *****
06/27 06:54:33 PM   acc = 1.0
06/27 06:54:33 PM   cls_loss = 1.686692714691162
06/27 06:54:33 PM   eval_loss = 1.9584240913391113
06/27 06:54:33 PM   global_step = 74
06/27 06:54:33 PM   loss = 1.686692714691162
06/27 06:54:33 PM ***** LOSS printing *****
06/27 06:54:33 PM loss
06/27 06:54:33 PM tensor(1.4153, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:33 PM ***** LOSS printing *****
06/27 06:54:33 PM loss
06/27 06:54:33 PM tensor(1.1508, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:33 PM ***** LOSS printing *****
06/27 06:54:33 PM loss
06/27 06:54:33 PM tensor(0.9136, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:33 PM ***** LOSS printing *****
06/27 06:54:33 PM loss
06/27 06:54:33 PM tensor(1.2090, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:33 PM ***** LOSS printing *****
06/27 06:54:33 PM loss
06/27 06:54:33 PM tensor(1.0487, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:34 PM ***** Running evaluation MLM *****
06/27 06:54:34 PM   Epoch = 6 iter 79 step
06/27 06:54:34 PM   Num examples = 16
06/27 06:54:34 PM   Batch size = 32
06/27 06:54:34 PM ***** Eval results *****
06/27 06:54:34 PM   acc = 1.0
06/27 06:54:34 PM   cls_loss = 1.3015406472342355
06/27 06:54:34 PM   eval_loss = 1.88945734500885
06/27 06:54:34 PM   global_step = 79
06/27 06:54:34 PM   loss = 1.3015406472342355
06/27 06:54:34 PM ***** LOSS printing *****
06/27 06:54:34 PM loss
06/27 06:54:34 PM tensor(1.2964, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:34 PM ***** LOSS printing *****
06/27 06:54:34 PM loss
06/27 06:54:34 PM tensor(2.5148, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:35 PM ***** LOSS printing *****
06/27 06:54:35 PM loss
06/27 06:54:35 PM tensor(1.6890, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:35 PM ***** LOSS printing *****
06/27 06:54:35 PM loss
06/27 06:54:35 PM tensor(1.7960, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:35 PM ***** LOSS printing *****
06/27 06:54:35 PM loss
06/27 06:54:35 PM tensor(1.5427, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:35 PM ***** Running evaluation MLM *****
06/27 06:54:35 PM   Epoch = 6 iter 84 step
06/27 06:54:35 PM   Num examples = 16
06/27 06:54:35 PM   Batch size = 32
06/27 06:54:36 PM ***** Eval results *****
06/27 06:54:36 PM   acc = 1.0
06/27 06:54:36 PM   cls_loss = 1.4958179692427318
06/27 06:54:36 PM   eval_loss = 1.294750690460205
06/27 06:54:36 PM   global_step = 84
06/27 06:54:36 PM   loss = 1.4958179692427318
06/27 06:54:36 PM ***** LOSS printing *****
06/27 06:54:36 PM loss
06/27 06:54:36 PM tensor(1.2110, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:36 PM ***** LOSS printing *****
06/27 06:54:36 PM loss
06/27 06:54:36 PM tensor(1.0588, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:36 PM ***** LOSS printing *****
06/27 06:54:36 PM loss
06/27 06:54:36 PM tensor(1.3598, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:36 PM ***** LOSS printing *****
06/27 06:54:36 PM loss
06/27 06:54:36 PM tensor(1.4009, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:37 PM ***** LOSS printing *****
06/27 06:54:37 PM loss
06/27 06:54:37 PM tensor(1.7097, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:37 PM ***** Running evaluation MLM *****
06/27 06:54:37 PM   Epoch = 7 iter 89 step
06/27 06:54:37 PM   Num examples = 16
06/27 06:54:37 PM   Batch size = 32
06/27 06:54:37 PM ***** Eval results *****
06/27 06:54:37 PM   acc = 1.0
06/27 06:54:37 PM   cls_loss = 1.3480406522750854
06/27 06:54:37 PM   eval_loss = 0.4011456072330475
06/27 06:54:37 PM   global_step = 89
06/27 06:54:37 PM   loss = 1.3480406522750854
06/27 06:54:37 PM ***** LOSS printing *****
06/27 06:54:37 PM loss
06/27 06:54:37 PM tensor(1.9506, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:38 PM ***** LOSS printing *****
06/27 06:54:38 PM loss
06/27 06:54:38 PM tensor(1.5089, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:38 PM ***** LOSS printing *****
06/27 06:54:38 PM loss
06/27 06:54:38 PM tensor(1.4005, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:38 PM ***** LOSS printing *****
06/27 06:54:38 PM loss
06/27 06:54:38 PM tensor(0.9175, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:38 PM ***** LOSS printing *****
06/27 06:54:38 PM loss
06/27 06:54:38 PM tensor(1.4340, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:38 PM ***** Running evaluation MLM *****
06/27 06:54:38 PM   Epoch = 7 iter 94 step
06/27 06:54:38 PM   Num examples = 16
06/27 06:54:38 PM   Batch size = 32
06/27 06:54:39 PM ***** Eval results *****
06/27 06:54:39 PM   acc = 1.0
06/27 06:54:39 PM   cls_loss = 1.3951757669448852
06/27 06:54:39 PM   eval_loss = 0.849025309085846
06/27 06:54:39 PM   global_step = 94
06/27 06:54:39 PM   loss = 1.3951757669448852
06/27 06:54:39 PM ***** LOSS printing *****
06/27 06:54:39 PM loss
06/27 06:54:39 PM tensor(1.7475, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:39 PM ***** LOSS printing *****
06/27 06:54:39 PM loss
06/27 06:54:39 PM tensor(1.2810, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:39 PM ***** LOSS printing *****
06/27 06:54:39 PM loss
06/27 06:54:39 PM tensor(1.2873, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:40 PM ***** LOSS printing *****
06/27 06:54:40 PM loss
06/27 06:54:40 PM tensor(1.2134, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:40 PM ***** LOSS printing *****
06/27 06:54:40 PM loss
06/27 06:54:40 PM tensor(1.0193, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:40 PM ***** Running evaluation MLM *****
06/27 06:54:40 PM   Epoch = 8 iter 99 step
06/27 06:54:40 PM   Num examples = 16
06/27 06:54:40 PM   Batch size = 32
06/27 06:54:41 PM ***** Eval results *****
06/27 06:54:41 PM   acc = 0.9375
06/27 06:54:41 PM   cls_loss = 1.1733677784601848
06/27 06:54:41 PM   eval_loss = 1.3616845607757568
06/27 06:54:41 PM   global_step = 99
06/27 06:54:41 PM   loss = 1.1733677784601848
06/27 06:54:41 PM ***** LOSS printing *****
06/27 06:54:41 PM loss
06/27 06:54:41 PM tensor(1.6128, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:41 PM ***** LOSS printing *****
06/27 06:54:41 PM loss
06/27 06:54:41 PM tensor(1.6190, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:41 PM ***** LOSS printing *****
06/27 06:54:41 PM loss
06/27 06:54:41 PM tensor(1.1574, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:41 PM ***** LOSS printing *****
06/27 06:54:41 PM loss
06/27 06:54:41 PM tensor(1.5815, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:42 PM ***** LOSS printing *****
06/27 06:54:42 PM loss
06/27 06:54:42 PM tensor(1.4037, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:42 PM ***** Running evaluation MLM *****
06/27 06:54:42 PM   Epoch = 8 iter 104 step
06/27 06:54:42 PM   Num examples = 16
06/27 06:54:42 PM   Batch size = 32
06/27 06:54:42 PM ***** Eval results *****
06/27 06:54:42 PM   acc = 0.9375
06/27 06:54:42 PM   cls_loss = 1.3618124723434448
06/27 06:54:42 PM   eval_loss = 1.290891408920288
06/27 06:54:42 PM   global_step = 104
06/27 06:54:42 PM   loss = 1.3618124723434448
06/27 06:54:42 PM ***** LOSS printing *****
06/27 06:54:42 PM loss
06/27 06:54:42 PM tensor(1.5287, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:42 PM ***** LOSS printing *****
06/27 06:54:42 PM loss
06/27 06:54:42 PM tensor(0.9107, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:43 PM ***** LOSS printing *****
06/27 06:54:43 PM loss
06/27 06:54:43 PM tensor(1.1550, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:43 PM ***** LOSS printing *****
06/27 06:54:43 PM loss
06/27 06:54:43 PM tensor(1.4425, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:43 PM ***** LOSS printing *****
06/27 06:54:43 PM loss
06/27 06:54:43 PM tensor(0.9061, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:43 PM ***** Running evaluation MLM *****
06/27 06:54:43 PM   Epoch = 9 iter 109 step
06/27 06:54:43 PM   Num examples = 16
06/27 06:54:43 PM   Batch size = 32
06/27 06:54:44 PM ***** Eval results *****
06/27 06:54:44 PM   acc = 0.9375
06/27 06:54:44 PM   cls_loss = 0.9061315059661865
06/27 06:54:44 PM   eval_loss = 1.4846150875091553
06/27 06:54:44 PM   global_step = 109
06/27 06:54:44 PM   loss = 0.9061315059661865
06/27 06:54:44 PM ***** LOSS printing *****
06/27 06:54:44 PM loss
06/27 06:54:44 PM tensor(1.6786, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:44 PM ***** LOSS printing *****
06/27 06:54:44 PM loss
06/27 06:54:44 PM tensor(1.5716, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:44 PM ***** LOSS printing *****
06/27 06:54:44 PM loss
06/27 06:54:44 PM tensor(1.2618, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:45 PM ***** LOSS printing *****
06/27 06:54:45 PM loss
06/27 06:54:45 PM tensor(1.4895, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:45 PM ***** LOSS printing *****
06/27 06:54:45 PM loss
06/27 06:54:45 PM tensor(1.0300, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:45 PM ***** Running evaluation MLM *****
06/27 06:54:45 PM   Epoch = 9 iter 114 step
06/27 06:54:45 PM   Num examples = 16
06/27 06:54:45 PM   Batch size = 32
06/27 06:54:45 PM ***** Eval results *****
06/27 06:54:45 PM   acc = 0.9375
06/27 06:54:45 PM   cls_loss = 1.3229421575864155
06/27 06:54:45 PM   eval_loss = 0.878131628036499
06/27 06:54:45 PM   global_step = 114
06/27 06:54:45 PM   loss = 1.3229421575864155
06/27 06:54:45 PM ***** LOSS printing *****
06/27 06:54:45 PM loss
06/27 06:54:45 PM tensor(1.0089, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:46 PM ***** LOSS printing *****
06/27 06:54:46 PM loss
06/27 06:54:46 PM tensor(1.5030, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:46 PM ***** LOSS printing *****
06/27 06:54:46 PM loss
06/27 06:54:46 PM tensor(0.8737, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:46 PM ***** LOSS printing *****
06/27 06:54:46 PM loss
06/27 06:54:46 PM tensor(1.7740, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:46 PM ***** LOSS printing *****
06/27 06:54:46 PM loss
06/27 06:54:46 PM tensor(1.4108, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 06:54:47 PM ***** Running evaluation MLM *****
06/27 06:54:47 PM   Epoch = 9 iter 119 step
06/27 06:54:47 PM   Num examples = 16
06/27 06:54:47 PM   Batch size = 32
06/27 06:54:47 PM ***** Eval results *****
06/27 06:54:47 PM   acc = 0.9375
06/27 06:54:47 PM   cls_loss = 1.318900460546667
06/27 06:54:47 PM   eval_loss = 0.9500975608825684
06/27 06:54:47 PM   global_step = 119
06/27 06:54:47 PM   loss = 1.318900460546667
06/27 06:54:47 PM ***** LOSS printing *****
06/27 06:54:47 PM loss
06/27 06:54:47 PM tensor(2.4078, device='cuda:0', grad_fn=<NllLossBackward0>)

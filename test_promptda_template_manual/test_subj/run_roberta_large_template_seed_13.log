06/27 07:04:24 PM The args: Namespace(TD_alternate_1=False, TD_alternate_2=False, TD_alternate_3=False, TD_alternate_4=False, TD_alternate_5=False, TD_alternate_6=False, TD_alternate_7=False, TD_alternate_feature_distill=False, TD_alternate_feature_epochs=10, TD_alternate_last_layer_mapping=False, TD_alternate_prediction=False, TD_alternate_prediction_distill=False, TD_alternate_prediction_epochs=3, TD_alternate_uniform_layer_mapping=False, TD_baseline=False, TD_baseline_feature_att_epochs=10, TD_baseline_feature_epochs=13, TD_baseline_feature_repre_epochs=3, TD_baseline_prediction_epochs=3, TD_fine_tune_mlm=False, TD_fine_tune_normal=False, TD_one_step=False, TD_three_step=False, TD_three_step_att_distill=False, TD_three_step_att_pairwise_distill=False, TD_three_step_prediction_distill=False, TD_three_step_repre_distill=False, TD_two_step=False, aug_train=False, beta=0.001, cache_dir='', data_dir='../../data/k-shot/subj/8-13/', data_seed=13, data_url='', dataset_num=8, do_eval=False, do_eval_mlm=False, do_lower_case=True, eval_batch_size=32, eval_step=5, gradient_accumulation_steps=1, init_method='', learning_rate=3e-06, max_seq_length=128, no_cuda=False, num_train_epochs=10.0, only_one_layer_mapping=False, ood_data_dir=None, ood_eval=False, ood_max_seq_length=128, ood_task_name=None, output_dir='../../output/dataset_num_8_fine_tune_mlm_few_shot_3_label_word_batch_size_4_bert-large-uncased/', pred_distill=False, pred_distill_multi_loss=False, seed=42, student_model='../../data/model/roberta-large/', task_name='subj', teacher_model=None, temperature=1.0, train_batch_size=4, train_url='', use_CLS=False, warmup_proportion=0.1, weight_decay=0.0001)
06/27 07:04:24 PM device: cuda n_gpu: 1
06/27 07:04:24 PM Writing example 0 of 48
06/27 07:04:24 PM *** Example ***
06/27 07:04:24 PM guid: train-1
06/27 07:04:24 PM tokens: <s> this Ġis Ġthe Ġstory Ġof Ġnat Ġbanks Ġ, Ġan Ġ8 th Ġgeneration Ġvirgin ian Ġgentleman Ġfarmer Ġliving Ġin Ġthe Ġpast Ġ, Ġwho Ġloses Ġhis Ġfamily Ġfarm Ġ, Ġgreen wood Ġ, Ġto Ġa Ġpair Ġof Ġland Ġspec ulators Ġfrom Ġwashing ton Ġ, Ġd Ġ. Ġc Ġ. </s> ĠIt Ġis <mask>
06/27 07:04:24 PM input_ids: 0 9226 16 5 527 9 23577 1520 2156 41 290 212 2706 33799 811 22164 10305 1207 11 5 375 2156 54 13585 39 284 3380 2156 2272 1845 2156 7 10 1763 9 1212 12002 17810 31 14784 1054 2156 385 479 740 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:24 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:24 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:24 PM label: ['Ġgood']
06/27 07:04:24 PM Writing example 0 of 16
06/27 07:04:24 PM *** Example ***
06/27 07:04:24 PM guid: dev-1
06/27 07:04:24 PM tokens: <s> he Ġis Ġstill Ġfamous Ġ, Ġalthough Ġstill Ġdisliked Ġby Ġsn ape Ġ, Ġmalf oy Ġ, Ġand Ġthe Ġrest Ġof Ġthe Ġsly ther ins Ġ. </s> ĠIt Ġis <mask>
06/27 07:04:24 PM input_ids: 0 700 16 202 3395 2156 1712 202 40891 30 4543 5776 2156 36432 2160 2156 8 5 1079 9 5 40568 12968 1344 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:24 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:24 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:24 PM label: ['Ġgood']
06/27 07:04:25 PM Writing example 0 of 2000
06/27 07:04:25 PM *** Example ***
06/27 07:04:25 PM guid: dev-1
06/27 07:04:25 PM tokens: <s> smart Ġand Ġalert Ġ, Ġthirteen Ġconversations Ġabout Ġone Ġthing Ġis Ġa Ġsmall Ġgem Ġ. </s> ĠIt Ġis <mask>
06/27 07:04:25 PM input_ids: 0 22914 8 5439 2156 30361 5475 59 65 631 16 10 650 15538 479 2 85 16 50264 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:25 PM input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:25 PM segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
06/27 07:04:25 PM label: ['Ġbad']
06/27 07:04:38 PM ***** Running training *****
06/27 07:04:38 PM   Num examples = 48
06/27 07:04:38 PM   Batch size = 4
06/27 07:04:38 PM   Num steps = 120
06/27 07:04:38 PM n: embeddings.word_embeddings.weight
06/27 07:04:38 PM n: embeddings.position_embeddings.weight
06/27 07:04:38 PM n: embeddings.token_type_embeddings.weight
06/27 07:04:38 PM n: embeddings.LayerNorm.weight
06/27 07:04:38 PM n: embeddings.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.0.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.0.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.0.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.0.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.0.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.0.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.0.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.0.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.0.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.0.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.0.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.0.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.0.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.0.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.1.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.1.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.1.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.1.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.1.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.1.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.1.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.1.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.1.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.1.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.1.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.1.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.1.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.1.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.2.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.2.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.2.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.2.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.2.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.2.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.2.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.2.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.2.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.2.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.2.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.2.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.2.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.2.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.3.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.3.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.3.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.3.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.3.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.3.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.3.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.3.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.3.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.3.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.3.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.3.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.3.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.3.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.4.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.4.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.4.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.4.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.4.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.4.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.4.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.4.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.4.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.4.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.4.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.4.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.4.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.4.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.5.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.5.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.5.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.5.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.5.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.5.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.5.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.5.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.5.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.5.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.5.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.5.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.5.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.5.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.6.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.6.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.6.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.6.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.6.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.6.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.6.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.6.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.6.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.6.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.6.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.6.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.6.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.6.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.7.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.7.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.7.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.7.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.7.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.7.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.7.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.7.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.7.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.7.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.7.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.7.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.7.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.7.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.8.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.8.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.8.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.8.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.8.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.8.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.8.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.8.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.8.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.8.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.8.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.8.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.8.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.8.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.9.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.9.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.9.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.9.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.9.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.9.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.9.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.9.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.9.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.9.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.9.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.9.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.9.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.9.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.10.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.10.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.10.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.10.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.10.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.10.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.10.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.10.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.10.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.10.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.10.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.10.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.10.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.10.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.11.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.11.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.11.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.11.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.11.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.11.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.11.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.11.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.11.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.11.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.11.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.11.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.11.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.11.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.12.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.12.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.12.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.12.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.12.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.12.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.12.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.12.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.12.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.12.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.12.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.12.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.12.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.12.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.13.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.13.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.13.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.13.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.13.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.13.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.13.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.13.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.13.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.13.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.13.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.13.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.13.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.13.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.14.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.14.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.14.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.14.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.14.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.14.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.14.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.14.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.14.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.14.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.14.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.14.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.14.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.14.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.15.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.15.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.15.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.15.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.15.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.15.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.15.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.15.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.15.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.15.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.15.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.15.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.15.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.15.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.16.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.16.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.16.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.16.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.16.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.16.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.16.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.16.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.16.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.16.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.16.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.16.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.16.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.16.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.17.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.17.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.17.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.17.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.17.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.17.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.17.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.17.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.17.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.17.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.17.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.17.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.17.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.17.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.18.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.18.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.18.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.18.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.18.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.18.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.18.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.18.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.18.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.18.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.18.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.18.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.18.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.18.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.19.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.19.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.19.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.19.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.19.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.19.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.19.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.19.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.19.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.19.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.19.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.19.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.19.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.19.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.20.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.20.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.20.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.20.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.20.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.20.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.20.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.20.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.20.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.20.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.20.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.20.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.20.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.20.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.21.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.21.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.21.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.21.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.21.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.21.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.21.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.21.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.21.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.21.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.21.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.21.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.21.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.21.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.22.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.22.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.22.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.22.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.22.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.22.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.22.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.22.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.22.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.22.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.22.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.22.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.22.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.22.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.23.attention.self.query.weight
06/27 07:04:38 PM n: encoder.layer.23.attention.self.query.bias
06/27 07:04:38 PM n: encoder.layer.23.attention.self.key.weight
06/27 07:04:38 PM n: encoder.layer.23.attention.self.key.bias
06/27 07:04:38 PM n: encoder.layer.23.attention.self.value.weight
06/27 07:04:38 PM n: encoder.layer.23.attention.self.value.bias
06/27 07:04:38 PM n: encoder.layer.23.attention.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.23.attention.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: encoder.layer.23.intermediate.dense.weight
06/27 07:04:38 PM n: encoder.layer.23.intermediate.dense.bias
06/27 07:04:38 PM n: encoder.layer.23.output.dense.weight
06/27 07:04:38 PM n: encoder.layer.23.output.dense.bias
06/27 07:04:38 PM n: encoder.layer.23.output.LayerNorm.weight
06/27 07:04:38 PM n: encoder.layer.23.output.LayerNorm.bias
06/27 07:04:38 PM n: pooler.dense.weight
06/27 07:04:38 PM n: pooler.dense.bias
06/27 07:04:38 PM n: roberta.embeddings.word_embeddings.weight
06/27 07:04:38 PM n: roberta.embeddings.position_embeddings.weight
06/27 07:04:38 PM n: roberta.embeddings.token_type_embeddings.weight
06/27 07:04:38 PM n: roberta.embeddings.LayerNorm.weight
06/27 07:04:38 PM n: roberta.embeddings.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.0.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.0.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.0.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.0.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.0.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.0.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.0.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.1.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.1.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.1.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.1.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.1.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.1.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.1.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.2.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.2.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.2.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.2.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.2.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.2.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.2.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.3.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.3.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.3.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.3.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.3.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.3.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.3.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.4.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.4.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.4.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.4.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.4.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.4.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.4.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.5.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.5.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.5.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.5.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.5.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.5.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.5.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.6.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.6.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.6.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.6.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.6.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.6.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.6.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.7.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.7.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.7.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.7.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.7.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.7.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.7.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.8.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.8.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.8.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.8.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.8.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.8.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.8.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.9.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.9.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.9.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.9.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.9.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.9.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.9.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.10.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.10.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.10.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.10.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.10.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.10.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.10.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.11.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.11.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.11.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.11.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.11.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.11.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.11.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.12.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.12.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.12.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.12.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.12.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.12.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.12.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.13.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.13.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.13.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.13.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.13.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.13.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.13.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.14.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.14.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.14.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.14.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.14.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.14.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.14.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.15.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.15.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.15.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.15.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.15.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.15.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.15.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.16.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.16.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.16.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.16.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.16.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.16.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.16.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.17.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.17.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.17.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.17.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.17.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.17.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.17.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.18.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.18.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.18.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.18.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.18.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.18.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.18.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.19.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.19.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.19.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.19.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.19.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.19.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.19.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.20.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.20.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.20.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.20.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.20.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.20.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.20.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.21.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.21.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.21.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.21.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.21.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.21.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.21.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.22.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.22.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.22.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.22.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.22.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.22.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.22.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.self.query.weight
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.self.query.bias
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.self.key.weight
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.self.key.bias
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.self.value.weight
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.self.value.bias
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.23.attention.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.encoder.layer.23.intermediate.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.23.intermediate.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.23.output.dense.weight
06/27 07:04:38 PM n: roberta.encoder.layer.23.output.dense.bias
06/27 07:04:38 PM n: roberta.encoder.layer.23.output.LayerNorm.weight
06/27 07:04:38 PM n: roberta.encoder.layer.23.output.LayerNorm.bias
06/27 07:04:38 PM n: roberta.pooler.dense.weight
06/27 07:04:38 PM n: roberta.pooler.dense.bias
06/27 07:04:38 PM n: lm_head.bias
06/27 07:04:38 PM n: lm_head.dense.weight
06/27 07:04:38 PM n: lm_head.dense.bias
06/27 07:04:38 PM n: lm_head.layer_norm.weight
06/27 07:04:38 PM n: lm_head.layer_norm.bias
06/27 07:04:38 PM n: lm_head.decoder.weight
06/27 07:04:38 PM Total parameters: 763292761
06/27 07:04:38 PM ***** LOSS printing *****
06/27 07:04:38 PM loss
06/27 07:04:38 PM tensor(22.5767, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:04:38 PM ***** LOSS printing *****
06/27 07:04:38 PM loss
06/27 07:04:38 PM tensor(14.7146, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:04:38 PM ***** LOSS printing *****
06/27 07:04:38 PM loss
06/27 07:04:38 PM tensor(11.9133, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:04:38 PM ***** LOSS printing *****
06/27 07:04:38 PM loss
06/27 07:04:38 PM tensor(7.1776, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:04:39 PM ***** Running evaluation MLM *****
06/27 07:04:39 PM   Epoch = 0 iter 4 step
06/27 07:04:39 PM   Num examples = 16
06/27 07:04:39 PM   Batch size = 32
06/27 07:04:39 PM ***** Eval results *****
06/27 07:04:39 PM   acc = 0.5
06/27 07:04:39 PM   cls_loss = 14.095549464225769
06/27 07:04:39 PM   eval_loss = 4.653552532196045
06/27 07:04:39 PM   global_step = 4
06/27 07:04:39 PM   loss = 14.095549464225769
06/27 07:04:39 PM ***** Save model *****
06/27 07:04:39 PM ***** Test Dataset Eval Result *****
06/27 07:05:42 PM ***** Eval results *****
06/27 07:05:42 PM   acc = 0.4945
06/27 07:05:42 PM   cls_loss = 14.095549464225769
06/27 07:05:42 PM   eval_loss = 4.775189051552425
06/27 07:05:42 PM   global_step = 4
06/27 07:05:42 PM   loss = 14.095549464225769
06/27 07:05:46 PM ***** LOSS printing *****
06/27 07:05:46 PM loss
06/27 07:05:46 PM tensor(4.8729, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:46 PM ***** LOSS printing *****
06/27 07:05:46 PM loss
06/27 07:05:46 PM tensor(4.4099, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:47 PM ***** LOSS printing *****
06/27 07:05:47 PM loss
06/27 07:05:47 PM tensor(2.9147, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:47 PM ***** LOSS printing *****
06/27 07:05:47 PM loss
06/27 07:05:47 PM tensor(4.0204, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:47 PM ***** LOSS printing *****
06/27 07:05:47 PM loss
06/27 07:05:47 PM tensor(4.6231, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:47 PM ***** Running evaluation MLM *****
06/27 07:05:47 PM   Epoch = 0 iter 9 step
06/27 07:05:47 PM   Num examples = 16
06/27 07:05:47 PM   Batch size = 32
06/27 07:05:48 PM ***** Eval results *****
06/27 07:05:48 PM   acc = 0.5
06/27 07:05:48 PM   cls_loss = 8.580362187491524
06/27 07:05:48 PM   eval_loss = 1.9536856412887573
06/27 07:05:48 PM   global_step = 9
06/27 07:05:48 PM   loss = 8.580362187491524
06/27 07:05:48 PM ***** LOSS printing *****
06/27 07:05:48 PM loss
06/27 07:05:48 PM tensor(2.9852, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:48 PM ***** LOSS printing *****
06/27 07:05:48 PM loss
06/27 07:05:48 PM tensor(3.7936, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:48 PM ***** LOSS printing *****
06/27 07:05:48 PM loss
06/27 07:05:48 PM tensor(5.2347, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:48 PM ***** LOSS printing *****
06/27 07:05:48 PM loss
06/27 07:05:48 PM tensor(2.5963, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:49 PM ***** LOSS printing *****
06/27 07:05:49 PM loss
06/27 07:05:49 PM tensor(2.9665, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:49 PM ***** Running evaluation MLM *****
06/27 07:05:49 PM   Epoch = 1 iter 14 step
06/27 07:05:49 PM   Num examples = 16
06/27 07:05:49 PM   Batch size = 32
06/27 07:05:49 PM ***** Eval results *****
06/27 07:05:49 PM   acc = 0.5
06/27 07:05:49 PM   cls_loss = 2.7814055681228638
06/27 07:05:49 PM   eval_loss = 1.7781970500946045
06/27 07:05:49 PM   global_step = 14
06/27 07:05:49 PM   loss = 2.7814055681228638
06/27 07:05:49 PM ***** LOSS printing *****
06/27 07:05:49 PM loss
06/27 07:05:49 PM tensor(1.8654, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:50 PM ***** LOSS printing *****
06/27 07:05:50 PM loss
06/27 07:05:50 PM tensor(2.3973, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:50 PM ***** LOSS printing *****
06/27 07:05:50 PM loss
06/27 07:05:50 PM tensor(2.8604, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:50 PM ***** LOSS printing *****
06/27 07:05:50 PM loss
06/27 07:05:50 PM tensor(1.7762, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:50 PM ***** LOSS printing *****
06/27 07:05:50 PM loss
06/27 07:05:50 PM tensor(2.6797, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:05:50 PM ***** Running evaluation MLM *****
06/27 07:05:50 PM   Epoch = 1 iter 19 step
06/27 07:05:50 PM   Num examples = 16
06/27 07:05:50 PM   Batch size = 32
06/27 07:05:51 PM ***** Eval results *****
06/27 07:05:51 PM   acc = 0.5625
06/27 07:05:51 PM   cls_loss = 2.448830417224339
06/27 07:05:51 PM   eval_loss = 2.494476318359375
06/27 07:05:51 PM   global_step = 19
06/27 07:05:51 PM   loss = 2.448830417224339
06/27 07:05:51 PM ***** Save model *****
06/27 07:05:51 PM ***** Test Dataset Eval Result *****
06/27 07:06:53 PM ***** Eval results *****
06/27 07:06:53 PM   acc = 0.5675
06/27 07:06:53 PM   cls_loss = 2.448830417224339
06/27 07:06:53 PM   eval_loss = 2.4519613432505776
06/27 07:06:53 PM   global_step = 19
06/27 07:06:53 PM   loss = 2.448830417224339
06/27 07:06:58 PM ***** LOSS printing *****
06/27 07:06:58 PM loss
06/27 07:06:58 PM tensor(2.0536, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:06:58 PM ***** LOSS printing *****
06/27 07:06:58 PM loss
06/27 07:06:58 PM tensor(2.3427, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:06:58 PM ***** LOSS printing *****
06/27 07:06:58 PM loss
06/27 07:06:58 PM tensor(2.1691, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:06:58 PM ***** LOSS printing *****
06/27 07:06:58 PM loss
06/27 07:06:58 PM tensor(1.8938, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:06:59 PM ***** LOSS printing *****
06/27 07:06:59 PM loss
06/27 07:06:59 PM tensor(3.3984, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:06:59 PM ***** Running evaluation MLM *****
06/27 07:06:59 PM   Epoch = 1 iter 24 step
06/27 07:06:59 PM   Num examples = 16
06/27 07:06:59 PM   Batch size = 32
06/27 07:06:59 PM ***** Eval results *****
06/27 07:06:59 PM   acc = 0.6875
06/27 07:06:59 PM   cls_loss = 2.416609595219294
06/27 07:06:59 PM   eval_loss = 1.4010001420974731
06/27 07:06:59 PM   global_step = 24
06/27 07:06:59 PM   loss = 2.416609595219294
06/27 07:06:59 PM ***** Save model *****
06/27 07:06:59 PM ***** Test Dataset Eval Result *****
06/27 07:08:02 PM ***** Eval results *****
06/27 07:08:02 PM   acc = 0.756
06/27 07:08:02 PM   cls_loss = 2.416609595219294
06/27 07:08:02 PM   eval_loss = 1.3913912205469041
06/27 07:08:02 PM   global_step = 24
06/27 07:08:02 PM   loss = 2.416609595219294
06/27 07:08:06 PM ***** LOSS printing *****
06/27 07:08:06 PM loss
06/27 07:08:06 PM tensor(1.3099, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:06 PM ***** LOSS printing *****
06/27 07:08:06 PM loss
06/27 07:08:06 PM tensor(3.1057, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:06 PM ***** LOSS printing *****
06/27 07:08:06 PM loss
06/27 07:08:06 PM tensor(1.8016, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:07 PM ***** LOSS printing *****
06/27 07:08:07 PM loss
06/27 07:08:07 PM tensor(2.2184, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:07 PM ***** LOSS printing *****
06/27 07:08:07 PM loss
06/27 07:08:07 PM tensor(2.0838, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:07 PM ***** Running evaluation MLM *****
06/27 07:08:07 PM   Epoch = 2 iter 29 step
06/27 07:08:07 PM   Num examples = 16
06/27 07:08:07 PM   Batch size = 32
06/27 07:08:07 PM ***** Eval results *****
06/27 07:08:07 PM   acc = 0.625
06/27 07:08:07 PM   cls_loss = 2.1038829565048216
06/27 07:08:07 PM   eval_loss = 1.7662241458892822
06/27 07:08:07 PM   global_step = 29
06/27 07:08:07 PM   loss = 2.1038829565048216
06/27 07:08:08 PM ***** LOSS printing *****
06/27 07:08:08 PM loss
06/27 07:08:08 PM tensor(2.3493, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:08 PM ***** LOSS printing *****
06/27 07:08:08 PM loss
06/27 07:08:08 PM tensor(1.1123, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:08 PM ***** LOSS printing *****
06/27 07:08:08 PM loss
06/27 07:08:08 PM tensor(1.7587, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:08 PM ***** LOSS printing *****
06/27 07:08:08 PM loss
06/27 07:08:08 PM tensor(2.3182, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:08 PM ***** LOSS printing *****
06/27 07:08:08 PM loss
06/27 07:08:08 PM tensor(2.0439, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:09 PM ***** Running evaluation MLM *****
06/27 07:08:09 PM   Epoch = 2 iter 34 step
06/27 07:08:09 PM   Num examples = 16
06/27 07:08:09 PM   Batch size = 32
06/27 07:08:09 PM ***** Eval results *****
06/27 07:08:09 PM   acc = 0.6875
06/27 07:08:09 PM   cls_loss = 2.0101805925369263
06/27 07:08:09 PM   eval_loss = 2.181060314178467
06/27 07:08:09 PM   global_step = 34
06/27 07:08:09 PM   loss = 2.0101805925369263
06/27 07:08:09 PM ***** LOSS printing *****
06/27 07:08:09 PM loss
06/27 07:08:09 PM tensor(1.6887, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:09 PM ***** LOSS printing *****
06/27 07:08:09 PM loss
06/27 07:08:09 PM tensor(1.5223, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:10 PM ***** LOSS printing *****
06/27 07:08:10 PM loss
06/27 07:08:10 PM tensor(1.6367, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:10 PM ***** LOSS printing *****
06/27 07:08:10 PM loss
06/27 07:08:10 PM tensor(1.8633, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:10 PM ***** LOSS printing *****
06/27 07:08:10 PM loss
06/27 07:08:10 PM tensor(2.3830, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:10 PM ***** Running evaluation MLM *****
06/27 07:08:10 PM   Epoch = 3 iter 39 step
06/27 07:08:10 PM   Num examples = 16
06/27 07:08:10 PM   Batch size = 32
06/27 07:08:11 PM ***** Eval results *****
06/27 07:08:11 PM   acc = 0.5625
06/27 07:08:11 PM   cls_loss = 1.9610154628753662
06/27 07:08:11 PM   eval_loss = 2.8298726081848145
06/27 07:08:11 PM   global_step = 39
06/27 07:08:11 PM   loss = 1.9610154628753662
06/27 07:08:11 PM ***** LOSS printing *****
06/27 07:08:11 PM loss
06/27 07:08:11 PM tensor(1.8848, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:11 PM ***** LOSS printing *****
06/27 07:08:11 PM loss
06/27 07:08:11 PM tensor(2.0453, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:11 PM ***** LOSS printing *****
06/27 07:08:11 PM loss
06/27 07:08:11 PM tensor(1.1051, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:11 PM ***** LOSS printing *****
06/27 07:08:11 PM loss
06/27 07:08:11 PM tensor(2.2701, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:12 PM ***** LOSS printing *****
06/27 07:08:12 PM loss
06/27 07:08:12 PM tensor(1.8689, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:12 PM ***** Running evaluation MLM *****
06/27 07:08:12 PM   Epoch = 3 iter 44 step
06/27 07:08:12 PM   Num examples = 16
06/27 07:08:12 PM   Batch size = 32
06/27 07:08:12 PM ***** Eval results *****
06/27 07:08:12 PM   acc = 0.5
06/27 07:08:12 PM   cls_loss = 1.882152944803238
06/27 07:08:12 PM   eval_loss = 1.9602433443069458
06/27 07:08:12 PM   global_step = 44
06/27 07:08:12 PM   loss = 1.882152944803238
06/27 07:08:12 PM ***** LOSS printing *****
06/27 07:08:12 PM loss
06/27 07:08:12 PM tensor(1.2028, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:13 PM ***** LOSS printing *****
06/27 07:08:13 PM loss
06/27 07:08:13 PM tensor(3.0196, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:13 PM ***** LOSS printing *****
06/27 07:08:13 PM loss
06/27 07:08:13 PM tensor(1.8390, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:13 PM ***** LOSS printing *****
06/27 07:08:13 PM loss
06/27 07:08:13 PM tensor(2.2875, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:13 PM ***** LOSS printing *****
06/27 07:08:13 PM loss
06/27 07:08:13 PM tensor(1.1223, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:08:13 PM ***** Running evaluation MLM *****
06/27 07:08:13 PM   Epoch = 4 iter 49 step
06/27 07:08:13 PM   Num examples = 16
06/27 07:08:13 PM   Batch size = 32
06/27 07:08:14 PM ***** Eval results *****
06/27 07:08:14 PM   acc = 0.9375
06/27 07:08:14 PM   cls_loss = 1.122287392616272
06/27 07:08:14 PM   eval_loss = 1.009444236755371
06/27 07:08:14 PM   global_step = 49
06/27 07:08:14 PM   loss = 1.122287392616272
06/27 07:08:14 PM ***** Save model *****
06/27 07:08:14 PM ***** Test Dataset Eval Result *****
06/27 07:09:17 PM ***** Eval results *****
06/27 07:09:17 PM   acc = 0.831
06/27 07:09:17 PM   cls_loss = 1.122287392616272
06/27 07:09:17 PM   eval_loss = 1.2136439122850933
06/27 07:09:17 PM   global_step = 49
06/27 07:09:17 PM   loss = 1.122287392616272
06/27 07:09:21 PM ***** LOSS printing *****
06/27 07:09:21 PM loss
06/27 07:09:21 PM tensor(1.5713, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:21 PM ***** LOSS printing *****
06/27 07:09:21 PM loss
06/27 07:09:21 PM tensor(1.5785, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:21 PM ***** LOSS printing *****
06/27 07:09:21 PM loss
06/27 07:09:21 PM tensor(2.3781, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:21 PM ***** LOSS printing *****
06/27 07:09:21 PM loss
06/27 07:09:21 PM tensor(1.8371, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:21 PM ***** LOSS printing *****
06/27 07:09:21 PM loss
06/27 07:09:21 PM tensor(1.3205, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:22 PM ***** Running evaluation MLM *****
06/27 07:09:22 PM   Epoch = 4 iter 54 step
06/27 07:09:22 PM   Num examples = 16
06/27 07:09:22 PM   Batch size = 32
06/27 07:09:22 PM ***** Eval results *****
06/27 07:09:22 PM   acc = 0.9375
06/27 07:09:22 PM   cls_loss = 1.6346314748128254
06/27 07:09:22 PM   eval_loss = 1.0921916961669922
06/27 07:09:22 PM   global_step = 54
06/27 07:09:22 PM   loss = 1.6346314748128254
06/27 07:09:22 PM ***** LOSS printing *****
06/27 07:09:22 PM loss
06/27 07:09:22 PM tensor(0.9963, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:22 PM ***** LOSS printing *****
06/27 07:09:22 PM loss
06/27 07:09:22 PM tensor(1.7541, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:23 PM ***** LOSS printing *****
06/27 07:09:23 PM loss
06/27 07:09:23 PM tensor(1.6056, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:23 PM ***** LOSS printing *****
06/27 07:09:23 PM loss
06/27 07:09:23 PM tensor(1.5609, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:23 PM ***** LOSS printing *****
06/27 07:09:23 PM loss
06/27 07:09:23 PM tensor(1.7597, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:09:23 PM ***** Running evaluation MLM *****
06/27 07:09:23 PM   Epoch = 4 iter 59 step
06/27 07:09:23 PM   Num examples = 16
06/27 07:09:23 PM   Batch size = 32
06/27 07:09:24 PM ***** Eval results *****
06/27 07:09:24 PM   acc = 1.0
06/27 07:09:24 PM   cls_loss = 1.5894846320152283
06/27 07:09:24 PM   eval_loss = 1.0907572507858276
06/27 07:09:24 PM   global_step = 59
06/27 07:09:24 PM   loss = 1.5894846320152283
06/27 07:09:24 PM ***** Save model *****
06/27 07:09:24 PM ***** Test Dataset Eval Result *****
06/27 07:10:26 PM ***** Eval results *****
06/27 07:10:26 PM   acc = 0.8625
06/27 07:10:26 PM   cls_loss = 1.5894846320152283
06/27 07:10:26 PM   eval_loss = 1.2214256676416548
06/27 07:10:26 PM   global_step = 59
06/27 07:10:26 PM   loss = 1.5894846320152283
06/27 07:10:30 PM ***** LOSS printing *****
06/27 07:10:30 PM loss
06/27 07:10:30 PM tensor(1.5628, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:30 PM ***** LOSS printing *****
06/27 07:10:30 PM loss
06/27 07:10:30 PM tensor(1.3375, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:30 PM ***** LOSS printing *****
06/27 07:10:30 PM loss
06/27 07:10:30 PM tensor(1.6746, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:30 PM ***** LOSS printing *****
06/27 07:10:30 PM loss
06/27 07:10:30 PM tensor(1.4329, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:30 PM ***** LOSS printing *****
06/27 07:10:30 PM loss
06/27 07:10:30 PM tensor(1.0970, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:31 PM ***** Running evaluation MLM *****
06/27 07:10:31 PM   Epoch = 5 iter 64 step
06/27 07:10:31 PM   Num examples = 16
06/27 07:10:31 PM   Batch size = 32
06/27 07:10:31 PM ***** Eval results *****
06/27 07:10:31 PM   acc = 0.9375
06/27 07:10:31 PM   cls_loss = 1.3854915499687195
06/27 07:10:31 PM   eval_loss = 1.341433048248291
06/27 07:10:31 PM   global_step = 64
06/27 07:10:31 PM   loss = 1.3854915499687195
06/27 07:10:31 PM ***** LOSS printing *****
06/27 07:10:31 PM loss
06/27 07:10:31 PM tensor(1.2322, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:31 PM ***** LOSS printing *****
06/27 07:10:31 PM loss
06/27 07:10:31 PM tensor(1.5218, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:32 PM ***** LOSS printing *****
06/27 07:10:32 PM loss
06/27 07:10:32 PM tensor(1.2361, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:32 PM ***** LOSS printing *****
06/27 07:10:32 PM loss
06/27 07:10:32 PM tensor(1.8919, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:32 PM ***** LOSS printing *****
06/27 07:10:32 PM loss
06/27 07:10:32 PM tensor(1.6596, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:32 PM ***** Running evaluation MLM *****
06/27 07:10:32 PM   Epoch = 5 iter 69 step
06/27 07:10:32 PM   Num examples = 16
06/27 07:10:32 PM   Batch size = 32
06/27 07:10:33 PM ***** Eval results *****
06/27 07:10:33 PM   acc = 0.9375
06/27 07:10:33 PM   cls_loss = 1.4537284109327528
06/27 07:10:33 PM   eval_loss = 1.2321845293045044
06/27 07:10:33 PM   global_step = 69
06/27 07:10:33 PM   loss = 1.4537284109327528
06/27 07:10:33 PM ***** LOSS printing *****
06/27 07:10:33 PM loss
06/27 07:10:33 PM tensor(1.3201, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:33 PM ***** LOSS printing *****
06/27 07:10:33 PM loss
06/27 07:10:33 PM tensor(1.1401, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:33 PM ***** LOSS printing *****
06/27 07:10:33 PM loss
06/27 07:10:33 PM tensor(1.4846, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:33 PM ***** LOSS printing *****
06/27 07:10:33 PM loss
06/27 07:10:33 PM tensor(1.2690, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:34 PM ***** LOSS printing *****
06/27 07:10:34 PM loss
06/27 07:10:34 PM tensor(1.6060, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:34 PM ***** Running evaluation MLM *****
06/27 07:10:34 PM   Epoch = 6 iter 74 step
06/27 07:10:34 PM   Num examples = 16
06/27 07:10:34 PM   Batch size = 32
06/27 07:10:34 PM ***** Eval results *****
06/27 07:10:34 PM   acc = 1.0
06/27 07:10:34 PM   cls_loss = 1.4374956488609314
06/27 07:10:34 PM   eval_loss = 1.3878178596496582
06/27 07:10:34 PM   global_step = 74
06/27 07:10:34 PM   loss = 1.4374956488609314
06/27 07:10:34 PM ***** LOSS printing *****
06/27 07:10:34 PM loss
06/27 07:10:34 PM tensor(1.3084, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:35 PM ***** LOSS printing *****
06/27 07:10:35 PM loss
06/27 07:10:35 PM tensor(1.1723, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:35 PM ***** LOSS printing *****
06/27 07:10:35 PM loss
06/27 07:10:35 PM tensor(1.3163, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:35 PM ***** LOSS printing *****
06/27 07:10:35 PM loss
06/27 07:10:35 PM tensor(1.4086, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:35 PM ***** LOSS printing *****
06/27 07:10:35 PM loss
06/27 07:10:35 PM tensor(1.0611, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:35 PM ***** Running evaluation MLM *****
06/27 07:10:35 PM   Epoch = 6 iter 79 step
06/27 07:10:35 PM   Num examples = 16
06/27 07:10:35 PM   Batch size = 32
06/27 07:10:36 PM ***** Eval results *****
06/27 07:10:36 PM   acc = 1.0
06/27 07:10:36 PM   cls_loss = 1.305949364389692
06/27 07:10:36 PM   eval_loss = 1.3698865175247192
06/27 07:10:36 PM   global_step = 79
06/27 07:10:36 PM   loss = 1.305949364389692
06/27 07:10:36 PM ***** LOSS printing *****
06/27 07:10:36 PM loss
06/27 07:10:36 PM tensor(1.3812, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:36 PM ***** LOSS printing *****
06/27 07:10:36 PM loss
06/27 07:10:36 PM tensor(1.8904, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:36 PM ***** LOSS printing *****
06/27 07:10:36 PM loss
06/27 07:10:36 PM tensor(1.8052, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:37 PM ***** LOSS printing *****
06/27 07:10:37 PM loss
06/27 07:10:37 PM tensor(1.6527, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:37 PM ***** LOSS printing *****
06/27 07:10:37 PM loss
06/27 07:10:37 PM tensor(1.1792, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:37 PM ***** Running evaluation MLM *****
06/27 07:10:37 PM   Epoch = 6 iter 84 step
06/27 07:10:37 PM   Num examples = 16
06/27 07:10:37 PM   Batch size = 32
06/27 07:10:38 PM ***** Eval results *****
06/27 07:10:38 PM   acc = 1.0
06/27 07:10:38 PM   cls_loss = 1.420863687992096
06/27 07:10:38 PM   eval_loss = 1.4225375652313232
06/27 07:10:38 PM   global_step = 84
06/27 07:10:38 PM   loss = 1.420863687992096
06/27 07:10:38 PM ***** LOSS printing *****
06/27 07:10:38 PM loss
06/27 07:10:38 PM tensor(1.4987, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:38 PM ***** LOSS printing *****
06/27 07:10:38 PM loss
06/27 07:10:38 PM tensor(1.0940, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:38 PM ***** LOSS printing *****
06/27 07:10:38 PM loss
06/27 07:10:38 PM tensor(1.2949, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:38 PM ***** LOSS printing *****
06/27 07:10:38 PM loss
06/27 07:10:38 PM tensor(1.2929, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:38 PM ***** LOSS printing *****
06/27 07:10:38 PM loss
06/27 07:10:38 PM tensor(1.2800, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:39 PM ***** Running evaluation MLM *****
06/27 07:10:39 PM   Epoch = 7 iter 89 step
06/27 07:10:39 PM   Num examples = 16
06/27 07:10:39 PM   Batch size = 32
06/27 07:10:39 PM ***** Eval results *****
06/27 07:10:39 PM   acc = 1.0
06/27 07:10:39 PM   cls_loss = 1.292092537879944
06/27 07:10:39 PM   eval_loss = 0.7776151299476624
06/27 07:10:39 PM   global_step = 89
06/27 07:10:39 PM   loss = 1.292092537879944
06/27 07:10:39 PM ***** LOSS printing *****
06/27 07:10:39 PM loss
06/27 07:10:39 PM tensor(1.5900, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:39 PM ***** LOSS printing *****
06/27 07:10:39 PM loss
06/27 07:10:39 PM tensor(1.6201, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:40 PM ***** LOSS printing *****
06/27 07:10:40 PM loss
06/27 07:10:40 PM tensor(1.6111, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:40 PM ***** LOSS printing *****
06/27 07:10:40 PM loss
06/27 07:10:40 PM tensor(1.1328, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:40 PM ***** LOSS printing *****
06/27 07:10:40 PM loss
06/27 07:10:40 PM tensor(1.2054, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:40 PM ***** Running evaluation MLM *****
06/27 07:10:40 PM   Epoch = 7 iter 94 step
06/27 07:10:40 PM   Num examples = 16
06/27 07:10:40 PM   Batch size = 32
06/27 07:10:41 PM ***** Eval results *****
06/27 07:10:41 PM   acc = 1.0
06/27 07:10:41 PM   cls_loss = 1.361989676952362
06/27 07:10:41 PM   eval_loss = 1.2623767852783203
06/27 07:10:41 PM   global_step = 94
06/27 07:10:41 PM   loss = 1.361989676952362
06/27 07:10:41 PM ***** LOSS printing *****
06/27 07:10:41 PM loss
06/27 07:10:41 PM tensor(1.8777, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:41 PM ***** LOSS printing *****
06/27 07:10:41 PM loss
06/27 07:10:41 PM tensor(1.4157, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:41 PM ***** LOSS printing *****
06/27 07:10:41 PM loss
06/27 07:10:41 PM tensor(1.6300, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:41 PM ***** LOSS printing *****
06/27 07:10:41 PM loss
06/27 07:10:41 PM tensor(1.5199, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:42 PM ***** LOSS printing *****
06/27 07:10:42 PM loss
06/27 07:10:42 PM tensor(1.1982, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:42 PM ***** Running evaluation MLM *****
06/27 07:10:42 PM   Epoch = 8 iter 99 step
06/27 07:10:42 PM   Num examples = 16
06/27 07:10:42 PM   Batch size = 32
06/27 07:10:42 PM ***** Eval results *****
06/27 07:10:42 PM   acc = 1.0
06/27 07:10:42 PM   cls_loss = 1.4493696292241414
06/27 07:10:42 PM   eval_loss = 1.634246587753296
06/27 07:10:42 PM   global_step = 99
06/27 07:10:42 PM   loss = 1.4493696292241414
06/27 07:10:42 PM ***** LOSS printing *****
06/27 07:10:42 PM loss
06/27 07:10:42 PM tensor(1.6342, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:43 PM ***** LOSS printing *****
06/27 07:10:43 PM loss
06/27 07:10:43 PM tensor(1.7026, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:43 PM ***** LOSS printing *****
06/27 07:10:43 PM loss
06/27 07:10:43 PM tensor(0.8037, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:43 PM ***** LOSS printing *****
06/27 07:10:43 PM loss
06/27 07:10:43 PM tensor(1.9488, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:43 PM ***** LOSS printing *****
06/27 07:10:43 PM loss
06/27 07:10:43 PM tensor(1.1767, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:43 PM ***** Running evaluation MLM *****
06/27 07:10:43 PM   Epoch = 8 iter 104 step
06/27 07:10:43 PM   Num examples = 16
06/27 07:10:43 PM   Batch size = 32
06/27 07:10:44 PM ***** Eval results *****
06/27 07:10:44 PM   acc = 1.0
06/27 07:10:44 PM   cls_loss = 1.4517627730965614
06/27 07:10:44 PM   eval_loss = 1.4400523900985718
06/27 07:10:44 PM   global_step = 104
06/27 07:10:44 PM   loss = 1.4517627730965614
06/27 07:10:44 PM ***** LOSS printing *****
06/27 07:10:44 PM loss
06/27 07:10:44 PM tensor(1.5553, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:44 PM ***** LOSS printing *****
06/27 07:10:44 PM loss
06/27 07:10:44 PM tensor(0.7980, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:44 PM ***** LOSS printing *****
06/27 07:10:44 PM loss
06/27 07:10:44 PM tensor(1.4335, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:45 PM ***** LOSS printing *****
06/27 07:10:45 PM loss
06/27 07:10:45 PM tensor(1.4725, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:45 PM ***** LOSS printing *****
06/27 07:10:45 PM loss
06/27 07:10:45 PM tensor(0.9239, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:45 PM ***** Running evaluation MLM *****
06/27 07:10:45 PM   Epoch = 9 iter 109 step
06/27 07:10:45 PM   Num examples = 16
06/27 07:10:45 PM   Batch size = 32
06/27 07:10:46 PM ***** Eval results *****
06/27 07:10:46 PM   acc = 1.0
06/27 07:10:46 PM   cls_loss = 0.9239388108253479
06/27 07:10:46 PM   eval_loss = 1.391179084777832
06/27 07:10:46 PM   global_step = 109
06/27 07:10:46 PM   loss = 0.9239388108253479
06/27 07:10:46 PM ***** LOSS printing *****
06/27 07:10:46 PM loss
06/27 07:10:46 PM tensor(1.2918, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:46 PM ***** LOSS printing *****
06/27 07:10:46 PM loss
06/27 07:10:46 PM tensor(1.4319, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:46 PM ***** LOSS printing *****
06/27 07:10:46 PM loss
06/27 07:10:46 PM tensor(1.2505, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:46 PM ***** LOSS printing *****
06/27 07:10:46 PM loss
06/27 07:10:46 PM tensor(1.7220, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:46 PM ***** LOSS printing *****
06/27 07:10:46 PM loss
06/27 07:10:46 PM tensor(1.1773, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:47 PM ***** Running evaluation MLM *****
06/27 07:10:47 PM   Epoch = 9 iter 114 step
06/27 07:10:47 PM   Num examples = 16
06/27 07:10:47 PM   Batch size = 32
06/27 07:10:47 PM ***** Eval results *****
06/27 07:10:47 PM   acc = 1.0
06/27 07:10:47 PM   cls_loss = 1.2995812197526295
06/27 07:10:47 PM   eval_loss = 0.819766640663147
06/27 07:10:47 PM   global_step = 114
06/27 07:10:47 PM   loss = 1.2995812197526295
06/27 07:10:47 PM ***** LOSS printing *****
06/27 07:10:47 PM loss
06/27 07:10:47 PM tensor(0.9281, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:47 PM ***** LOSS printing *****
06/27 07:10:47 PM loss
06/27 07:10:47 PM tensor(1.6967, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:48 PM ***** LOSS printing *****
06/27 07:10:48 PM loss
06/27 07:10:48 PM tensor(1.1145, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:48 PM ***** LOSS printing *****
06/27 07:10:48 PM loss
06/27 07:10:48 PM tensor(2.5447, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:48 PM ***** LOSS printing *****
06/27 07:10:48 PM loss
06/27 07:10:48 PM tensor(1.5492, device='cuda:0', grad_fn=<NllLossBackward0>)
06/27 07:10:48 PM ***** Running evaluation MLM *****
06/27 07:10:48 PM   Epoch = 9 iter 119 step
06/27 07:10:48 PM   Num examples = 16
06/27 07:10:48 PM   Batch size = 32
06/27 07:10:49 PM ***** Eval results *****
06/27 07:10:49 PM   acc = 1.0
06/27 07:10:49 PM   cls_loss = 1.4209704724225132
06/27 07:10:49 PM   eval_loss = 1.032977819442749
06/27 07:10:49 PM   global_step = 119
06/27 07:10:49 PM   loss = 1.4209704724225132
06/27 07:10:49 PM ***** LOSS printing *****
06/27 07:10:49 PM loss
06/27 07:10:49 PM tensor(2.2699, device='cuda:0', grad_fn=<NllLossBackward0>)
